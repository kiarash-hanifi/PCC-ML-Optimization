{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "_gotG2Tyos9k",
      "metadata": {
        "id": "_gotG2Tyos9k"
      },
      "source": [
        "```\n",
        "MODELS:\n",
        "1. TabPFN\n",
        "2. ANN\n",
        "3. PSO-XGBoost\n",
        "4. PSO-GBR\n",
        "5. PSO-CatBoost\n",
        "6. PSO-AdaBoost\n",
        "```\n",
        "Features:\n",
        "1. Flue Gas CO2 (mol%)\n",
        "2. Flue Gas Flow (kg/h)\n",
        "3. Lean Solvent Flow (kg/h)\n",
        "4. CO2-amine Loading (Lean Solvent Loading)\n",
        "5. Amine Type\n",
        "6. Amine Concentration (wt%)\n",
        "7. Lean Solvent Temperature\n",
        "8. Reboiler Duty\n",
        "\n",
        "Targets:\n",
        "1. Recovery%\n",
        "2. SEC (kJ/kg)\n",
        "3. CO2 Purity (mol%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B0T62JfjEbLL",
      "metadata": {
        "id": "B0T62JfjEbLL"
      },
      "source": [
        "# **Data cleaning and pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e16e1c3-d3e8-4f73-a13f-e9c8e6f53d3f",
      "metadata": {
        "id": "9e16e1c3-d3e8-4f73-a13f-e9c8e6f53d3f",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Importing the Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f47282b-a771-4172-ae1d-931f5710da7d",
      "metadata": {
        "id": "6f47282b-a771-4172-ae1d-931f5710da7d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bc304ec-d4a8-490a-8c33-8ee4f10a2ba4",
      "metadata": {
        "id": "5bc304ec-d4a8-490a-8c33-8ee4f10a2ba4",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Importing the data and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pZUGYv-AbfsW",
      "metadata": {
        "id": "pZUGYv-AbfsW"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(file_name)\n",
        "# # ----------------------------------------------------------\n",
        "# df = pd.read_excel(\"C:\\\\Users\\\\Pgshco\\\\Desktop\\\\Python ML\\\\Carbon Capture\\\\ML codes\\\\Final Code\\\\FinalData_Rev03.xlsx\")\n",
        "# # ----------------------------------------------------------\n",
        "# Convert to CSV and display\n",
        "df.to_csv(\"Data.csv\", index=True)\n",
        "print(\"First 5 rows of data:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "980d75b3-582a-411c-8411-f9ac39482532",
      "metadata": {
        "id": "980d75b3-582a-411c-8411-f9ac39482532"
      },
      "outputs": [],
      "source": [
        "print(\"Before dropna:\", df.shape)\n",
        "df_cleaned = df.dropna()\n",
        "print(\"After dropna:\", df_cleaned.shape)\n",
        "# Check missing data\n",
        "print(df.isna().sum())\n",
        "\n",
        "# Drop only rows where 'Amine type' is missing\n",
        "df_cleaned = df.dropna(subset=['Amine type'])\n",
        "\n",
        "# Encode\n",
        "df_cleaned = df_cleaned.replace({\n",
        "    'Amine type': {'MEA': 1, 'DGA': 2, 'DEA': 3, 'TEA': 4, 'MDEA': 5}\n",
        "})\n",
        "\n",
        "# Save\n",
        "df_cleaned.to_csv('cleaned_data_encoded.csv', index=True)\n",
        "print(\"✅ Encoded DataFrame saved as 'cleaned_data_encoded.csv'\")\n",
        "\n",
        "# Reload\n",
        "data = pd.read_csv('cleaned_data_encoded.csv')\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OGn7NEU9iKvd",
      "metadata": {
        "id": "OGn7NEU9iKvd"
      },
      "outputs": [],
      "source": [
        "data.describe().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab34b875-4a79-4b04-b9b3-8c6ae028376b",
      "metadata": {
        "id": "ab34b875-4a79-4b04-b9b3-8c6ae028376b",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Feature Selection (before standardization)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f64e8ed4-09d1-4b3f-8b7e-0f60af5b090e",
      "metadata": {
        "id": "f64e8ed4-09d1-4b3f-8b7e-0f60af5b090e",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Heat map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f8923bc-d5e8-4a9a-afc4-ca4374d0d454",
      "metadata": {
        "id": "4f8923bc-d5e8-4a9a-afc4-ca4374d0d454"
      },
      "outputs": [],
      "source": [
        "# Drop the \"Unnamed: 0\" column from the data\n",
        "data = data.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Now generate the correlation matrix and heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "mask = np.triu(np.ones_like(data.corr(), dtype=bool), k=1)\n",
        "heatmap = sns.heatmap(data.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='RdBu')\n",
        "\n",
        "# Save the plot as a PNG with 600 dpi\n",
        "plt.savefig('Figures/correlation_heatmap.png', dpi=600, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eNRcfMSxDcwh",
      "metadata": {
        "id": "eNRcfMSxDcwh",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Box plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bf23917-caab-4bc3-9409-7888cb87accb",
      "metadata": {
        "id": "5bf23917-caab-4bc3-9409-7888cb87accb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "box_color = \"#377eb8\"\n",
        "cols = list(data.columns)          # 11 parameters\n",
        "n = len(cols)\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "\n",
        "# Outer grid: 3 rows, 1 column → each row takes full width\n",
        "outer = gridspec.GridSpec(3, 1, height_ratios=[1, 1, 1], hspace=0.4)\n",
        "\n",
        "# Row 1: 4 plots\n",
        "gs_top = outer[0].subgridspec(1, 4, wspace=0.4)\n",
        "# Row 2: 4 plots\n",
        "gs_mid = outer[1].subgridspec(1, 4, wspace=0.4)\n",
        "# Row 3: 3 centered plots\n",
        "gs_bot = outer[2].subgridspec(1, 3, wspace=0.4)\n",
        "\n",
        "idx = 0\n",
        "\n",
        "# ---- First row (4 plots) ----\n",
        "for j in range(4):\n",
        "    if idx >= n:\n",
        "        break\n",
        "    ax = fig.add_subplot(gs_top[0, j])\n",
        "    sns.boxplot(\n",
        "        y=data[cols[idx]],\n",
        "        ax=ax,\n",
        "        color=box_color,\n",
        "        showmeans=True,\n",
        "        meanprops={\"marker\": \"x\",\n",
        "                   \"markeredgecolor\": \"black\",\n",
        "                   \"markerfacecolor\": \"black\"}\n",
        "    )\n",
        "    ax.set_title(cols[idx], fontsize=12)\n",
        "    ax.set_xticks([])\n",
        "    idx += 1\n",
        "\n",
        "# ---- Second row (4 plots) ----\n",
        "for j in range(4):\n",
        "    if idx >= n:\n",
        "        break\n",
        "    ax = fig.add_subplot(gs_mid[0, j])\n",
        "    sns.boxplot(\n",
        "        y=data[cols[idx]],\n",
        "        ax=ax,\n",
        "        color=box_color,\n",
        "        showmeans=True,\n",
        "        meanprops={\"marker\": \"x\",\n",
        "                   \"markeredgecolor\": \"black\",\n",
        "                   \"markerfacecolor\": \"black\"}\n",
        "    )\n",
        "    ax.set_title(cols[idx], fontsize=12)\n",
        "    ax.set_xticks([])\n",
        "    idx += 1\n",
        "\n",
        "# ---- Third row (3 centered plots) ----\n",
        "for j in range(3):\n",
        "    if idx >= n:\n",
        "        break\n",
        "    ax = fig.add_subplot(gs_bot[0, j])\n",
        "    sns.boxplot(\n",
        "        y=data[cols[idx]],\n",
        "        ax=ax,\n",
        "        color=box_color,\n",
        "        showmeans=True,\n",
        "        meanprops={\"marker\": \"x\",\n",
        "                   \"markeredgecolor\": \"black\",\n",
        "                   \"markerfacecolor\": \"black\"}\n",
        "    )\n",
        "    ax.set_title(cols[idx], fontsize=12)\n",
        "    ax.set_xticks([])\n",
        "    idx += 1\n",
        "\n",
        "plt.savefig('Figures/boxPlot.png', dpi=600, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef84f28-51b4-49de-b89e-52d9ee3dba9d",
      "metadata": {
        "id": "6ef84f28-51b4-49de-b89e-52d9ee3dba9d",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Violin-Box Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c817902-2d9a-4129-a43f-59a59b59bd3e",
      "metadata": {
        "id": "3c817902-2d9a-4129-a43f-59a59b59bd3e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "# ---- Data ----\n",
        "numeric_df = data.select_dtypes(include=[\"float64\", \"int64\"])\n",
        "\n",
        "# ---- Journal styling ----\n",
        "sns.set_theme(style=\"whitegrid\", font_scale=1.3, rc={\n",
        "    \"axes.labelsize\": 14,\n",
        "    \"axes.titlesize\": 16,\n",
        "    \"xtick.labelsize\": 11,\n",
        "    \"ytick.labelsize\": 11,\n",
        "    \"axes.linewidth\": 1.2,\n",
        "    \"font.family\": \"serif\",\n",
        "})\n",
        "\n",
        "# Colors\n",
        "box_color   = \"#377eb8\"   # your chosen blue\n",
        "violin_col  = \"#f4a582\"   # light red (RdBu-compatible)\n",
        "\n",
        "cols = list(numeric_df.columns)\n",
        "num_features = len(cols)   # e.g. 11\n",
        "\n",
        "# Figure and outer grid: 3 rows x 1 col\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "outer = gridspec.GridSpec(3, 1, figure=fig, hspace=0.6)\n",
        "\n",
        "# Row 1: 4 plots\n",
        "gs_top = outer[0].subgridspec(1, 4, wspace=0.4)\n",
        "# Row 2: 4 plots\n",
        "gs_mid = outer[1].subgridspec(1, 4, wspace=0.4)\n",
        "# Row 3: 3 centered plots (they span full width, so naturally centered)\n",
        "gs_bot = outer[2].subgridspec(1, 3, wspace=0.6)\n",
        "\n",
        "idx = 0\n",
        "\n",
        "# ---- First row (up to 4 features) ----\n",
        "for j in range(4):\n",
        "    if idx >= num_features:\n",
        "        break\n",
        "    ax = fig.add_subplot(gs_top[0, j])\n",
        "\n",
        "    sns.violinplot(\n",
        "        y=numeric_df[cols[idx]],\n",
        "        ax=ax,\n",
        "        color=violin_col,\n",
        "        inner=None,\n",
        "        cut=0,\n",
        "        bw_adjust=1.4,\n",
        "        alpha=0.35,\n",
        "        linewidth=0.6,\n",
        "        width=0.6\n",
        "    )\n",
        "\n",
        "    sns.boxplot(\n",
        "        y=numeric_df[cols[idx]],\n",
        "        ax=ax,\n",
        "        width=0.22,\n",
        "        color=box_color,\n",
        "        showmeans=True,\n",
        "        meanprops={\n",
        "            \"marker\": \"D\",\n",
        "            \"markeredgecolor\": \"black\",\n",
        "            \"markerfacecolor\": \"black\",\n",
        "            \"markersize\": 5\n",
        "        }\n",
        "    )\n",
        "\n",
        "    ax.set_title(cols[idx], fontsize=14)\n",
        "    ax.set_xticks([])\n",
        "    ax.yaxis.grid(True, linestyle=\"--\", linewidth=0.7, alpha=0.7)\n",
        "    sns.despine(ax=ax, left=True, bottom=True)\n",
        "\n",
        "    idx += 1\n",
        "\n",
        "# ---- Second row (next up to 4 features) ----\n",
        "for j in range(4):\n",
        "    if idx >= num_features:\n",
        "        break\n",
        "    ax = fig.add_subplot(gs_mid[0, j])\n",
        "\n",
        "    sns.violinplot(\n",
        "        y=numeric_df[cols[idx]],\n",
        "        ax=ax,\n",
        "        color=violin_col,\n",
        "        inner=None,\n",
        "        cut=0,\n",
        "        bw_adjust=1.4,\n",
        "        alpha=0.35,\n",
        "        linewidth=0.6,\n",
        "        width=0.6\n",
        "    )\n",
        "\n",
        "    sns.boxplot(\n",
        "        y=numeric_df[cols[idx]],\n",
        "        ax=ax,\n",
        "        width=0.22,\n",
        "        color=box_color,\n",
        "        showmeans=True,\n",
        "        meanprops={\n",
        "            \"marker\": \"D\",\n",
        "            \"markeredgecolor\": \"black\",\n",
        "            \"markerfacecolor\": \"black\",\n",
        "            \"markersize\": 5\n",
        "        }\n",
        "    )\n",
        "\n",
        "    ax.set_title(cols[idx], fontsize=14)\n",
        "    ax.set_xticks([])\n",
        "    ax.yaxis.grid(True, linestyle=\"--\", linewidth=0.7, alpha=0.7)\n",
        "    sns.despine(ax=ax, left=True, bottom=True)\n",
        "\n",
        "    idx += 1\n",
        "\n",
        "# ---- Third row (3 centered plots) ----\n",
        "for j in range(3):\n",
        "    if idx >= num_features:\n",
        "        break\n",
        "    ax = fig.add_subplot(gs_bot[0, j])\n",
        "\n",
        "    sns.violinplot(\n",
        "        y=numeric_df[cols[idx]],\n",
        "        ax=ax,\n",
        "        color=violin_col,\n",
        "        inner=None,\n",
        "        cut=0,\n",
        "        bw_adjust=1.4,\n",
        "        alpha=0.35,\n",
        "        linewidth=0.6,\n",
        "        width=0.6\n",
        "    )\n",
        "\n",
        "    sns.boxplot(\n",
        "        y=numeric_df[cols[idx]],\n",
        "        ax=ax,\n",
        "        width=0.22,\n",
        "        color=box_color,\n",
        "        showmeans=True,\n",
        "        meanprops={\n",
        "            \"marker\": \"D\",\n",
        "            \"markeredgecolor\": \"black\",\n",
        "            \"markerfacecolor\": \"black\",\n",
        "            \"markersize\": 5\n",
        "        }\n",
        "    )\n",
        "\n",
        "    ax.set_title(cols[idx], fontsize=14)\n",
        "    ax.set_xticks([])\n",
        "    ax.yaxis.grid(True, linestyle=\"--\", linewidth=0.7, alpha=0.7)\n",
        "    sns.despine(ax=ax, left=True, bottom=True)\n",
        "\n",
        "    idx += 1\n",
        "\n",
        "plt.savefig(\"Figures/violin_box_subplots.png\", dpi=600, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9660c3c2-b5e1-48bb-83ae-00dc23aa4b6e",
      "metadata": {
        "id": "9660c3c2-b5e1-48bb-83ae-00dc23aa4b6e",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Pair plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f478d94d-723b-4a1c-b2fb-6d1ba612ebc2",
      "metadata": {
        "id": "f478d94d-723b-4a1c-b2fb-6d1ba612ebc2"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "# Unified color matching your palette\n",
        "point_color = \"#377eb8\"   # chosen medium blue\n",
        "\n",
        "# Create pairplot with professional styling\n",
        "g = sns.pairplot(\n",
        "    data,\n",
        "    diag_kind=\"hist\",\n",
        "    plot_kws={\n",
        "        \"s\": 25,                # marker size\n",
        "        \"color\": point_color,   # fill color\n",
        "        \"edgecolor\": \"black\",   # outline color\n",
        "        \"linewidth\": 0.5,       # thickness of outline\n",
        "        \"alpha\": 0.8            # slight transparency\n",
        "    },\n",
        "    diag_kws={\n",
        "        \"color\": point_color,\n",
        "        \"edgecolor\": \"black\"\n",
        "    }\n",
        ")\n",
        "\n",
        "plt.savefig('Figures/pairPlot.png', dpi=1000, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "saJUd25k_M9y",
      "metadata": {
        "id": "saJUd25k_M9y",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Train/Test dataset, standardization, and evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "npIhDYy9_Udx",
      "metadata": {
        "id": "npIhDYy9_Udx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"cleaned_data_encoded.csv\")\n",
        "train_set , test_set = train_test_split(data, test_size=0.2 ,random_state=40,shuffle=True, stratify=None)\n",
        "\n",
        "# test data\n",
        "ts = test_set.copy()\n",
        "\n",
        "ts_label1 = ts[\"Recovery%\"].copy()\n",
        "ts_label2 = ts[\"SEC (kJ/kg)\"].copy()\n",
        "ts_label3 = ts[\"CO2 Purity (mol%)\"].copy()\n",
        "ts = ts.drop([\"Recovery%\", \"SEC (kJ/kg)\", \"CO2 Purity (mol%)\"],axis=1)\n",
        "\n",
        "# train data\n",
        "df = train_set.copy()\n",
        "\n",
        "df_label1 = df[\"Recovery%\"].copy()           # Only the first target is kept\n",
        "df_label2 = df[\"SEC (kJ/kg)\"].copy()         # Only the second target is kept\n",
        "df_label3 = df[\"CO2 Purity (mol%)\"].copy()   # Only the third target is kept\n",
        "df = df.drop([\"Recovery%\", \"SEC (kJ/kg)\", \"CO2 Purity (mol%)\"],axis=1) # drop targets (Only features are kept)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xZaTsmdMiVyx",
      "metadata": {
        "id": "xZaTsmdMiVyx"
      },
      "outputs": [],
      "source": [
        "# Standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "# train data\n",
        "df_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(df), # train data\n",
        "    columns=df.columns,      # keep original names\n",
        "    index=df.index           # keep original index\n",
        ")\n",
        "# test data\n",
        "ts_scaled = pd.DataFrame(\n",
        "    scaler.transform(ts), # test\n",
        "    columns=ts.columns, # keep original names\n",
        "    index=ts.index\n",
        ")\n",
        "\n",
        "df = df_scaled.copy()\n",
        "ts = ts_scaled.copy()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eipEJCnVh0p4",
      "metadata": {
        "id": "eipEJCnVh0p4"
      },
      "source": [
        "**evaluation metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4SZRJqTFhumj",
      "metadata": {
        "id": "4SZRJqTFhumj"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (r2_score, mean_squared_error, mean_absolute_error,\n",
        "                            mean_absolute_percentage_error, median_absolute_error,\n",
        "                            explained_variance_score, root_mean_squared_log_error)\n",
        "\n",
        "def evaluate(true, pred, label=\"Test\", control=None):\n",
        "    # 1. R2 score\n",
        "    r2 = r2_score(true, pred)\n",
        "    # 2. Adjusted R2 score\n",
        "    if label == \"Test\":\n",
        "      n = ts.shape[0]\n",
        "    elif label == \"Train\":\n",
        "      n = df.shape[0]\n",
        "    elif label == \"Total\":\n",
        "      n = ts.shape[0] + df.shape[0]\n",
        "    else:\n",
        "      raise ValueError(\"Invalid label. Use 'Test' or 'Train' or 'Total'.\")\n",
        "\n",
        "    k = df.shape[1] # number of features\n",
        "    adj_r2 = 1 - (1-r2)*(n-1)/ (n-k-1)\n",
        "    # 3.mean squared error\n",
        "    mse = mean_squared_error(true, pred)\n",
        "    # 4.root mean squared error\n",
        "    rmse = np.sqrt(mse)\n",
        "    # 5.Normalized RMSE\n",
        "    nrmse = rmse / (true.max() - true.min())\n",
        "    # 6.Standard Deviation of Errors\n",
        "    std_er = np.std(true - pred)\n",
        "    # 7.Mean Absolute Error\n",
        "    mae = mean_absolute_error(true, pred)\n",
        "    # 8.Mean Absolute Percentage Error\n",
        "    mape = mean_absolute_percentage_error(true, pred)\n",
        "    # 9.Median Absolute Error\n",
        "    medae = median_absolute_error(true, pred)\n",
        "    # 10.Explained Variance Score\n",
        "    evs = explained_variance_score(true, pred, force_finite=False)\n",
        "\n",
        "    # Print the metrics\n",
        "    print(f\"\\n--- {label} set metrics ---\")\n",
        "    print(f\"R²     : {r2:.4f}\")\n",
        "    print(f\"Adj-R² : {adj_r2:.4f}\")\n",
        "    print(f\"MSE    : {mse:.6f}\")\n",
        "    print(f\"RMSE   : {rmse:.6f}\")\n",
        "    print(f\"NRMSE  : {nrmse:.6f}\")\n",
        "    print(f\"std_er : {std_er:.6f}\")\n",
        "    print(f\"MAE    : {mae:.6f}\")\n",
        "    print(f\"MAPE   : {mape:.6f}\")\n",
        "    print(f\"MedAE  : {medae:.6f}\")\n",
        "    print(f\"EVS    : {evs:.6f}\")\n",
        "\n",
        "    return r2, adj_r2, mse, rmse, nrmse, std_er, mae, mape, medae, evs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wv8Ur-_jsDi8",
      "metadata": {
        "id": "Wv8Ur-_jsDi8",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# **Target1: Recovery**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KdrziX7-yxO5",
      "metadata": {
        "id": "KdrziX7-yxO5",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## ANN-Recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jmiTMHpmy3Di",
      "metadata": {
        "id": "jmiTMHpmy3Di"
      },
      "outputs": [],
      "source": [
        "xtrain = df.copy()\n",
        "xtest = ts.copy()\n",
        "# Recovery%\n",
        "ytrain = df_label1.copy()\n",
        "ytest = ts_label1.copy()\n",
        "pd.concat([xtrain, ytrain]).describe().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HW1vc0T9pWoW",
      "metadata": {
        "id": "HW1vc0T9pWoW"
      },
      "source": [
        "**tuning the hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xEi4YnFNoD0g",
      "metadata": {
        "collapsed": true,
        "id": "xEi4YnFNoD0g",
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GuoTCXj39Agb",
      "metadata": {
        "collapsed": true,
        "id": "GuoTCXj39Agb",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow.keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XskCEE_AA9y1",
      "metadata": {
        "collapsed": true,
        "id": "XskCEE_AA9y1",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# --- Fix seeds for reproducibility ---\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# -- Normalizer ---\n",
        "normalizer = keras.layers.Normalization()\n",
        "normalizer.adapt(np.array(xtrain))\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(normalizer)\n",
        "\n",
        "    # Tune number of hidden layers (1 to 3)\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        model.add(layers.Dense(\n",
        "            units=hp.Int(f'units_{i}', min_value=16, max_value=128, step=8),\n",
        "            activation='relu'\n",
        "        ))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    # Tune learning rate\n",
        "    learning_rate = hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3, 1e-4])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='mean_absolute_error'\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Instantiate tuner (set seed here too)\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='tuner_dir',\n",
        "    project_name='ann_tuning',\n",
        "    overwrite=True,  # ensures consistency between runs\n",
        "    seed=SEED        # ensures deterministic search\n",
        ")\n",
        "\n",
        "# Search best hyperparameters\n",
        "tuner.search(xtrain, ytrain,\n",
        "             epochs=50,\n",
        "             validation_split=0.2,\n",
        "             verbose=1)\n",
        "\n",
        "# Retrieve best model and hyperparameters\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(\"Best number of layers:\", best_hps.get('num_layers'))\n",
        "for i in range(best_hps.get('num_layers')):\n",
        "    print(f\"Units in layer {i}:\", best_hps.get(f'units_{i}'))\n",
        "print(\"Best learning rate:\", best_hps.get('learning_rate'))\n",
        "\n",
        "# Train best model\n",
        "best_model.fit(xtrain, ytrain, epochs=100, validation_split=0.2, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uJsuWp2Zz_w4",
      "metadata": {
        "id": "uJsuWp2Zz_w4"
      },
      "outputs": [],
      "source": [
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CsOnZ1FT0Ckx",
      "metadata": {
        "id": "CsOnZ1FT0Ckx"
      },
      "outputs": [],
      "source": [
        "best_model.compile(optimizer = keras.optimizers.Adam(), #keras.optimizers.Adam change if needed\n",
        "              loss= 'mean_absolute_error')\n",
        "best_model.fit(xtrain, ytrain, verbose=0, epochs=101)\n",
        "\n",
        "# predctions\n",
        "y_pred_test_ann1 = best_model.predict(xtest).flatten()\n",
        "y_pred_train_ann1 = best_model.predict(xtrain).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VhAV7k0L7YK0",
      "metadata": {
        "id": "VhAV7k0L7YK0"
      },
      "outputs": [],
      "source": [
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_ann1.flatten()\n",
        "test_vals = y_pred_test_ann1.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_ann1\": train_vals,\n",
        "    \"y_pred_test_ann1\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"ann1_predictions.xlsx\", sheet_name=\"ann1\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'ann1_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Evh38s170Pfx",
      "metadata": {
        "id": "Evh38s170Pfx"
      },
      "outputs": [],
      "source": [
        "evaluate(ytrain, y_pred_train_ann1, \"Train\")\n",
        "evaluate(ytest, y_pred_test_ann1, \"Test\")\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(ytest, y_pred_test_ann1, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(ytest), np.min(y_pred_test_ann1))\n",
        "mx = max(np.max(ytest), np.max(y_pred_test_ann1))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual Recovery\")\n",
        "plt.ylabel(\"Predicted Recvery\")\n",
        "plt.title(\"ANN: Predicted vs Actual (Test)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iI5J66EidAFF",
      "metadata": {
        "id": "iI5J66EidAFF",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## TabPFN-Recovery"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CIjzwjDx2lN3",
      "metadata": {
        "id": "CIjzwjDx2lN3"
      },
      "source": [
        "**Installing TabPFN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PfcirH5RdAie",
      "metadata": {
        "collapsed": true,
        "id": "PfcirH5RdAie",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install tabpfn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XxZaxeff2wpw",
      "metadata": {
        "id": "XxZaxeff2wpw"
      },
      "source": [
        "**Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FVBqSYJodHL9",
      "metadata": {
        "id": "FVBqSYJodHL9"
      },
      "outputs": [],
      "source": [
        "from tabpfn import TabPFNRegressor\n",
        "\n",
        "X_train = df.copy().values\n",
        "y_train = df_label1.copy().values\n",
        "\n",
        "X_test = ts.copy().values\n",
        "y_test = ts_label1.copy().values # Recovery\n",
        "\n",
        "# Initialize the regressor\n",
        "regressor = TabPFNRegressor()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_test_tab1 = regressor.predict(X_test)\n",
        "y_pred_train_tab1 = regressor.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bNtRVWMw25M1",
      "metadata": {
        "id": "bNtRVWMw25M1"
      },
      "source": [
        "**Saving the predictions to an Excel file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JyIk4Fmx1E-h",
      "metadata": {
        "id": "JyIk4Fmx1E-h"
      },
      "outputs": [],
      "source": [
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_tab1.flatten()\n",
        "test_vals = y_pred_test_tab1.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_tab1\": train_vals,\n",
        "    \"y_pred_test_tab1\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"tab1_predictions.xlsx\", sheet_name=\"tab1\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'tab1_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HuDFqYa06-TS",
      "metadata": {
        "id": "HuDFqYa06-TS"
      },
      "source": [
        "**Plot and metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CPhRIbhll3x9",
      "metadata": {
        "id": "CPhRIbhll3x9"
      },
      "outputs": [],
      "source": [
        "evaluate(y_train, y_pred_train_tab1, \"Train\")\n",
        "evaluate(y_test, y_pred_test_tab1, \"Test\")\n",
        "\n",
        "# Plot Predicted vs Actual\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred_test_tab1, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(y_test), np.min(y_pred_test_tab1))\n",
        "mx = max(np.max(y_test), np.max(y_pred_test_tab1))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual Recovery%\")\n",
        "plt.ylabel(\"Predicted Recovery%\")\n",
        "plt.title(\"TabPFN: Predicted vs Actual (Test)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KEKTIloTqOGL",
      "metadata": {
        "id": "KEKTIloTqOGL",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-XGBoost-Recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f6295c-c568-44bb-b430-dbcbf7423a16",
      "metadata": {
        "collapsed": true,
        "id": "e0f6295c-c568-44bb-b430-dbcbf7423a16",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install pyswarms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15f2ac2-b9b1-4dfa-a419-208a1718943e",
      "metadata": {
        "id": "c15f2ac2-b9b1-4dfa-a419-208a1718943e"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PSO-tuned XGBoost Regression\n",
        "- Uses pyswarms GlobalBestPSO to minimize CV MSE\n",
        "- Tunes: learning_rate, max_depth, n_estimators, subsample, colsample_bytree, gamma\n",
        "\"\"\"\n",
        "from xgboost import XGBRegressor\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Load data and prepare\n",
        "# ---------------------------\n",
        "\n",
        "y_train = df_label1.copy().values # Recovery\n",
        "X_train = df.copy().values        # features (scaled)\n",
        "\n",
        "y_test = ts_label1.copy().values\n",
        "X_test = ts.copy().values\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PSO fitness function\n",
        "# ---------------------------\n",
        "# We'll tune 6 parameters:\n",
        "# idx 0 -> learning_rate in [0.001, 0.3]\n",
        "# idx 1 -> max_depth in [1, 10] (int)\n",
        "# idx 2 -> n_estimators in [50, 1000] (int)\n",
        "# idx 3 -> subsample in [0.3, 1.0]\n",
        "# idx 4 -> colsample_bytree in [0.3, 1.0]\n",
        "# idx 5 -> gamma in [0.0, 5.0]\n",
        "\n",
        "# bounds arrays for pyswarms\n",
        "lower_bounds = np.array([0.001, 1, 50, 0.3, 0.3, 0.0])\n",
        "upper_bounds = np.array([0.3,   10, 1000, 1.0, 1.0, 5.0])\n",
        "\n",
        "def _params_from_particle(particle):\n",
        "    \"\"\"Map a single particle vector to XGBoost params (with casts).\"\"\"\n",
        "    learning_rate = float(particle[0])\n",
        "    max_depth = int(np.round(particle[1]))\n",
        "    max_depth = max(1, max_depth)\n",
        "    n_estimators = int(np.round(particle[2]))\n",
        "    n_estimators = max(1, n_estimators)\n",
        "    subsample = float(particle[3])\n",
        "    colsample_bytree = float(particle[4])\n",
        "    gamma = float(particle[5])\n",
        "\n",
        "    params = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"max_depth\": max_depth,\n",
        "        \"n_estimators\": n_estimators,\n",
        "        \"subsample\": subsample,\n",
        "        \"colsample_bytree\": colsample_bytree,\n",
        "        \"gamma\": gamma,\n",
        "        \"random_state\": RANDOM_STATE,\n",
        "        \"verbosity\": 0,\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "        # 'tree_method':'hist' could speed up on large data (optional)\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    particles: array shape (n_particles, dims)\n",
        "    returns: array shape (n_particles,) of CV MSE (we minimize)\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "    # We use 3-fold CV on training set and return mean MSE\n",
        "    for i in range(n_particles):\n",
        "        p = particles[i]\n",
        "        params = _params_from_particle(p)\n",
        "        try:\n",
        "            model = XGBRegressor(**params)\n",
        "            # negative MSE returned -> convert to positive MSE\n",
        "            cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1)\n",
        "            mse = -np.mean(cv_scores)\n",
        "            # small regularization to prefer simpler models: add tiny penalty for large n_estimators / depth\n",
        "            penalty = 1e-6 * (params[\"n_estimators\"] * params[\"max_depth\"])\n",
        "            scores[i] = mse + penalty\n",
        "        except Exception as e:\n",
        "            # something went wrong for this particle: give a large penalty\n",
        "            scores[i] = 1e10\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Run PSO\n",
        "# ---------------------------\n",
        "# PSO settings\n",
        "options = {\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7}\n",
        "\n",
        "# dimension equals number of hyperparams\n",
        "dimensions = 6\n",
        "# initialize optimizer\n",
        "optimizer = GlobalBestPSO(\n",
        "    n_particles=16,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=(lower_bounds, upper_bounds),\n",
        "    # ftol, etc. could be set; keep defaults\n",
        ")\n",
        "\n",
        "print(\"Starting PSO optimization (this may take some minutes depending on data size)...\")\n",
        "# run optimization: choose moderate iterations to balance runtime vs search\n",
        "cost, pos = optimizer.optimize(fitness_function, iters=30, verbose=True)\n",
        "print(\"PSO finished. best cost (CV MSE):\", cost)\n",
        "print(\"Best raw particle position:\", pos)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train final XGBoost with best params\n",
        "# ---------------------------\n",
        "best_params = _params_from_particle(pos)\n",
        "print(\"\\nBest hyperparameters (rounded/casted):\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "final_model = XGBRegressor(**best_params)\n",
        "final_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1I1kmK3EoBf",
      "metadata": {
        "id": "e1I1kmK3EoBf"
      },
      "outputs": [],
      "source": [
        "y_pred_test_xg1 = final_model.predict(X_test)    # \"xg\" is the model and \"1\" is the target which is \"Recovery%\"\n",
        "y_pred_train_xg1 = final_model.predict(X_train)\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_xg1.flatten()\n",
        "test_vals = y_pred_test_xg1.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_xg1\": train_vals,\n",
        "    \"y_pred_test_xg1\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"xg1_predictions.xlsx\", sheet_name=\"xg1\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'xg1_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fe8W1nQxgiM3",
      "metadata": {
        "id": "Fe8W1nQxgiM3"
      },
      "outputs": [],
      "source": [
        "evaluate(y_train, y_pred_train_xg1, \"Train\")\n",
        "evaluate(y_test, y_pred_test_xg1, \"Test\")\n",
        "\n",
        "# Plot Predicted vs Actual\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred_test_xg1, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(y_test), np.min(y_pred_test_xg1))\n",
        "mx = max(np.max(y_test), np.max(y_pred_test_xg1))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual Recovery%\")\n",
        "plt.ylabel(\"Predicted Recovery%\")\n",
        "plt.title(\"PSO-XGBoost: Predicted vs Actual (Test)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yf7M3eot5Xja",
      "metadata": {
        "id": "yf7M3eot5Xja",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-GBR-Recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EWtio1zSKUdw",
      "metadata": {
        "collapsed": true,
        "id": "EWtio1zSKUdw",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install pyswarms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ABCq5DrCsFNg",
      "metadata": {
        "id": "ABCq5DrCsFNg"
      },
      "outputs": [],
      "source": [
        "y_train_gbr1 = df_label1.values\n",
        "y_test_gbr1 = ts_label1.values\n",
        "X_train_gbr1 = df.values\n",
        "X_test_gbr1 = ts.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3saNeXUSwkHp",
      "metadata": {
        "id": "3saNeXUSwkHp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PSO-tuned Gradient Boosting Regression (GBR)\n",
        "- Uses pyswarms GlobalBestPSO to minimize CV MSE\n",
        "- Tunes: learning_rate, max_depth, n_estimators, subsample,\n",
        "         min_samples_split, min_samples_leaf, max_features\n",
        "\"\"\"\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Your prepared data\n",
        "# ---------------------------\n",
        "# Assumes these already exist exactly like in your code:\n",
        "# X_train, y_train, X_test, y_test\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PSO fitness function\n",
        "# ---------------------------\n",
        "# We'll tune 7 parameters:\n",
        "# idx 0 -> learning_rate      in [0.005, 0.30]        (float)\n",
        "# idx 1 -> max_depth          in [1, 8]               (int)\n",
        "# idx 2 -> n_estimators       in [50, 1000]           (int)\n",
        "# idx 3 -> subsample          in [0.50, 1.00]         (float)\n",
        "# idx 4 -> min_samples_split  in [2, 20]              (int)\n",
        "# idx 5 -> min_samples_leaf   in [1, 10]              (int)\n",
        "# idx 6 -> max_features_code  in [0.20, 1.00]         (float) -> maps to None if >=0.99, else fraction\n",
        "\n",
        "# bounds arrays for pyswarms\n",
        "lower_bounds = np.array([0.005, 1,   50, 0.50,  2,  1, 0.20])\n",
        "upper_bounds = np.array([0.300, 8, 1000, 1.00, 20, 10, 1.00])\n",
        "\n",
        "def _params_from_particle(particle):\n",
        "    \"\"\"Map a single particle vector to GBR params (with casts and clipping).\"\"\"\n",
        "    learning_rate = float(np.clip(particle[0], lower_bounds[0], upper_bounds[0]))\n",
        "    max_depth = int(np.round(np.clip(particle[1], lower_bounds[1], upper_bounds[1])))\n",
        "    n_estimators = int(np.round(np.clip(particle[2], lower_bounds[2], upper_bounds[2])))\n",
        "    subsample = float(np.clip(particle[3], lower_bounds[3], upper_bounds[3]))\n",
        "    min_samples_split = int(np.round(np.clip(particle[4], lower_bounds[4], upper_bounds[4])))\n",
        "    min_samples_leaf  = int(np.round(np.clip(particle[5], lower_bounds[5], upper_bounds[5])))\n",
        "\n",
        "    # max_features mapping: treat near-1.0 as None (i.e., use all features)\n",
        "    max_features_code = float(np.clip(particle[6], lower_bounds[6], upper_bounds[6]))\n",
        "    max_features = None if max_features_code >= 0.99 else float(max_features_code)\n",
        "\n",
        "    params = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"max_depth\": max_depth,\n",
        "        \"n_estimators\": n_estimators,\n",
        "        \"subsample\": subsample,\n",
        "        \"min_samples_split\": min_samples_split,\n",
        "        \"min_samples_leaf\": min_samples_leaf,\n",
        "        \"max_features\": max_features,\n",
        "        \"random_state\": RANDOM_STATE,\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    particles: array shape (n_particles, dims)\n",
        "    returns: array shape (n_particles,) of CV MSE (we minimize)\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "\n",
        "    for i in range(n_particles):\n",
        "        p = particles[i]\n",
        "        params = _params_from_particle(p)\n",
        "        try:\n",
        "            model = GradientBoostingRegressor(**params)\n",
        "            # negative MSE returned -> convert to positive MSE\n",
        "            cv_scores = cross_val_score(\n",
        "                model, X_train, y_train,\n",
        "                cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1\n",
        "            )\n",
        "            mse = -np.mean(cv_scores)\n",
        "\n",
        "            # tiny regularization to discourage overly large trees / ensembles\n",
        "            penalty = 1e-6 * (params[\"n_estimators\"] * params[\"max_depth\"])\n",
        "            scores[i] = mse + penalty\n",
        "        except Exception:\n",
        "            # any failure gets a large penalty\n",
        "            scores[i] = 1e10\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Run PSO\n",
        "# ---------------------------\n",
        "options = {\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7}\n",
        "dimensions = 7\n",
        "\n",
        "optimizer = GlobalBestPSO(\n",
        "    n_particles=16,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=(lower_bounds, upper_bounds),\n",
        "\n",
        ")\n",
        "\n",
        "print(\"Starting PSO optimization for GBR (minimize 3-fold CV MSE)...\")\n",
        "cost, pos = optimizer.optimize(fitness_function, iters=30, verbose=True)\n",
        "print(\"PSO finished. best cost (CV MSE):\", cost)\n",
        "print(\"Best raw particle position:\", pos)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train final GBR with best params\n",
        "# ---------------------------\n",
        "best_params = _params_from_particle(pos)\n",
        "print(\"\\nBest hyperparameters (rounded/casted):\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "final_model = GradientBoostingRegressor(**best_params)\n",
        "final_model.fit(X_train_gbr1, y_train_gbr1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pyjUNaAWMy5P",
      "metadata": {
        "id": "pyjUNaAWMy5P"
      },
      "outputs": [],
      "source": [
        "y_pred_test_gbr1 = final_model.predict(X_test_gbr1)\n",
        "y_pred_train_gbr1 = final_model.predict(X_train_gbr1)\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_gbr1.flatten()\n",
        "test_vals = y_pred_test_gbr1.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_gbr1\": train_vals,\n",
        "    \"y_pred_test_gbr1\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"gbr1_predictions.xlsx\", sheet_name=\"gbr1\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'gbr1_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2KUrfEBr9Sp",
      "metadata": {
        "id": "b2KUrfEBr9Sp"
      },
      "outputs": [],
      "source": [
        "evaluate(df_label1, y_pred_train_gbr1, \"Train\")\n",
        "evaluate(ts_label1, y_pred_test_gbr1, \"Test\")\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(ts_label1, y_pred_test_gbr1, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([ts_label1.min(), ts_label1.max()], [ts_label1.min(), ts_label1.max()], 'r--', lw=2)\n",
        "plt.title(\"GBR: Predicted vs Actual (Test Set)\")\n",
        "plt.xlabel(\"Actual Recovery%\")\n",
        "plt.ylabel(\"Predicted Recovery%\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2xVSXfujlcOT",
      "metadata": {
        "id": "2xVSXfujlcOT",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-CatBoost-Recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63SZzu58luTK",
      "metadata": {
        "id": "63SZzu58luTK"
      },
      "outputs": [],
      "source": [
        "!pip install pyswarms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t4wOKuU0pLBx",
      "metadata": {
        "id": "t4wOKuU0pLBx"
      },
      "outputs": [],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lKodl-skmPsC",
      "metadata": {
        "id": "lKodl-skmPsC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PSO-tuned CatBoost Regression\n",
        "- Uses pyswarms GlobalBestPSO to minimize CV MSE\n",
        "- Tunes: learning_rate, depth, iterations, subsample, rsm, l2_leaf_reg\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Data (same as your XGB block)\n",
        "# ---------------------------\n",
        "y_train = df_label1.copy().values  # Recovery\n",
        "X_train = df.copy().values         # features (scaled)\n",
        "\n",
        "y_test  = ts_label1.copy().values\n",
        "X_test  = ts.copy().values\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PSO fitness function\n",
        "# ---------------------------\n",
        "# dims:\n",
        "# 0 -> learning_rate      [0.001, 0.3]\n",
        "# 1 -> depth              [1, 10] (int)\n",
        "# 2 -> iterations         [50, 1000] (int)\n",
        "# 3 -> subsample          [0.3, 1.0]\n",
        "# 4 -> rsm                [0.3, 1.0]  (feature subsampling)\n",
        "# 5 -> l2_leaf_reg        [0.0, 10.0]\n",
        "\n",
        "lower_bounds = np.array([0.001, 1,   50, 0.3, 0.3, 0.0])\n",
        "upper_bounds = np.array([0.300, 10, 1000, 1.0, 1.0,10.0])\n",
        "\n",
        "def _params_from_particle(p):\n",
        "    \"\"\"Map a particle to CatBoost params (cast + clip).\"\"\"\n",
        "    learning_rate = float(np.clip(p[0], lower_bounds[0], upper_bounds[0]))\n",
        "    depth         = int(np.round(np.clip(p[1], lower_bounds[1], upper_bounds[1])))\n",
        "    iterations    = int(np.round(np.clip(p[2], lower_bounds[2], upper_bounds[2])))\n",
        "    subsample     = float(np.clip(p[3], lower_bounds[3], upper_bounds[3]))\n",
        "    rsm           = float(np.clip(p[4], lower_bounds[4], upper_bounds[4]))\n",
        "    l2_leaf_reg   = float(np.clip(p[5], lower_bounds[5], upper_bounds[5]))\n",
        "\n",
        "    params = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"depth\": depth,\n",
        "        \"iterations\": iterations,\n",
        "        \"subsample\": subsample,\n",
        "        \"rsm\": rsm,\n",
        "        \"l2_leaf_reg\": l2_leaf_reg,\n",
        "        \"loss_function\": \"RMSE\",\n",
        "        \"random_seed\": RANDOM_STATE,\n",
        "        \"verbose\": False,\n",
        "        \"allow_writing_files\": False,\n",
        "        \"bootstrap_type\": \"Bernoulli\",   # enables 'subsample'\n",
        "        \"task_type\": \"CPU\"\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    particles: (n_particles, dims)\n",
        "    returns:   (n_particles,) mean CV MSE (to minimize)\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "\n",
        "    for i in range(n_particles):\n",
        "        p = particles[i]\n",
        "        params = _params_from_particle(p)\n",
        "        try:\n",
        "            model = CatBoostRegressor(**params)\n",
        "            # cross_val_score returns neg MSE -> take positive MSE\n",
        "            cv_scores = cross_val_score(\n",
        "                model, X_train, y_train,\n",
        "                cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1\n",
        "            )\n",
        "            mse = -np.mean(cv_scores)\n",
        "            # tiny complexity penalty (iterations * depth)\n",
        "            penalty = 1e-6 * (params[\"iterations\"] * params[\"depth\"])\n",
        "            scores[i] = mse + penalty\n",
        "        except Exception:\n",
        "            scores[i] = 1e10\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Run PSO\n",
        "# ---------------------------\n",
        "options = {\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7}\n",
        "dimensions = 6\n",
        "\n",
        "optimizer = GlobalBestPSO(\n",
        "    n_particles=16,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=(lower_bounds, upper_bounds),\n",
        ")\n",
        "\n",
        "print(\"Starting PSO optimization for CatBoost (minimize 3-fold CV MSE)...\")\n",
        "cost, pos = optimizer.optimize(fitness_function, iters=30, verbose=True)\n",
        "print(\"PSO finished. best cost (CV MSE):\", cost)\n",
        "print(\"Best raw particle position:\", pos)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train final CatBoost with best params\n",
        "# ---------------------------\n",
        "best_params = _params_from_particle(pos)\n",
        "print(\"\\nBest hyperparameters (rounded/casted):\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "final_model = CatBoostRegressor(**best_params)\n",
        "final_model.fit(X_train, y_train, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f647cfd5-cd1b-4d5d-8582-a82eb7d16471",
      "metadata": {
        "id": "f647cfd5-cd1b-4d5d-8582-a82eb7d16471"
      },
      "outputs": [],
      "source": [
        "print(\"CatBoostRegressor(\", end=\"\")\n",
        "for i, (k, v) in enumerate(final_model.get_params().items()):\n",
        "    comma = \",\" if i < len(final_model.get_params()) - 1 else \"\"\n",
        "    print(f\"{k}={repr(v)}{comma}\")\n",
        "print(\")\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d85421-0882-48d8-b84f-016250f23c8d",
      "metadata": {
        "id": "47d85421-0882-48d8-b84f-016250f23c8d"
      },
      "outputs": [],
      "source": [
        "y_pred_test_cb1 = final_model.predict(X_test)\n",
        "y_pred_train_cb1 = final_model.predict(X_train)\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_cb1.flatten()\n",
        "test_vals = y_pred_test_cb1.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_cb1\": train_vals,\n",
        "    \"y_pred_test_cb1\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"cb1_predictions.xlsx\", sheet_name=\"cb1\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'cb1_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e0711c-c971-41fd-b919-5d52b164dbf7",
      "metadata": {
        "id": "d6e0711c-c971-41fd-b919-5d52b164dbf7"
      },
      "outputs": [],
      "source": [
        "evaluate(y_train, y_pred_train_cb1, \"Train\")\n",
        "evaluate(y_test, y_pred_test_cb1, \"Test\")\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred_test_cb1, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title(\"CatBoost: Predicted vs Actual (Test Set)\")\n",
        "plt.xlabel(\"Actual Recovery%\")\n",
        "plt.ylabel(\"Predicted Recovery%\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64b3f088-9f4e-4187-8cf4-e88b6c9a1b41",
      "metadata": {
        "id": "64b3f088-9f4e-4187-8cf4-e88b6c9a1b41",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-AdaBoost-Recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c26592d4-a38a-4b56-9929-50b4ca7906f8",
      "metadata": {
        "id": "c26592d4-a38a-4b56-9929-50b4ca7906f8"
      },
      "outputs": [],
      "source": [
        "!pip install pyswarms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de6ba630-9447-498b-b878-bda29500e07f",
      "metadata": {
        "id": "de6ba630-9447-498b-b878-bda29500e07f"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PSO-tuned AdaBoost Regression\n",
        "- Uses pyswarms GlobalBestPSO to minimize CV MSE\n",
        "- Tunes: learning_rate, n_estimators, max_depth (of base tree),\n",
        "         min_samples_leaf (of base tree), loss\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Data (same pattern as yours)\n",
        "# ---------------------------\n",
        "y_train = df_label1.copy().values  # e.g., Recovery\n",
        "X_train = df.copy().values\n",
        "y_test  = ts_label1.copy().values\n",
        "X_test  = ts.copy().values\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PSO fitness function\n",
        "# ---------------------------\n",
        "# dims:\n",
        "# 0 -> learning_rate       [0.005, 1.0]\n",
        "# 1 -> n_estimators        [50, 1000] (int)\n",
        "# 2 -> max_depth           [1, 10]    (int) for base DecisionTree\n",
        "# 3 -> min_samples_leaf    [1, 20]    (int) for base DecisionTree\n",
        "# 4 -> loss_code           [0, 2]     (int) -> 0:'linear', 1:'square', 2:'exponential'\n",
        "\n",
        "lower_bounds = np.array([0.005,   50,  1,  1, 0])\n",
        "upper_bounds = np.array([1.000, 1000, 10, 20, 2])\n",
        "\n",
        "_LOSSES = ['linear', 'square', 'exponential']\n",
        "\n",
        "def _params_from_particle(p):\n",
        "    \"\"\"Map a particle to AdaBoost params (cast + clip).\"\"\"\n",
        "    learning_rate    = float(np.clip(p[0], lower_bounds[0], upper_bounds[0]))\n",
        "    n_estimators     = int(np.round(np.clip(p[1], lower_bounds[1], upper_bounds[1])))\n",
        "    max_depth        = int(np.round(np.clip(p[2], lower_bounds[2], upper_bounds[2])))\n",
        "    min_samples_leaf = int(np.round(np.clip(p[3], lower_bounds[3], upper_bounds[3])))\n",
        "    loss_code        = int(np.round(np.clip(p[4], lower_bounds[4], upper_bounds[4])))\n",
        "    loss             = _LOSSES[loss_code]\n",
        "\n",
        "    # Base learner\n",
        "    base_tree = DecisionTreeRegressor(\n",
        "        max_depth=max_depth,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    params = {\n",
        "        \"estimator\": base_tree,          # sklearn >= 1.2\n",
        "        \"n_estimators\": n_estimators,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"loss\": loss,\n",
        "        \"random_state\": RANDOM_STATE\n",
        "    }\n",
        "    return params, dict(\n",
        "        learning_rate=learning_rate, n_estimators=n_estimators,\n",
        "        max_depth=max_depth, min_samples_leaf=min_samples_leaf, loss=loss\n",
        "    )\n",
        "\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    particles: (n_particles, dims)\n",
        "    returns:   (n_particles,) mean CV MSE (to minimize)\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "\n",
        "    for i in range(n_particles):\n",
        "        p = particles[i]\n",
        "        params, flat = _params_from_particle(p)\n",
        "        try:\n",
        "            model = AdaBoostRegressor(**params)\n",
        "            # cross_val_score returns neg MSE -> convert to +MSE\n",
        "            cv_scores = cross_val_score(\n",
        "                model, X_train, y_train.ravel(),\n",
        "                cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1\n",
        "            )\n",
        "            mse = -np.mean(cv_scores)\n",
        "            # tiny complexity penalty to prefer simpler ensembles\n",
        "            penalty = 1e-6 * (flat[\"n_estimators\"] * flat[\"max_depth\"])\n",
        "            scores[i] = mse + penalty\n",
        "        except Exception:\n",
        "            scores[i] = 1e10\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Run PSO\n",
        "# ---------------------------\n",
        "options = {\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7}\n",
        "dimensions = 5\n",
        "\n",
        "optimizer = GlobalBestPSO(\n",
        "    n_particles=16,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=(lower_bounds, upper_bounds),\n",
        ")\n",
        "\n",
        "print(\"Starting PSO optimization for AdaBoost (minimize 3-fold CV MSE)...\")\n",
        "cost, pos = optimizer.optimize(fitness_function, iters=30, verbose=True)\n",
        "print(\"PSO finished. best cost (CV MSE):\", cost)\n",
        "print(\"Best raw particle position:\", pos)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train final AdaBoost with best params\n",
        "# ---------------------------\n",
        "best_params, best_flat = _params_from_particle(pos)\n",
        "print(\"\\nBest hyperparameters (rounded/casted):\")\n",
        "for k, v in best_flat.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "final_model = AdaBoostRegressor(**best_params)\n",
        "final_model.fit(X_train, y_train.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f0b18e-2935-4f17-a380-6d8c25717c9f",
      "metadata": {
        "id": "47f0b18e-2935-4f17-a380-6d8c25717c9f"
      },
      "outputs": [],
      "source": [
        "print(\"AdaBoostRegressor(\", end=\"\")\n",
        "for i, (k, v) in enumerate(final_model.get_params().items()):\n",
        "    comma = \",\" if i < len(final_model.get_params()) - 1 else \"\"\n",
        "    print(f\"{k}={repr(v)}{comma}\")\n",
        "print(\")\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0ee4bc4-240e-416d-873d-ec0dc2003734",
      "metadata": {
        "id": "a0ee4bc4-240e-416d-873d-ec0dc2003734"
      },
      "outputs": [],
      "source": [
        "y_pred_test_adb1 = final_model.predict(X_test)\n",
        "y_pred_train_adb1 = final_model.predict(X_train)\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_adb1.flatten()\n",
        "test_vals = y_pred_test_adb1.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_adb1\": train_vals,\n",
        "    \"y_pred_test_adb1\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"adb1_predictions.xlsx\", sheet_name=\"adb1\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'adb1_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b560518f-3dce-4377-8725-284ce2749be3",
      "metadata": {
        "id": "b560518f-3dce-4377-8725-284ce2749be3"
      },
      "outputs": [],
      "source": [
        "evaluate(y_train, y_pred_train_adb1, \"Train\")\n",
        "evaluate(y_test, y_pred_test_adb1, \"Test\")\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred_test_adb1, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title(\"AdaBoost: Predicted vs Actual (Test Set)\")\n",
        "plt.xlabel(\"Actual Recovery%\")\n",
        "plt.ylabel(\"Predicted Recovery%\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sKl_o8eWQlcX",
      "metadata": {
        "id": "sKl_o8eWQlcX",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# **Target2: SEC**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ho-atkgGV1mP",
      "metadata": {
        "id": "ho-atkgGV1mP",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## ANN-SEC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CBty_c8OV1mP",
      "metadata": {
        "id": "CBty_c8OV1mP"
      },
      "outputs": [],
      "source": [
        "xtrain_ann2 = df.copy()\n",
        "xtest_ann2 = ts.copy()\n",
        "# SEC\n",
        "ytrain_ann2 = df_label2.copy()\n",
        "ytest_ann2 = ts_label2.copy()\n",
        "pd.concat([xtrain_ann2, ytrain_ann2]).describe().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rGrTpAxJV1mQ",
      "metadata": {
        "id": "rGrTpAxJV1mQ"
      },
      "source": [
        "**tuning the hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8-Fn4PDSV1mQ",
      "metadata": {
        "collapsed": true,
        "id": "8-Fn4PDSV1mQ",
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nkJubMW0V1mR",
      "metadata": {
        "id": "nkJubMW0V1mR",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# --- Fix seeds for reproducibility ---\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# -- Normalizer ---\n",
        "normalizer = keras.layers.Normalization()\n",
        "normalizer.adapt(np.array(xtrain_ann2))\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(normalizer)\n",
        "\n",
        "    # Tune number of hidden layers (1 to 3)\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        model.add(layers.Dense(\n",
        "            units=hp.Int(f'units_{i}', min_value=16, max_value=128, step=8),\n",
        "            activation='relu'\n",
        "        ))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    # Tune learning rate\n",
        "    learning_rate = hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3, 1e-4])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='mean_absolute_error'\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Instantiate tuner (set seed here too)\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='tuner_dir',\n",
        "    project_name='ann_tuning',\n",
        "    overwrite=True,  # ensures consistency between runs\n",
        "    seed=SEED        # ensures deterministic search\n",
        ")\n",
        "\n",
        "# Search best hyperparameters\n",
        "tuner.search(xtrain_ann2, ytrain_ann2,\n",
        "             epochs=50,\n",
        "             validation_split=0.2,\n",
        "             verbose=1)\n",
        "\n",
        "# Retrieve best model and hyperparameters\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(\"Best number of layers:\", best_hps.get('num_layers'))\n",
        "for i in range(best_hps.get('num_layers')):\n",
        "    print(f\"Units in layer {i}:\", best_hps.get(f'units_{i}'))\n",
        "print(\"Best learning rate:\", best_hps.get('learning_rate'))\n",
        "\n",
        "# Train best model\n",
        "best_model.fit(xtrain_ann2, ytrain_ann2, epochs=100, validation_split=0.2, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "byZSivOAV1mR",
      "metadata": {
        "id": "byZSivOAV1mR"
      },
      "outputs": [],
      "source": [
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nb2ZowJoV1mR",
      "metadata": {
        "id": "Nb2ZowJoV1mR"
      },
      "outputs": [],
      "source": [
        "best_model.compile(optimizer = keras.optimizers.Adam(),\n",
        "              loss= 'mean_absolute_error')\n",
        "best_model.fit(xtrain_ann2, ytrain_ann2, verbose=0, epochs=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76nxsHO_Oju7",
      "metadata": {
        "id": "76nxsHO_Oju7"
      },
      "outputs": [],
      "source": [
        "y_pred_test_ann2 = best_model.predict(xtest_ann2).flatten()\n",
        "y_pred_train_ann2 = best_model.predict(xtrain_ann2).flatten()\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_ann2.flatten()\n",
        "test_vals = y_pred_test_ann2.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_ann2\": train_vals,\n",
        "    \"y_pred_test_ann2\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"ann2_predictions.xlsx\", sheet_name=\"ann2\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'ann2_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0poBV6fdV1mS",
      "metadata": {
        "id": "0poBV6fdV1mS"
      },
      "outputs": [],
      "source": [
        "evaluate(ytrain_ann2, y_pred_train_ann2, \"Train\")\n",
        "evaluate(ytest_ann2, y_pred_test_ann2, \"Test\")\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(ytest_ann2, y_pred_test_ann2, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(ytest_ann2), np.min(y_pred_test_ann2))\n",
        "mx = max(np.max(ytest_ann2), np.max(y_pred_test_ann2))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual SEC\")\n",
        "plt.ylabel(\"Predicted SEC\")\n",
        "plt.title(\"ANN: Predicted vs Actual (Test)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521mM8KzUEk7",
      "metadata": {
        "id": "521mM8KzUEk7",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## TabPFN-SEC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JEGDKth3UFFZ",
      "metadata": {
        "collapsed": true,
        "id": "JEGDKth3UFFZ",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install tabpfn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KAr3Fj6JUFhC",
      "metadata": {
        "id": "KAr3Fj6JUFhC"
      },
      "outputs": [],
      "source": [
        "from tabpfn import TabPFNRegressor\n",
        "\n",
        "X_train_tab2 = df.copy()\n",
        "y_train_tab2 = df_label2.copy()\n",
        "\n",
        "X_test_tab2 = ts.copy()\n",
        "y_test_tab2 = ts_label2.copy()\n",
        "\n",
        "# Initialize the regressor\n",
        "regressor = TabPFNRegressor()\n",
        "regressor.fit(X_train_tab2, y_train_tab2)\n",
        "\n",
        "# Predict on the test/train set\n",
        "y_pred_test_tab2 = regressor.predict(X_test_tab2)\n",
        "y_pred_train_tab2 = regressor.predict(X_train_tab2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335aLpPeQ_ag",
      "metadata": {
        "id": "335aLpPeQ_ag"
      },
      "outputs": [],
      "source": [
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_tab2.flatten()\n",
        "test_vals = y_pred_test_tab2.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_tab2\": train_vals,\n",
        "    \"y_pred_test_tab2\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"tab2_predictions.xlsx\", sheet_name=\"tab2\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'tab2_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9hQiY1cLBKyJ",
      "metadata": {
        "id": "9hQiY1cLBKyJ"
      },
      "outputs": [],
      "source": [
        "evaluate(y_train_tab2, y_pred_train_tab2, \"Train\")\n",
        "evaluate(y_test_tab2, y_pred_test_tab2, \"Test\")\n",
        "\n",
        "# Plot Predicted vs Actual\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test_tab2, y_pred_test_tab2, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(y_test_tab2), np.min(y_pred_test_tab2))\n",
        "mx = max(np.max(y_test_tab2), np.max(y_pred_test_tab2))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual SEC\")\n",
        "plt.ylabel(\"Predicted SEC\")\n",
        "plt.title(\"TabPFN: Predicted vs Actual (Test)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FJ8zmb4nQ_wP",
      "metadata": {
        "id": "FJ8zmb4nQ_wP",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-XGBoost-SEC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "msQBE7MbR-LG",
      "metadata": {
        "collapsed": true,
        "id": "msQBE7MbR-LG",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install pyswarms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdmxjOiFQl4u",
      "metadata": {
        "id": "cdmxjOiFQl4u"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PSO-tuned XGBoost Regression\n",
        "- Uses pyswarms GlobalBestPSO to minimize CV MSE\n",
        "- Tunes: learning_rate, max_depth, n_estimators, subsample, colsample_bytree, gamma\n",
        "\"\"\"\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Load data and prepare\n",
        "# ---------------------------\n",
        "\n",
        "y_train_xg2 = df_label2.copy() # SEC\n",
        "X_train_xg2 = df.copy()        # features (scaled)\n",
        "\n",
        "y_test_xg2 = ts_label2.copy()\n",
        "X_test_xg2 = ts.copy()\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PSO fitness function\n",
        "# ---------------------------\n",
        "# We'll tune 6 parameters:\n",
        "# idx 0 -> learning_rate in [0.001, 0.3]\n",
        "# idx 1 -> max_depth in [1, 10] (int)\n",
        "# idx 2 -> n_estimators in [50, 1000] (int)\n",
        "# idx 3 -> subsample in [0.3, 1.0]\n",
        "# idx 4 -> colsample_bytree in [0.3, 1.0]\n",
        "# idx 5 -> gamma in [0.0, 5.0]\n",
        "\n",
        "# bounds arrays for pyswarms\n",
        "lower_bounds = np.array([0.001, 1, 50, 0.3, 0.3, 0.0])\n",
        "upper_bounds = np.array([0.3,   10, 1000, 1.0, 1.0, 5.0])\n",
        "\n",
        "def _params_from_particle(particle):\n",
        "    \"\"\"Map a single particle vector to XGBoost params (with casts).\"\"\"\n",
        "    learning_rate = float(particle[0])\n",
        "    max_depth = int(np.round(particle[1]))\n",
        "    max_depth = max(1, max_depth)\n",
        "    n_estimators = int(np.round(particle[2]))\n",
        "    n_estimators = max(1, n_estimators)\n",
        "    subsample = float(particle[3])\n",
        "    colsample_bytree = float(particle[4])\n",
        "    gamma = float(particle[5])\n",
        "\n",
        "    params = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"max_depth\": max_depth,\n",
        "        \"n_estimators\": n_estimators,\n",
        "        \"subsample\": subsample,\n",
        "        \"colsample_bytree\": colsample_bytree,\n",
        "        \"gamma\": gamma,\n",
        "        \"random_state\": RANDOM_STATE,\n",
        "        \"verbosity\": 0,\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "        # 'tree_method':'hist' could speed up on large data (optional)\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    particles: array shape (n_particles, dims)\n",
        "    returns: array shape (n_particles,) of CV MSE (we minimize)\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "    # We use 3-fold CV on training set and return mean MSE\n",
        "    for i in range(n_particles):\n",
        "        p = particles[i]\n",
        "        params = _params_from_particle(p)\n",
        "        try:\n",
        "            model = XGBRegressor(**params)\n",
        "            # negative MSE returned -> convert to positive MSE\n",
        "            cv_scores = cross_val_score(model, X_train_xg2, y_train_xg2, cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1)\n",
        "            mse = -np.mean(cv_scores)\n",
        "            # small regularization to prefer simpler models: add tiny penalty for large n_estimators / depth\n",
        "            penalty = 1e-6 * (params[\"n_estimators\"] * params[\"max_depth\"])\n",
        "            scores[i] = mse + penalty\n",
        "        except Exception as e:\n",
        "            # something went wrong for this particle: give a large penalty\n",
        "            scores[i] = 1e10\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Run PSO\n",
        "# ---------------------------\n",
        "# PSO settings\n",
        "options = {\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7}\n",
        "\n",
        "# dimension equals number of hyperparams\n",
        "dimensions = 6\n",
        "# initialize optimizer\n",
        "optimizer = GlobalBestPSO(\n",
        "    n_particles=16,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=(lower_bounds, upper_bounds),\n",
        "    # ftol, etc. could be set; keep defaults\n",
        ")\n",
        "\n",
        "print(\"Starting PSO optimization (this may take some minutes depending on data size)...\")\n",
        "# run optimization: choose moderate iterations to balance runtime vs search\n",
        "cost, pos = optimizer.optimize(fitness_function, iters=30, verbose=True)\n",
        "print(\"PSO finished. best cost (CV MSE):\", cost)\n",
        "print(\"Best raw particle position:\", pos)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train final XGBoost with best params\n",
        "# ---------------------------\n",
        "best_params = _params_from_particle(pos)\n",
        "print(\"\\nBest hyperparameters (rounded/casted):\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "final_model = XGBRegressor(**best_params)\n",
        "final_model.fit(X_train_xg2, y_train_xg2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XT7n4UQySL5U",
      "metadata": {
        "id": "XT7n4UQySL5U"
      },
      "outputs": [],
      "source": [
        "y_pred_test_xg2 = final_model.predict(X_test_xg2)    # \"xg\" is the model and \"2\" is the targer which is \"SEC\"\n",
        "y_pred_train_xg2 = final_model.predict(X_train_xg2)\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_xg2.flatten()\n",
        "test_vals = y_pred_test_xg2.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_xg2\": train_vals,\n",
        "    \"y_pred_test_xg2\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"xg2_predictions.xlsx\", sheet_name=\"xg2\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'xg2_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sBI-0C64im5d",
      "metadata": {
        "id": "sBI-0C64im5d"
      },
      "outputs": [],
      "source": [
        "evaluate(y_train_xg2, y_pred_train_xg2, \"Train\")\n",
        "evaluate(y_test_xg2, y_pred_test_xg2, \"Test\")\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test_xg2, y_pred_test_xg2, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(y_test_xg2), np.min(y_pred_test_xg2))\n",
        "mx = max(np.max(y_test_xg2), np.max(y_pred_test_xg2))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual SEC\")\n",
        "plt.ylabel(\"Predicted SEC\")\n",
        "plt.title(\"PSO-XGBoost: Predicted vs Actual (Test)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wi18J1eoAyBn",
      "metadata": {
        "id": "wi18J1eoAyBn",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-GBR-SEC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "joeZNHBPAyBo",
      "metadata": {
        "id": "joeZNHBPAyBo"
      },
      "outputs": [],
      "source": [
        "y_train = df_label2.copy()\n",
        "y_test = ts_label2.copy()\n",
        "X_train = df.copy()\n",
        "X_test = ts.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SnnDIXDBdjCb",
      "metadata": {
        "collapsed": true,
        "id": "SnnDIXDBdjCb",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install pyswarms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ozf1zHjnGMRU",
      "metadata": {
        "id": "ozf1zHjnGMRU"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PSO-tuned Gradient Boosting Regression (GBR)\n",
        "- Uses pyswarms GlobalBestPSO to minimize CV MSE\n",
        "- Tunes: learning_rate, max_depth, n_estimators, subsample,\n",
        "         min_samples_split, min_samples_leaf, max_features\n",
        "\"\"\"\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Your prepared data\n",
        "# ---------------------------\n",
        "# Assumes these already exist exactly like in your code:\n",
        "# X_train, y_train, X_test, y_test\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PSO fitness function\n",
        "# ---------------------------\n",
        "# We'll tune 7 parameters:\n",
        "# idx 0 -> learning_rate      in [0.005, 0.30]        (float)\n",
        "# idx 1 -> max_depth          in [1, 8]               (int)\n",
        "# idx 2 -> n_estimators       in [50, 1000]           (int)\n",
        "# idx 3 -> subsample          in [0.50, 1.00]         (float)\n",
        "# idx 4 -> min_samples_split  in [2, 20]              (int)\n",
        "# idx 5 -> min_samples_leaf   in [1, 10]              (int)\n",
        "# idx 6 -> max_features_code  in [0.20, 1.00]         (float) -> maps to None if >=0.99, else fraction\n",
        "\n",
        "# bounds arrays for pyswarms\n",
        "lower_bounds = np.array([0.005, 1,   50, 0.50,  2,  1, 0.20])\n",
        "upper_bounds = np.array([0.300, 8, 1000, 1.00, 20, 10, 1.00])\n",
        "\n",
        "def _params_from_particle(particle):\n",
        "    \"\"\"Map a single particle vector to GBR params (with casts and clipping).\"\"\"\n",
        "    learning_rate = float(np.clip(particle[0], lower_bounds[0], upper_bounds[0]))\n",
        "    max_depth = int(np.round(np.clip(particle[1], lower_bounds[1], upper_bounds[1])))\n",
        "    n_estimators = int(np.round(np.clip(particle[2], lower_bounds[2], upper_bounds[2])))\n",
        "    subsample = float(np.clip(particle[3], lower_bounds[3], upper_bounds[3]))\n",
        "    min_samples_split = int(np.round(np.clip(particle[4], lower_bounds[4], upper_bounds[4])))\n",
        "    min_samples_leaf  = int(np.round(np.clip(particle[5], lower_bounds[5], upper_bounds[5])))\n",
        "\n",
        "    # max_features mapping: treat near-1.0 as None (i.e., use all features)\n",
        "    max_features_code = float(np.clip(particle[6], lower_bounds[6], upper_bounds[6]))\n",
        "    max_features = None if max_features_code >= 0.99 else float(max_features_code)\n",
        "\n",
        "    params = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"max_depth\": max_depth,\n",
        "        \"n_estimators\": n_estimators,\n",
        "        \"subsample\": subsample,\n",
        "        \"min_samples_split\": min_samples_split,\n",
        "        \"min_samples_leaf\": min_samples_leaf,\n",
        "        \"max_features\": max_features,\n",
        "        \"random_state\": RANDOM_STATE,\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    particles: array shape (n_particles, dims)\n",
        "    returns: array shape (n_particles,) of CV MSE (we minimize)\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "\n",
        "    for i in range(n_particles):\n",
        "        p = particles[i]\n",
        "        params = _params_from_particle(p)\n",
        "        try:\n",
        "            model = GradientBoostingRegressor(**params)\n",
        "            # negative MSE returned -> convert to positive MSE\n",
        "            cv_scores = cross_val_score(\n",
        "                model, X_train, y_train,\n",
        "                cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1\n",
        "            )\n",
        "            mse = -np.mean(cv_scores)\n",
        "\n",
        "            # tiny regularization to discourage overly large trees / ensembles\n",
        "            penalty = 1e-6 * (params[\"n_estimators\"] * params[\"max_depth\"])\n",
        "            scores[i] = mse + penalty\n",
        "        except Exception:\n",
        "            # any failure gets a large penalty\n",
        "            scores[i] = 1e10\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Run PSO\n",
        "# ---------------------------\n",
        "options = {\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7}\n",
        "dimensions = 7\n",
        "\n",
        "optimizer = GlobalBestPSO(\n",
        "    n_particles=16,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=(lower_bounds, upper_bounds),\n",
        "\n",
        ")\n",
        "\n",
        "print(\"Starting PSO optimization for GBR (minimize 3-fold CV MSE)...\")\n",
        "cost, pos = optimizer.optimize(fitness_function, iters=30, verbose=True)\n",
        "print(\"PSO finished. best cost (CV MSE):\", cost)\n",
        "print(\"Best raw particle position:\", pos)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train final GBR with best params\n",
        "# ---------------------------\n",
        "best_params = _params_from_particle(pos)\n",
        "print(\"\\nBest hyperparameters (rounded/casted):\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "final_model = GradientBoostingRegressor(**best_params)\n",
        "final_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WC4Kv6VXYkrS",
      "metadata": {
        "id": "WC4Kv6VXYkrS"
      },
      "outputs": [],
      "source": [
        "y_pred_test_gbr2 = final_model.predict(X_test)\n",
        "y_pred_train_gbr2 = final_model.predict(X_train)\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_gbr2.flatten()\n",
        "test_vals = y_pred_test_gbr2.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_gbr2\": train_vals,\n",
        "    \"y_pred_test_gbr2\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"gbr2_predictions.xlsx\", sheet_name=\"gbr2\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'gbr2_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kDsf8ZnzGMdN",
      "metadata": {
        "id": "kDsf8ZnzGMdN"
      },
      "outputs": [],
      "source": [
        "evaluate(df_label2, y_pred_train_gbr2, \"Train\")\n",
        "evaluate(ts_label2, y_pred_test_gbr2, \"Test\")\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(ts_label2, y_pred_test_gbr2, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([ts_label2.min(), ts_label2.max()], [ts_label2.min(), ts_label2.max()], 'r--', lw=2)\n",
        "plt.title(\"GBR: Predicted vs Actual (Test Set)\")\n",
        "plt.xlabel(\"Actual SEC\")\n",
        "plt.ylabel(\"Predicted SEC\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aed70d62-bdfb-4a38-921c-11b86bdea90a",
      "metadata": {
        "id": "aed70d62-bdfb-4a38-921c-11b86bdea90a",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-CatBoost-SEC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3bbbebd-a5ea-444b-9e98-dc86d70b10f1",
      "metadata": {
        "id": "f3bbbebd-a5ea-444b-9e98-dc86d70b10f1"
      },
      "outputs": [],
      "source": [
        "!pip install pyswarms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "190c2a31-d395-4869-8781-9dc63dcac993",
      "metadata": {
        "id": "190c2a31-d395-4869-8781-9dc63dcac993"
      },
      "outputs": [],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f05c2c82-a7da-46c0-8463-1feb66bde95d",
      "metadata": {
        "id": "f05c2c82-a7da-46c0-8463-1feb66bde95d"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PSO-tuned CatBoost Regression\n",
        "- Uses pyswarms GlobalBestPSO to minimize CV MSE\n",
        "- Tunes: learning_rate, depth, iterations, subsample, rsm, l2_leaf_reg\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Data (same as your XGB block)\n",
        "# ---------------------------\n",
        "y_train = df_label2.copy().values  # Recovery\n",
        "X_train = df.copy().values         # features (scaled)\n",
        "\n",
        "y_test  = ts_label2.copy().values\n",
        "X_test  = ts.copy().values\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PSO fitness function\n",
        "# ---------------------------\n",
        "# dims:\n",
        "# 0 -> learning_rate      [0.001, 0.3]\n",
        "# 1 -> depth              [1, 10] (int)\n",
        "# 2 -> iterations         [50, 1000] (int)\n",
        "# 3 -> subsample          [0.3, 1.0]\n",
        "# 4 -> rsm                [0.3, 1.0]  (feature subsampling)\n",
        "# 5 -> l2_leaf_reg        [0.0, 10.0]\n",
        "\n",
        "lower_bounds = np.array([0.001, 1,   50, 0.3, 0.3, 0.0])\n",
        "upper_bounds = np.array([0.300, 10, 1000, 1.0, 1.0,10.0])\n",
        "\n",
        "def _params_from_particle(p):\n",
        "    \"\"\"Map a particle to CatBoost params (cast + clip).\"\"\"\n",
        "    learning_rate = float(np.clip(p[0], lower_bounds[0], upper_bounds[0]))\n",
        "    depth         = int(np.round(np.clip(p[1], lower_bounds[1], upper_bounds[1])))\n",
        "    iterations    = int(np.round(np.clip(p[2], lower_bounds[2], upper_bounds[2])))\n",
        "    subsample     = float(np.clip(p[3], lower_bounds[3], upper_bounds[3]))\n",
        "    rsm           = float(np.clip(p[4], lower_bounds[4], upper_bounds[4]))\n",
        "    l2_leaf_reg   = float(np.clip(p[5], lower_bounds[5], upper_bounds[5]))\n",
        "\n",
        "    params = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"depth\": depth,\n",
        "        \"iterations\": iterations,\n",
        "        \"subsample\": subsample,\n",
        "        \"rsm\": rsm,\n",
        "        \"l2_leaf_reg\": l2_leaf_reg,\n",
        "        \"loss_function\": \"RMSE\",\n",
        "        \"random_seed\": RANDOM_STATE,\n",
        "        \"verbose\": False,\n",
        "        \"allow_writing_files\": False,\n",
        "        \"bootstrap_type\": \"Bernoulli\",\n",
        "        \"task_type\": \"GPU\"\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    particles: (n_particles, dims)\n",
        "    returns:   (n_particles,) mean CV MSE (to minimize)\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "\n",
        "    for i in range(n_particles):\n",
        "        p = particles[i]\n",
        "        params = _params_from_particle(p)\n",
        "        try:\n",
        "            model = CatBoostRegressor(**params)\n",
        "            # cross_val_score returns neg MSE -> take positive MSE\n",
        "            cv_scores = cross_val_score(\n",
        "                model, X_train, y_train,\n",
        "                cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1\n",
        "            )\n",
        "            mse = -np.mean(cv_scores)\n",
        "            # tiny complexity penalty (iterations * depth)\n",
        "            penalty = 1e-6 * (params[\"iterations\"] * params[\"depth\"])\n",
        "            scores[i] = mse + penalty\n",
        "        except Exception:\n",
        "            scores[i] = 1e10\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Run PSO\n",
        "# ---------------------------\n",
        "options = {\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7}\n",
        "dimensions = 6\n",
        "\n",
        "optimizer = GlobalBestPSO(\n",
        "    n_particles=16,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=(lower_bounds, upper_bounds),\n",
        ")\n",
        "\n",
        "print(\"Starting PSO optimization for CatBoost (minimize 3-fold CV MSE)...\")\n",
        "cost, pos = optimizer.optimize(fitness_function, iters=30, verbose=True)\n",
        "print(\"PSO finished. best cost (CV MSE):\", cost)\n",
        "print(\"Best raw particle position:\", pos)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train final CatBoost with best params\n",
        "# ---------------------------\n",
        "best_params = _params_from_particle(pos)\n",
        "print(\"\\nBest hyperparameters (rounded/casted):\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "final_model = CatBoostRegressor(**best_params)\n",
        "# Force CPU right before training to avoid pairwise error\n",
        "final_model.set_params(task_type=\"CPU\")   # <--- add this line\n",
        "final_model.fit(X_train, y_train, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a877272-35f9-4dfa-903a-4cbf82b34016",
      "metadata": {
        "id": "4a877272-35f9-4dfa-903a-4cbf82b34016"
      },
      "outputs": [],
      "source": [
        "print(\"CatBoostRegressor(\", end=\"\")\n",
        "for i, (k, v) in enumerate(final_model.get_params().items()):\n",
        "    comma = \",\" if i < len(final_model.get_params()) - 1 else \"\"\n",
        "    print(f\"{k}={repr(v)}{comma}\")\n",
        "print(\")\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a593e5-8d00-43cd-8263-54151cc9c908",
      "metadata": {
        "id": "37a593e5-8d00-43cd-8263-54151cc9c908"
      },
      "outputs": [],
      "source": [
        "y_pred_test_cb2 = final_model.predict(X_test)\n",
        "y_pred_train_cb2 = final_model.predict(X_train)\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_cb2.flatten()\n",
        "test_vals = y_pred_test_cb2.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_cb2\": train_vals,\n",
        "    \"y_pred_test_cb2\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"cb2_predictions.xlsx\", sheet_name=\"cb2\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'cb2_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf9fb9b-f25d-49a5-88c8-8e2a28706074",
      "metadata": {
        "id": "0cf9fb9b-f25d-49a5-88c8-8e2a28706074"
      },
      "outputs": [],
      "source": [
        "evaluate(y_train, y_pred_train_cb2, \"Train\")\n",
        "evaluate(y_test, y_pred_test_cb2, \"Test\")\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred_test_cb2, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title(\"CatBoost: Predicted vs Actual (Test Set)\")\n",
        "plt.xlabel(\"Actual SEC\")\n",
        "plt.ylabel(\"Predicted SEC\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55432305-c9dc-4239-ac15-0e7b8772df80",
      "metadata": {
        "id": "55432305-c9dc-4239-ac15-0e7b8772df80",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-AdaBoost-SEC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d5a20b-d637-47a2-8088-eb270c1e9f23",
      "metadata": {
        "id": "f3d5a20b-d637-47a2-8088-eb270c1e9f23"
      },
      "outputs": [],
      "source": [
        "!pip install pyswarms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de538184-47fd-459f-b830-fb156588df55",
      "metadata": {
        "id": "de538184-47fd-459f-b830-fb156588df55"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PSO-tuned AdaBoost Regression\n",
        "- Uses pyswarms GlobalBestPSO to minimize CV MSE\n",
        "- Tunes: learning_rate, n_estimators, max_depth (of base tree),\n",
        "         min_samples_leaf (of base tree), loss\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Data (same pattern as yours)\n",
        "# ---------------------------\n",
        "y_train = df_label2.copy().values  # e.g., Recovery\n",
        "X_train = df.copy().values\n",
        "y_test  = ts_label2.copy().values\n",
        "X_test  = ts.copy().values\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PSO fitness function\n",
        "# ---------------------------\n",
        "# dims:\n",
        "# 0 -> learning_rate       [0.005, 1.0]\n",
        "# 1 -> n_estimators        [50, 1000] (int)\n",
        "# 2 -> max_depth           [1, 10]    (int) for base DecisionTree\n",
        "# 3 -> min_samples_leaf    [1, 20]    (int) for base DecisionTree\n",
        "# 4 -> loss_code           [0, 2]     (int) -> 0:'linear', 1:'square', 2:'exponential'\n",
        "\n",
        "lower_bounds = np.array([0.005,   50,  1,  1, 0])\n",
        "upper_bounds = np.array([1.000, 1000, 10, 20, 2])\n",
        "\n",
        "_LOSSES = ['linear', 'square', 'exponential']\n",
        "\n",
        "def _params_from_particle(p):\n",
        "    \"\"\"Map a particle to AdaBoost params (cast + clip).\"\"\"\n",
        "    learning_rate    = float(np.clip(p[0], lower_bounds[0], upper_bounds[0]))\n",
        "    n_estimators     = int(np.round(np.clip(p[1], lower_bounds[1], upper_bounds[1])))\n",
        "    max_depth        = int(np.round(np.clip(p[2], lower_bounds[2], upper_bounds[2])))\n",
        "    min_samples_leaf = int(np.round(np.clip(p[3], lower_bounds[3], upper_bounds[3])))\n",
        "    loss_code        = int(np.round(np.clip(p[4], lower_bounds[4], upper_bounds[4])))\n",
        "    loss             = _LOSSES[loss_code]\n",
        "\n",
        "    # Base learner\n",
        "    base_tree = DecisionTreeRegressor(\n",
        "        max_depth=max_depth,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    params = {\n",
        "        \"estimator\": base_tree,          # sklearn >= 1.2\n",
        "        \"n_estimators\": n_estimators,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"loss\": loss,\n",
        "        \"random_state\": RANDOM_STATE\n",
        "    }\n",
        "    return params, dict(\n",
        "        learning_rate=learning_rate, n_estimators=n_estimators,\n",
        "        max_depth=max_depth, min_samples_leaf=min_samples_leaf, loss=loss\n",
        "    )\n",
        "\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    particles: (n_particles, dims)\n",
        "    returns:   (n_particles,) mean CV MSE (to minimize)\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "\n",
        "    for i in range(n_particles):\n",
        "        p = particles[i]\n",
        "        params, flat = _params_from_particle(p)\n",
        "        try:\n",
        "            model = AdaBoostRegressor(**params)\n",
        "            # cross_val_score returns neg MSE -> convert to +MSE\n",
        "            cv_scores = cross_val_score(\n",
        "                model, X_train, y_train.ravel(),\n",
        "                cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1\n",
        "            )\n",
        "            mse = -np.mean(cv_scores)\n",
        "            # tiny complexity penalty to prefer simpler ensembles\n",
        "            penalty = 1e-6 * (flat[\"n_estimators\"] * flat[\"max_depth\"])\n",
        "            scores[i] = mse + penalty\n",
        "        except Exception:\n",
        "            scores[i] = 1e10\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Run PSO\n",
        "# ---------------------------\n",
        "options = {\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7}\n",
        "dimensions = 5\n",
        "\n",
        "optimizer = GlobalBestPSO(\n",
        "    n_particles=16,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=(lower_bounds, upper_bounds),\n",
        ")\n",
        "\n",
        "print(\"Starting PSO optimization for AdaBoost (minimize 3-fold CV MSE)...\")\n",
        "cost, pos = optimizer.optimize(fitness_function, iters=20, verbose=True)\n",
        "print(\"PSO finished. best cost (CV MSE):\", cost)\n",
        "print(\"Best raw particle position:\", pos)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train final AdaBoost with best params\n",
        "# ---------------------------\n",
        "best_params, best_flat = _params_from_particle(pos)\n",
        "print(\"\\nBest hyperparameters (rounded/casted):\")\n",
        "for k, v in best_flat.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "final_model = AdaBoostRegressor(**best_params)\n",
        "final_model.fit(X_train, y_train.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b36eee21-a947-44f4-9c3e-d050f8c11f1d",
      "metadata": {
        "id": "b36eee21-a947-44f4-9c3e-d050f8c11f1d"
      },
      "outputs": [],
      "source": [
        "print(\"AdaBoostRegressor(\", end=\"\")\n",
        "for i, (k, v) in enumerate(final_model.get_params().items()):\n",
        "    comma = \",\" if i < len(final_model.get_params()) - 1 else \"\"\n",
        "    print(f\"{k}={repr(v)}{comma}\")\n",
        "print(\")\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05554a25-a231-4757-81d7-d95b9a870367",
      "metadata": {
        "id": "05554a25-a231-4757-81d7-d95b9a870367"
      },
      "outputs": [],
      "source": [
        "y_pred_test_adb2 = final_model.predict(X_test)\n",
        "y_pred_train_adb2 = final_model.predict(X_train)\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_adb2.flatten()\n",
        "test_vals = y_pred_test_adb2.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_adb2\": train_vals,\n",
        "    \"y_pred_test_adb2\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"adb2_predictions.xlsx\", sheet_name=\"adb2\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'adb2_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c36394-aad5-4c46-a134-b2214999aba5",
      "metadata": {
        "id": "08c36394-aad5-4c46-a134-b2214999aba5"
      },
      "outputs": [],
      "source": [
        "evaluate(y_train, y_pred_train_adb2, \"Train\")\n",
        "evaluate(y_test, y_pred_test_adb2, \"Test\")\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred_test_adb2, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title(\"AdaBoost: Predicted vs Actual (Test Set)\")\n",
        "plt.xlabel(\"Actual SEC\")\n",
        "plt.ylabel(\"Predicted SEC\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DH-S2nopUypw",
      "metadata": {
        "id": "DH-S2nopUypw",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# **Target3: Purity**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XrzOvxdUdEW8",
      "metadata": {
        "id": "XrzOvxdUdEW8",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## ANN-Purity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YYZ78w_edEW9",
      "metadata": {
        "id": "YYZ78w_edEW9"
      },
      "outputs": [],
      "source": [
        "xtrain = df.copy()\n",
        "xtest = ts.copy()\n",
        "# Recovery%\n",
        "ytrain = df_label3.copy()\n",
        "ytest = ts_label3.copy()\n",
        "pd.concat([xtrain, ytrain]).describe().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YOPGze_rdEW9",
      "metadata": {
        "id": "YOPGze_rdEW9"
      },
      "source": [
        "**tuning the hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eoTvuBq3dEW-",
      "metadata": {
        "collapsed": true,
        "id": "eoTvuBq3dEW-",
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5N0R9cODdEW-",
      "metadata": {
        "collapsed": true,
        "id": "5N0R9cODdEW-",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# --- Fix seeds for reproducibility ---\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# -- Normalizer ---\n",
        "normalizer = keras.layers.Normalization()\n",
        "normalizer.adapt(np.array(xtrain))\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(normalizer)\n",
        "\n",
        "    # Tune number of hidden layers (1 to 3)\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        model.add(layers.Dense(\n",
        "            units=hp.Int(f'units_{i}', min_value=16, max_value=128, step=8),\n",
        "            activation='relu'\n",
        "        ))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    # Tune learning rate\n",
        "    learning_rate = hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3, 1e-4])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='mean_absolute_error'\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Instantiate tuner (set seed here too)\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='tuner_dir',\n",
        "    project_name='ann_tuning',\n",
        "    overwrite=True,  # ensures consistency between runs\n",
        "    seed=SEED        # ensures deterministic search\n",
        ")\n",
        "\n",
        "# Search best hyperparameters\n",
        "tuner.search(xtrain, ytrain,\n",
        "             epochs=50,\n",
        "             validation_split=0.2,\n",
        "             verbose=1)\n",
        "\n",
        "# Retrieve best model and hyperparameters\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(\"Best number of layers:\", best_hps.get('num_layers'))\n",
        "for i in range(best_hps.get('num_layers')):\n",
        "    print(f\"Units in layer {i}:\", best_hps.get(f'units_{i}'))\n",
        "print(\"Best learning rate:\", best_hps.get('learning_rate'))\n",
        "\n",
        "# Train best model\n",
        "best_model.fit(xtrain, ytrain, epochs=100, validation_split=0.2, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y9giWu7AdEW-",
      "metadata": {
        "id": "y9giWu7AdEW-"
      },
      "outputs": [],
      "source": [
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GnxgrTk5dEW_",
      "metadata": {
        "id": "GnxgrTk5dEW_"
      },
      "outputs": [],
      "source": [
        "best_model.compile(optimizer = keras.optimizers.Adam(), #keras.optimizers.Adam change if needed\n",
        "              loss= 'mean_absolute_error')\n",
        "best_model.fit(xtrain, ytrain, verbose=0, epochs=101)\n",
        "\n",
        "y_pred_test_ann3 = best_model.predict(xtest).flatten()\n",
        "y_pred_train_ann3 = best_model.predict(xtrain).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wL78cZOufPTz",
      "metadata": {
        "id": "wL78cZOufPTz"
      },
      "outputs": [],
      "source": [
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_ann3.flatten()\n",
        "test_vals = y_pred_test_ann3.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_ann3\": train_vals,\n",
        "    \"y_pred_test_ann3\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"ann3_predictions.xlsx\", sheet_name=\"ann3\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'ann3_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H3vra5OudEW_",
      "metadata": {
        "id": "H3vra5OudEW_"
      },
      "outputs": [],
      "source": [
        "evaluate(ytrain, y_pred_train_ann3, \"Train\")\n",
        "evaluate(ytest, y_pred_test_ann3, \"Test\")\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(ytest, y_pred_test_ann3, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(ytest), np.min(y_pred_test_ann3))\n",
        "mx = max(np.max(ytest), np.max(y_pred_test_ann3))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual Purity\")\n",
        "plt.ylabel(\"Predicted Purity\")\n",
        "plt.title(\"ANN: Predicted vs Actual (Test)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SbAViSE3U5-w",
      "metadata": {
        "id": "SbAViSE3U5-w",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## TabPFN-Purity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2LAKOsYhVF1g",
      "metadata": {
        "collapsed": true,
        "id": "2LAKOsYhVF1g",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install tabpfn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j9HI5SVgU6gJ",
      "metadata": {
        "id": "j9HI5SVgU6gJ"
      },
      "outputs": [],
      "source": [
        "from tabpfn import TabPFNRegressor\n",
        "\n",
        "X_train = df.values\n",
        "y_train = df_label3.values\n",
        "\n",
        "X_test = ts.values\n",
        "y_test = ts_label3.values\n",
        "\n",
        "# Initialize the regressor\n",
        "regressor = TabPFNRegressor()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred_test_tab3 = regressor.predict(X_test)\n",
        "y_pred_train_tab3 = regressor.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tFaUulVRg4ph",
      "metadata": {
        "id": "tFaUulVRg4ph"
      },
      "outputs": [],
      "source": [
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_tab3.flatten()\n",
        "test_vals = y_pred_test_tab3.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_tab3\": train_vals,\n",
        "    \"y_pred_test_tab3\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"tab3_predictions.xlsx\", sheet_name=\"tab3\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'tab3_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K77RechnM0oX",
      "metadata": {
        "id": "K77RechnM0oX"
      },
      "outputs": [],
      "source": [
        "# Predict on the test set\n",
        "evaluate(y_train, y_pred_train_tab3, \"Train\")\n",
        "evaluate(y_test, y_pred_test_tab3, \"Test\")\n",
        "\n",
        "# Plot Predicted vs Actual\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred_test_tab3, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(y_test), np.min(y_pred_test_tab3))\n",
        "mx = max(np.max(y_test), np.max(y_pred_test_tab3))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=3)\n",
        "plt.xlabel(\"Actual Purity\")\n",
        "plt.ylabel(\"Predicted Purity\")\n",
        "plt.title(\"TabPFN: Predicted vs Actual (Test)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fLelr0jwWHrB",
      "metadata": {
        "id": "fLelr0jwWHrB",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-XGBoost-Purity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "md7XhY5cWMUv",
      "metadata": {
        "collapsed": true,
        "id": "md7XhY5cWMUv",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install pyswarms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gn3Bn-XnWNfX",
      "metadata": {
        "id": "gn3Bn-XnWNfX"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Load data and prepare\n",
        "# ---------------------------\n",
        "\n",
        "y_train = df_label3.values # Purity\n",
        "X_train = df.values        # features (scaled)\n",
        "\n",
        "y_test = ts_label3.values\n",
        "X_test = ts.values\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PSO fitness function\n",
        "# ---------------------------\n",
        "# We'll tune 6 parameters:\n",
        "# idx 0 -> learning_rate in [0.001, 0.3]\n",
        "# idx 1 -> max_depth in [1, 10] (int)\n",
        "# idx 2 -> n_estimators in [50, 1000] (int)\n",
        "# idx 3 -> subsample in [0.3, 1.0]\n",
        "# idx 4 -> colsample_bytree in [0.3, 1.0]\n",
        "# idx 5 -> gamma in [0.0, 5.0]\n",
        "\n",
        "# bounds arrays for pyswarms\n",
        "lower_bounds = np.array([0.001, 1, 50, 0.3, 0.3, 0.0])\n",
        "upper_bounds = np.array([0.3,   10, 1000, 1.0, 1.0, 5.0])\n",
        "\n",
        "def _params_from_particle(particle):\n",
        "    \"\"\"Map a single particle vector to XGBoost params (with casts).\"\"\"\n",
        "    learning_rate = float(particle[0])\n",
        "    max_depth = int(np.round(particle[1]))\n",
        "    max_depth = max(1, max_depth)\n",
        "    n_estimators = int(np.round(particle[2]))\n",
        "    n_estimators = max(1, n_estimators)\n",
        "    subsample = float(particle[3])\n",
        "    colsample_bytree = float(particle[4])\n",
        "    gamma = float(particle[5])\n",
        "\n",
        "    params = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"max_depth\": max_depth,\n",
        "        \"n_estimators\": n_estimators,\n",
        "        \"subsample\": subsample,\n",
        "        \"colsample_bytree\": colsample_bytree,\n",
        "        \"gamma\": gamma,\n",
        "        \"random_state\": RANDOM_STATE,\n",
        "        \"verbosity\": 0,\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "        # 'tree_method':'hist' could speed up on large data (optional)\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    particles: array shape (n_particles, dims)\n",
        "    returns: array shape (n_particles,) of CV MSE (we minimize)\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "    # We use 3-fold CV on training set and return mean MSE\n",
        "    for i in range(n_particles):\n",
        "        p = particles[i]\n",
        "        params = _params_from_particle(p)\n",
        "        try:\n",
        "            model = XGBRegressor(**params)\n",
        "            # negative MSE returned -> convert to positive MSE\n",
        "            cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1)\n",
        "            mse = -np.mean(cv_scores)\n",
        "            # small regularization to prefer simpler models: add tiny penalty for large n_estimators / depth\n",
        "            penalty = 1e-6 * (params[\"n_estimators\"] * params[\"max_depth\"])\n",
        "            scores[i] = mse + penalty\n",
        "        except Exception as e:\n",
        "            # something went wrong for this particle: give a large penalty\n",
        "            scores[i] = 1e10\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Run PSO\n",
        "# ---------------------------\n",
        "# PSO settings\n",
        "options = {\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7}\n",
        "\n",
        "# dimension equals number of hyperparams\n",
        "dimensions = 6\n",
        "# initialize optimizer\n",
        "optimizer = GlobalBestPSO(\n",
        "    n_particles=16,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=(lower_bounds, upper_bounds),\n",
        "    # ftol, etc. could be set; keep defaults\n",
        ")\n",
        "\n",
        "print(\"Starting PSO optimization (this may take some minutes depending on data size)...\")\n",
        "# run optimization: choose moderate iterations to balance runtime vs search\n",
        "cost, pos = optimizer.optimize(fitness_function, iters=30, verbose=True)\n",
        "print(\"PSO finished. best cost (CV MSE):\", cost)\n",
        "print(\"Best raw particle position:\", pos)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train final XGBoost with best params\n",
        "# ---------------------------\n",
        "best_params = _params_from_particle(pos)\n",
        "print(\"\\nBest hyperparameters (rounded/casted):\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "final_model = XGBRegressor(**best_params)\n",
        "final_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4Wh3JXhAh9-y",
      "metadata": {
        "id": "4Wh3JXhAh9-y"
      },
      "outputs": [],
      "source": [
        "y_pred_test_xg3 = final_model.predict(X_test)    # \"xg\" is the model and \"3\" is the targer which is \"Purity\"\n",
        "y_pred_train_xg3 = final_model.predict(X_train)\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_xg3.flatten()\n",
        "test_vals = y_pred_test_xg3.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_xg3\": train_vals,\n",
        "    \"y_pred_test_xg3\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"xg3_predictions.xlsx\", sheet_name=\"xg3\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'xg3_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FivUrbzonDFT",
      "metadata": {
        "id": "FivUrbzonDFT"
      },
      "outputs": [],
      "source": [
        "evaluate(y_train, y_pred_train_xg3, \"Train\")\n",
        "evaluate(y_test, y_pred_test_xg3, \"Test\")\n",
        "\n",
        "# Plot Predicted vs Actual\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred_test_xg3, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(y_test), np.min(y_pred_test_xg3))\n",
        "mx = max(np.max(y_test), np.max(y_pred_test_xg3))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual Purity\")\n",
        "plt.ylabel(\"Predicted Purity\")\n",
        "plt.title(\"PSO-XGBoost: Predicted vs Actual (Test)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KluITpGbA5hy",
      "metadata": {
        "id": "KluITpGbA5hy",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## GBR-Purity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2QfqKGOFPUXt",
      "metadata": {
        "id": "2QfqKGOFPUXt"
      },
      "outputs": [],
      "source": [
        "y_train = df_label3.copy()\n",
        "y_test = ts_label3.copy()\n",
        "X_train = df.copy()\n",
        "X_test = ts.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ktD5hvosPWlF",
      "metadata": {
        "id": "ktD5hvosPWlF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PSO-tuned Gradient Boosting Regression (GBR)\n",
        "- Uses pyswarms GlobalBestPSO to minimize CV MSE\n",
        "- Tunes: learning_rate, max_depth, n_estimators, subsample,\n",
        "         min_samples_split, min_samples_leaf, max_features\n",
        "\"\"\"\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Your prepared data\n",
        "# ---------------------------\n",
        "# Assumes these already exist exactly like in your code:\n",
        "# X_train, y_train, X_test, y_test\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PSO fitness function\n",
        "# ---------------------------\n",
        "# We'll tune 7 parameters:\n",
        "# idx 0 -> learning_rate      in [0.005, 0.30]        (float)\n",
        "# idx 1 -> max_depth          in [1, 8]               (int)\n",
        "# idx 2 -> n_estimators       in [50, 1000]           (int)\n",
        "# idx 3 -> subsample          in [0.50, 1.00]         (float)\n",
        "# idx 4 -> min_samples_split  in [2, 20]              (int)\n",
        "# idx 5 -> min_samples_leaf   in [1, 10]              (int)\n",
        "# idx 6 -> max_features_code  in [0.20, 1.00]         (float) -> maps to None if >=0.99, else fraction\n",
        "\n",
        "# bounds arrays for pyswarms\n",
        "lower_bounds = np.array([0.005, 1,   50, 0.50,  2,  1, 0.20])\n",
        "upper_bounds = np.array([0.300, 8, 1000, 1.00, 20, 10, 1.00])\n",
        "\n",
        "def _params_from_particle(particle):\n",
        "    \"\"\"Map a single particle vector to GBR params (with casts and clipping).\"\"\"\n",
        "    learning_rate = float(np.clip(particle[0], lower_bounds[0], upper_bounds[0]))\n",
        "    max_depth = int(np.round(np.clip(particle[1], lower_bounds[1], upper_bounds[1])))\n",
        "    n_estimators = int(np.round(np.clip(particle[2], lower_bounds[2], upper_bounds[2])))\n",
        "    subsample = float(np.clip(particle[3], lower_bounds[3], upper_bounds[3]))\n",
        "    min_samples_split = int(np.round(np.clip(particle[4], lower_bounds[4], upper_bounds[4])))\n",
        "    min_samples_leaf  = int(np.round(np.clip(particle[5], lower_bounds[5], upper_bounds[5])))\n",
        "\n",
        "    # max_features mapping: treat near-1.0 as None (i.e., use all features)\n",
        "    max_features_code = float(np.clip(particle[6], lower_bounds[6], upper_bounds[6]))\n",
        "    max_features = None if max_features_code >= 0.99 else float(max_features_code)\n",
        "\n",
        "    params = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"max_depth\": max_depth,\n",
        "        \"n_estimators\": n_estimators,\n",
        "        \"subsample\": subsample,\n",
        "        \"min_samples_split\": min_samples_split,\n",
        "        \"min_samples_leaf\": min_samples_leaf,\n",
        "        \"max_features\": max_features,\n",
        "        \"random_state\": RANDOM_STATE,\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    particles: array shape (n_particles, dims)\n",
        "    returns: array shape (n_particles,) of CV MSE (we minimize)\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "\n",
        "    for i in range(n_particles):\n",
        "        p = particles[i]\n",
        "        params = _params_from_particle(p)\n",
        "        try:\n",
        "            model = GradientBoostingRegressor(**params)\n",
        "            # negative MSE returned -> convert to positive MSE\n",
        "            cv_scores = cross_val_score(\n",
        "                model, X_train, y_train,\n",
        "                cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1\n",
        "            )\n",
        "            mse = -np.mean(cv_scores)\n",
        "\n",
        "            # tiny regularization to discourage overly large trees / ensembles\n",
        "            penalty = 1e-6 * (params[\"n_estimators\"] * params[\"max_depth\"])\n",
        "            scores[i] = mse + penalty\n",
        "        except Exception:\n",
        "            # any failure gets a large penalty\n",
        "            scores[i] = 1e10\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Run PSO\n",
        "# ---------------------------\n",
        "options = {\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7}\n",
        "dimensions = 7\n",
        "\n",
        "optimizer = GlobalBestPSO(\n",
        "    n_particles=16,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=(lower_bounds, upper_bounds),\n",
        "\n",
        ")\n",
        "\n",
        "print(\"Starting PSO optimization for GBR (minimize 3-fold CV MSE)...\")\n",
        "cost, pos = optimizer.optimize(fitness_function, iters=30, verbose=True)\n",
        "print(\"PSO finished. best cost (CV MSE):\", cost)\n",
        "print(\"Best raw particle position:\", pos)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train final GBR with best params\n",
        "# ---------------------------\n",
        "best_params = _params_from_particle(pos)\n",
        "print(\"\\nBest hyperparameters (rounded/casted):\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "final_model = GradientBoostingRegressor(**best_params)\n",
        "final_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rJMeG0wOJV1T",
      "metadata": {
        "id": "rJMeG0wOJV1T"
      },
      "outputs": [],
      "source": [
        "y_pred_test_gbr3 = final_model.predict(X_test)\n",
        "y_pred_train_gbr3 = final_model.predict(X_train)\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_gbr3.flatten()\n",
        "test_vals = y_pred_test_gbr3.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_gbr3\": train_vals,\n",
        "    \"y_pred_test_gbr3\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"gbr3_predictions.xlsx\", sheet_name=\"gbr3\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'gbr3_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5_ClH4L9PcAM",
      "metadata": {
        "id": "5_ClH4L9PcAM"
      },
      "outputs": [],
      "source": [
        "y_pred_test_gbr3 = final_model.predict(X_test)\n",
        "y_pred_train_gbr3 = final_model.predict(X_train)\n",
        "\n",
        "evaluate(df_label3, y_pred_train_gbr3, \"Train\")\n",
        "evaluate(ts_label3, y_pred_test_gbr3, \"Test\")\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(ts_label3, y_pred_test_gbr3, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([ts_label3.min(), ts_label3.max()], [ts_label3.min(), ts_label3.max()], 'r--', lw=2)\n",
        "plt.title(\"GBR: Predicted vs Actual (Test Set)\")\n",
        "plt.xlabel(\"Actual Purity\")\n",
        "plt.ylabel(\"Predicted Purity\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d65d9314-fde4-439d-9e62-43c0b69e14ad",
      "metadata": {
        "id": "d65d9314-fde4-439d-9e62-43c0b69e14ad",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-CatBoost-Purity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "698ad945-a492-4929-8b26-f33cf63572b8",
      "metadata": {
        "id": "698ad945-a492-4929-8b26-f33cf63572b8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PSO-tuned CatBoost Regression\n",
        "- Uses pyswarms GlobalBestPSO to minimize CV MSE\n",
        "- Tunes: learning_rate, depth, iterations, subsample, rsm, l2_leaf_reg\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Data (same as your XGB block)\n",
        "# ---------------------------\n",
        "y_train = df_label3.copy().values  # Recovery\n",
        "X_train = df.copy().values         # features (scaled)\n",
        "\n",
        "y_test  = ts_label3.copy().values\n",
        "X_test  = ts.copy().values\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PSO fitness function\n",
        "# ---------------------------\n",
        "# dims:\n",
        "# 0 -> learning_rate      [0.001, 0.3]\n",
        "# 1 -> depth              [1, 10] (int)\n",
        "# 2 -> iterations         [50, 1000] (int)\n",
        "# 3 -> subsample          [0.3, 1.0]\n",
        "# 4 -> rsm                [0.3, 1.0]  (feature subsampling)\n",
        "# 5 -> l2_leaf_reg        [0.0, 10.0]\n",
        "\n",
        "lower_bounds = np.array([0.001, 1,   50, 0.3, 0.3, 0.0])\n",
        "upper_bounds = np.array([0.300, 10, 1000, 1.0, 1.0,10.0])\n",
        "\n",
        "def _params_from_particle(p):\n",
        "    \"\"\"Map a particle to CatBoost params (cast + clip).\"\"\"\n",
        "    learning_rate = float(np.clip(p[0], lower_bounds[0], upper_bounds[0]))\n",
        "    depth         = int(np.round(np.clip(p[1], lower_bounds[1], upper_bounds[1])))\n",
        "    iterations    = int(np.round(np.clip(p[2], lower_bounds[2], upper_bounds[2])))\n",
        "    subsample     = float(np.clip(p[3], lower_bounds[3], upper_bounds[3]))\n",
        "    rsm           = float(np.clip(p[4], lower_bounds[4], upper_bounds[4]))\n",
        "    l2_leaf_reg   = float(np.clip(p[5], lower_bounds[5], upper_bounds[5]))\n",
        "\n",
        "    params = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"depth\": depth,\n",
        "        \"iterations\": iterations,\n",
        "        \"subsample\": subsample,\n",
        "        \"rsm\": rsm,\n",
        "        \"l2_leaf_reg\": l2_leaf_reg,\n",
        "        \"loss_function\": \"RMSE\",\n",
        "        \"random_seed\": RANDOM_STATE,\n",
        "        \"verbose\": False,\n",
        "        \"allow_writing_files\": False,\n",
        "        \"bootstrap_type\": \"Bernoulli\",   # enables 'subsample'\n",
        "        \"task_type\": \"GPU\"\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    particles: (n_particles, dims)\n",
        "    returns:   (n_particles,) mean CV MSE (to minimize)\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "\n",
        "    for i in range(n_particles):\n",
        "        p = particles[i]\n",
        "        params = _params_from_particle(p)\n",
        "        try:\n",
        "            model = CatBoostRegressor(**params)\n",
        "            # cross_val_score returns neg MSE -> take positive MSE\n",
        "            cv_scores = cross_val_score(\n",
        "                model, X_train, y_train,\n",
        "                cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1\n",
        "            )\n",
        "            mse = -np.mean(cv_scores)\n",
        "            # tiny complexity penalty (iterations * depth)\n",
        "            penalty = 1e-6 * (params[\"iterations\"] * params[\"depth\"])\n",
        "            scores[i] = mse + penalty\n",
        "        except Exception:\n",
        "            scores[i] = 1e10\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Run PSO\n",
        "# ---------------------------\n",
        "options = {\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7}\n",
        "dimensions = 6\n",
        "\n",
        "optimizer = GlobalBestPSO(\n",
        "    n_particles=16,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=(lower_bounds, upper_bounds),\n",
        ")\n",
        "\n",
        "print(\"Starting PSO optimization for CatBoost (minimize 3-fold CV MSE)...\")\n",
        "cost, pos = optimizer.optimize(fitness_function, iters=30, verbose=True)\n",
        "print(\"PSO finished. best cost (CV MSE):\", cost)\n",
        "print(\"Best raw particle position:\", pos)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train final CatBoost with best params\n",
        "# ---------------------------\n",
        "best_params = _params_from_particle(pos)\n",
        "print(\"\\nBest hyperparameters (rounded/casted):\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "final_model = CatBoostRegressor(**best_params)\n",
        "# Force CPU right before training to avoid pairwise error\n",
        "final_model.set_params(task_type=\"CPU\")   # <--- add this line\n",
        "final_model.fit(X_train, y_train, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4cf4f53-9256-4e52-8309-33e2364ce48d",
      "metadata": {
        "id": "f4cf4f53-9256-4e52-8309-33e2364ce48d"
      },
      "outputs": [],
      "source": [
        "print(\"CatBoostRegressor(\", end=\"\")\n",
        "for i, (k, v) in enumerate(final_model.get_params().items()):\n",
        "    comma = \",\" if i < len(final_model.get_params()) - 1 else \"\"\n",
        "    print(f\"{k}={repr(v)}{comma}\")\n",
        "print(\")\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75b1eedb-78d7-4902-a080-c8899349a461",
      "metadata": {
        "id": "75b1eedb-78d7-4902-a080-c8899349a461"
      },
      "outputs": [],
      "source": [
        "y_pred_test_cb3 = final_model.predict(X_test)\n",
        "y_pred_train_cb3 = final_model.predict(X_train)\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_cb3.flatten()\n",
        "test_vals = y_pred_test_cb3.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_cb3\": train_vals,\n",
        "    \"y_pred_test_cb3\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"cb3_predictions.xlsx\", sheet_name=\"cb3\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'cb3_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35263000-c4a5-4c88-a183-9aadd4d37dd6",
      "metadata": {
        "id": "35263000-c4a5-4c88-a183-9aadd4d37dd6"
      },
      "outputs": [],
      "source": [
        "evaluate(y_train, y_pred_train_cb3, \"Train\")\n",
        "evaluate(y_test, y_pred_test_cb3, \"Test\")\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred_test_cb3, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title(\"CatBoost: Predicted vs Actual (Test Set)\")\n",
        "plt.xlabel(\"Actual Purity\")\n",
        "plt.ylabel(\"Predicted Purity\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22bb8c6b-c6cb-40ee-bd3d-b435573cbac5",
      "metadata": {
        "id": "22bb8c6b-c6cb-40ee-bd3d-b435573cbac5",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-AdaBoost-Purity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "254bb82d-6c94-406e-8c0f-129132ac84c0",
      "metadata": {
        "id": "254bb82d-6c94-406e-8c0f-129132ac84c0"
      },
      "outputs": [],
      "source": [
        "!pip install pyswarms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9f7969-737d-44a7-a8ce-e5ec37b35c58",
      "metadata": {
        "id": "ff9f7969-737d-44a7-a8ce-e5ec37b35c58"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PSO-tuned AdaBoost Regression\n",
        "- Uses pyswarms GlobalBestPSO to minimize CV MSE\n",
        "- Tunes: learning_rate, n_estimators, max_depth (of base tree),\n",
        "         min_samples_leaf (of base tree), loss\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Data (same pattern as yours)\n",
        "# ---------------------------\n",
        "y_train = df_label3.copy().values  # e.g., Recovery\n",
        "X_train = df.copy().values\n",
        "y_test  = ts_label3.copy().values\n",
        "X_test  = ts.copy().values\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PSO fitness function\n",
        "# ---------------------------\n",
        "# dims:\n",
        "# 0 -> learning_rate       [0.005, 1.0]\n",
        "# 1 -> n_estimators        [50, 1000] (int)\n",
        "# 2 -> max_depth           [1, 10]    (int) for base DecisionTree\n",
        "# 3 -> min_samples_leaf    [1, 20]    (int) for base DecisionTree\n",
        "# 4 -> loss_code           [0, 2]     (int) -> 0:'linear', 1:'square', 2:'exponential'\n",
        "\n",
        "lower_bounds = np.array([0.005,   50,  1,  1, 0])\n",
        "upper_bounds = np.array([1.000, 1000, 10, 20, 2])\n",
        "\n",
        "_LOSSES = ['linear', 'square', 'exponential']\n",
        "\n",
        "def _params_from_particle(p):\n",
        "    \"\"\"Map a particle to AdaBoost params (cast + clip).\"\"\"\n",
        "    learning_rate    = float(np.clip(p[0], lower_bounds[0], upper_bounds[0]))\n",
        "    n_estimators     = int(np.round(np.clip(p[1], lower_bounds[1], upper_bounds[1])))\n",
        "    max_depth        = int(np.round(np.clip(p[2], lower_bounds[2], upper_bounds[2])))\n",
        "    min_samples_leaf = int(np.round(np.clip(p[3], lower_bounds[3], upper_bounds[3])))\n",
        "    loss_code        = int(np.round(np.clip(p[4], lower_bounds[4], upper_bounds[4])))\n",
        "    loss             = _LOSSES[loss_code]\n",
        "\n",
        "    # Base learner\n",
        "    base_tree = DecisionTreeRegressor(\n",
        "        max_depth=max_depth,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    params = {\n",
        "        \"estimator\": base_tree,          # sklearn >= 1.2\n",
        "        \"n_estimators\": n_estimators,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"loss\": loss,\n",
        "        \"random_state\": RANDOM_STATE\n",
        "    }\n",
        "    return params, dict(\n",
        "        learning_rate=learning_rate, n_estimators=n_estimators,\n",
        "        max_depth=max_depth, min_samples_leaf=min_samples_leaf, loss=loss\n",
        "    )\n",
        "\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    particles: (n_particles, dims)\n",
        "    returns:   (n_particles,) mean CV MSE (to minimize)\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "\n",
        "    for i in range(n_particles):\n",
        "        p = particles[i]\n",
        "        params, flat = _params_from_particle(p)\n",
        "        try:\n",
        "            model = AdaBoostRegressor(**params)\n",
        "            # cross_val_score returns neg MSE -> convert to +MSE\n",
        "            cv_scores = cross_val_score(\n",
        "                model, X_train, y_train.ravel(),\n",
        "                cv=3, scoring=\"neg_mean_squared_error\", n_jobs=1\n",
        "            )\n",
        "            mse = -np.mean(cv_scores)\n",
        "            # tiny complexity penalty to prefer simpler ensembles\n",
        "            penalty = 1e-6 * (flat[\"n_estimators\"] * flat[\"max_depth\"])\n",
        "            scores[i] = mse + penalty\n",
        "        except Exception:\n",
        "            scores[i] = 1e10\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Run PSO\n",
        "# ---------------------------\n",
        "options = {\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7}\n",
        "dimensions = 5\n",
        "\n",
        "optimizer = GlobalBestPSO(\n",
        "    n_particles=16,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=(lower_bounds, upper_bounds),\n",
        ")\n",
        "\n",
        "print(\"Starting PSO optimization for AdaBoost (minimize 3-fold CV MSE)...\")\n",
        "cost, pos = optimizer.optimize(fitness_function, iters=20, verbose=True)\n",
        "print(\"PSO finished. best cost (CV MSE):\", cost)\n",
        "print(\"Best raw particle position:\", pos)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train final AdaBoost with best params\n",
        "# ---------------------------\n",
        "best_params, best_flat = _params_from_particle(pos)\n",
        "print(\"\\nBest hyperparameters (rounded/casted):\")\n",
        "for k, v in best_flat.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "final_model = AdaBoostRegressor(**best_params)\n",
        "final_model.fit(X_train, y_train.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f1a9af1-6326-4294-9df8-1f27a0cae1e0",
      "metadata": {
        "id": "0f1a9af1-6326-4294-9df8-1f27a0cae1e0"
      },
      "outputs": [],
      "source": [
        "print(\"AdaBoostRegressor(\", end=\"\")\n",
        "for i, (k, v) in enumerate(final_model.get_params().items()):\n",
        "    comma = \",\" if i < len(final_model.get_params()) - 1 else \"\"\n",
        "    print(f\"{k}={repr(v)}{comma}\")\n",
        "print(\")\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b622ee7-7079-41b4-9118-cec7ca8e5e98",
      "metadata": {
        "id": "1b622ee7-7079-41b4-9118-cec7ca8e5e98"
      },
      "outputs": [],
      "source": [
        "y_pred_test_adb3 = final_model.predict(X_test)\n",
        "y_pred_train_adb3 = final_model.predict(X_train)\n",
        "\n",
        "# Make sure both arrays are 1D\n",
        "train_vals = y_pred_train_adb3.flatten()\n",
        "test_vals = y_pred_test_adb3.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = max(len(train_vals), len(test_vals))\n",
        "train_vals = np.pad(train_vals, (0, max_len - len(train_vals)), constant_values=np.nan)\n",
        "test_vals = np.pad(test_vals, (0, max_len - len(test_vals)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_train_adb3\": train_vals,\n",
        "    \"y_pred_test_adb3\": test_vals\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"adb3_predictions.xlsx\", sheet_name=\"adb3\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'adb3_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0a2522d-d9f5-4d0d-843e-504ba062b2db",
      "metadata": {
        "id": "c0a2522d-d9f5-4d0d-843e-504ba062b2db"
      },
      "outputs": [],
      "source": [
        "evaluate(y_train, y_pred_train_adb3, \"Train\")\n",
        "evaluate(y_test, y_pred_test_adb3, \"Test\")\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred_test_adb3, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title(\"AdaBoost: Predicted vs Actual (Test Set)\")\n",
        "plt.xlabel(\"Actual Purity\")\n",
        "plt.ylabel(\"Predicted Purity\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iPS6_oDiY0Xp",
      "metadata": {
        "id": "iPS6_oDiY0Xp",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# **All Predictions based on \"Total Set\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abX-t5AFBgwJ",
      "metadata": {
        "id": "abX-t5AFBgwJ"
      },
      "outputs": [],
      "source": [
        "x_train = df.copy()\n",
        "x_test = ts.copy()\n",
        "# Recovery\n",
        "y_train1 = df_label1.copy()\n",
        "y_test1 = ts_label1.copy()\n",
        "# SEC\n",
        "y_train2 = df_label2.copy()\n",
        "y_test2 = ts_label2.copy()\n",
        "# Purity\n",
        "y_train3 = df_label3.copy()\n",
        "y_test3 = ts_label3.copy()\n",
        "# Total Set\n",
        "x_total = pd.concat([x_train, x_test], axis=0)\n",
        "y_total1 = pd.concat([y_train1, y_test1], axis=0)\n",
        "y_total2 = pd.concat([y_train2, y_test2], axis=0)\n",
        "y_total3 = pd.concat([y_train3, y_test3], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z_5JZDFoSGxf",
      "metadata": {
        "id": "Z_5JZDFoSGxf",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g6hzN0erGgCY",
      "metadata": {
        "id": "g6hzN0erGgCY"
      },
      "outputs": [],
      "source": [
        "# Recovery\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "n_features = x_total.shape[1]\n",
        "# Best model\n",
        "ann_model1 = keras.Sequential([\n",
        "    layers.Input(shape=(n_features,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(72, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)                       # single output\n",
        "])\n",
        "\n",
        "# Compile\n",
        "ann_model1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "                   loss='mean_absolute_error')\n",
        "# fit/predict/evaluate\n",
        "ann_model1.fit(x_total, y_total1, epochs=100, batch_size=32, verbose=0)\n",
        "y_pred_total_ann1 = ann_model1.predict(x_total).flatten()\n",
        "evaluate(y_total1, y_pred_total_ann1, \"Total\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total1, y_pred_total_ann1, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_total1.min(), y_total1.max()], [y_total1.min(), y_total1.max()], 'r--', lw=2)\n",
        "plt.title(\"ANN: Predicted vs Actual (Total Set)\")\n",
        "plt.xlabel(\"Actual Recovery%\")\n",
        "plt.ylabel(\"Predicted Recovery%\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n4ZnaMbCSLb0",
      "metadata": {
        "id": "n4ZnaMbCSLb0"
      },
      "outputs": [],
      "source": [
        "# SEC\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "n_features = x_total.shape[1]\n",
        "# Best model\n",
        "# ann_model2 = keras.Sequential([\n",
        "#     layers.Input(shape=(n_features,)),             # 8 input features\n",
        "#     layers.Dense(32, activation='relu'),   #120 128\n",
        "#     layers.Dense(72, activation='relu'),\n",
        "#     layers.Dense(32, activation='relu'),\n",
        "#     layers.Dense(1)                       # single output\n",
        "# ])\n",
        "ann_model2 = keras.Sequential([\n",
        "    layers.Input(shape=(n_features,)),    # 8 input features\n",
        "    layers.Dense(120, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1)                       # single output\n",
        "])\n",
        "\n",
        "# Compile\n",
        "ann_model2.compile(optimizer=keras.optimizers.Adam(learning_rate=0.1),\n",
        "                   loss='mean_absolute_error')\n",
        "# fit/predict/evaluate\n",
        "ann_model2.fit(x_total, y_total2, epochs=100, batch_size=32, verbose=0)\n",
        "y_pred_total_ann2 = ann_model2.predict(x_total).flatten()\n",
        "evaluate(y_total2, y_pred_total_ann2, \"Total\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total2, y_pred_total_ann2, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_total2.min(), y_total2.max()], [y_total2.min(), y_total2.max()], 'r--', lw=2)\n",
        "plt.title(\"ANN: Predicted vs Actual (Total Set)\")\n",
        "plt.xlabel(\"Actual SEC\")\n",
        "plt.ylabel(\"Predicted SEC\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K4svQWtXSLfv",
      "metadata": {
        "id": "K4svQWtXSLfv"
      },
      "outputs": [],
      "source": [
        "# Purity\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Best model\n",
        "ann_model3 = keras.Sequential([\n",
        "    layers.Input(shape=(n_features,)),    # 8 input features\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(72, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)                       # single output\n",
        "])\n",
        "\n",
        "# Compile\n",
        "ann_model3.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "                   loss='mean_absolute_error')\n",
        "# fit/predict/evaluate\n",
        "ann_model3.fit(x_total, y_total3, epochs=100, batch_size=32, verbose=0)\n",
        "y_pred_total_ann3 = ann_model3.predict(x_total).flatten()\n",
        "evaluate(y_total3, y_pred_total_ann3, \"Total\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total3, y_pred_total_ann3, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_total3.min(), y_total3.max()], [y_total3.min(), y_total3.max()], 'r--', lw=2)\n",
        "plt.title(\"ANN: Predicted vs Actual (Total Set)\")\n",
        "plt.xlabel(\"Actual Purity\")\n",
        "plt.ylabel(\"Predicted Purity\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aOWgd8Myt8iA",
      "metadata": {
        "id": "aOWgd8Myt8iA"
      },
      "source": [
        "**Save the 'Total Set' predictions as an excel file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M-Vlwh89uI9a",
      "metadata": {
        "id": "M-Vlwh89uI9a"
      },
      "outputs": [],
      "source": [
        "total_vals1 = y_pred_total_ann1.flatten()\n",
        "total_vals2 = y_pred_total_ann2.flatten()\n",
        "total_vals3 = y_pred_total_ann3.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = len(total_vals1)\n",
        "total_vals1 = np.pad(total_vals1, (0, max_len - len(total_vals1)), constant_values=np.nan)\n",
        "total_vals2 = np.pad(total_vals2, (0, max_len - len(total_vals2)), constant_values=np.nan)\n",
        "total_vals3 = np.pad(total_vals3, (0, max_len - len(total_vals3)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_total_ann1\": total_vals1,\n",
        "    \"y_pred_total_ann2\": total_vals2,\n",
        "    \"y_pred_total_ann3\": total_vals3\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"ann_total_predictions.xlsx\", sheet_name=\"ann\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'ann_total_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "teIvw9GNvuiB",
      "metadata": {
        "id": "teIvw9GNvuiB"
      },
      "outputs": [],
      "source": [
        "y_pred_total_ann3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N6yQdfoLSCLO",
      "metadata": {
        "id": "N6yQdfoLSCLO",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## TabPFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QI1yGWSJifTc",
      "metadata": {
        "id": "QI1yGWSJifTc"
      },
      "outputs": [],
      "source": [
        "!pip install tabpfn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e13wsYQnGgIS",
      "metadata": {
        "id": "e13wsYQnGgIS"
      },
      "outputs": [],
      "source": [
        "# Recovery\n",
        "from tabpfn import TabPFNRegressor\n",
        "\n",
        "tab_model1 = TabPFNRegressor()\n",
        "tab_model1.fit(x_total, y_total1)\n",
        "y_pred_total_tab1 = tab_model1.predict(x_total)\n",
        "evaluate(y_total1, y_pred_total_tab1, \"Total\")\n",
        "\n",
        "# Plot Predicted vs Actual\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total1, y_pred_total_tab1, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(y_total1), np.min(y_pred_total_tab1))\n",
        "mx = max(np.max(y_total1), np.max(y_pred_total_tab1))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual Recovery%\")\n",
        "plt.ylabel(\"Predicted Recovery%\")\n",
        "plt.title(\"TabPFN: Predicted vs Actual (Total Set)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7PGV0hnESFSw",
      "metadata": {
        "id": "7PGV0hnESFSw"
      },
      "outputs": [],
      "source": [
        "# SEC\n",
        "from tabpfn import TabPFNRegressor\n",
        "\n",
        "tab_model2 = TabPFNRegressor()\n",
        "tab_model2.fit(x_total, y_total2)\n",
        "y_pred_total_tab2 = tab_model2.predict(x_total)\n",
        "evaluate(y_total2, y_pred_total_tab2, \"Total\")\n",
        "\n",
        "# Plot Predicted vs Actual\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total2, y_pred_total_tab2, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(y_total2), np.min(y_pred_total_tab2))\n",
        "mx = max(np.max(y_total2), np.max(y_pred_total_tab2))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual SEC\")\n",
        "plt.ylabel(\"Predicted SEC\")\n",
        "plt.title(\"TabPFN: Predicted vs Actual (Total Set)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NV3q646YSFl-",
      "metadata": {
        "id": "NV3q646YSFl-"
      },
      "outputs": [],
      "source": [
        "# Purity\n",
        "from tabpfn import TabPFNRegressor\n",
        "\n",
        "tab_model3 = TabPFNRegressor()\n",
        "tab_model3.fit(x_total, y_total3)\n",
        "y_pred_total_tab3 = tab_model3.predict(x_total)\n",
        "evaluate(y_total3, y_pred_total_tab3, \"Total\")\n",
        "\n",
        "# Plot Predicted vs Actual\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total3, y_pred_total_tab3, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(y_total3), np.min(y_pred_total_tab3))\n",
        "mx = max(np.max(y_total3), np.max(y_pred_total_tab3))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual Purity\")\n",
        "plt.ylabel(\"Predicted Purity\")\n",
        "plt.title(\"TabPFN: Predicted vs Actual (Total Set)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69coAFRVwVJa",
      "metadata": {
        "id": "69coAFRVwVJa"
      },
      "outputs": [],
      "source": [
        "total_vals1 = y_pred_total_tab1.flatten()\n",
        "total_vals2 = y_pred_total_tab2.flatten()\n",
        "total_vals3 = y_pred_total_tab3.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = len(total_vals1)\n",
        "total_vals1 = np.pad(total_vals1, (0, max_len - len(total_vals1)), constant_values=np.nan)\n",
        "total_vals2 = np.pad(total_vals2, (0, max_len - len(total_vals2)), constant_values=np.nan)\n",
        "total_vals3 = np.pad(total_vals3, (0, max_len - len(total_vals3)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_total_tab1\": total_vals1,\n",
        "    \"y_pred_total_tab2\": total_vals2,\n",
        "    \"y_pred_total_tab3\": total_vals3\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"tab_total_predictions.xlsx\", sheet_name=\"tab\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'tab_total_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s4QoGGS0Gg6_",
      "metadata": {
        "id": "s4QoGGS0Gg6_",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKR_1M_HGgM8",
      "metadata": {
        "id": "UKR_1M_HGgM8"
      },
      "outputs": [],
      "source": [
        "# Recovery\n",
        "from xgboost import XGBRegressor\n",
        "xgb_model1 = XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
        "             colsample_bylevel=None, colsample_bynode=None,\n",
        "             colsample_bytree=0.794347904457994, device=None,\n",
        "             early_stopping_rounds=None, enable_categorical=False,\n",
        "             eval_metric=None, feature_types=None, feature_weights=None,\n",
        "             gamma=0.45398325921376137, grow_policy=None, importance_type=None,\n",
        "             interaction_constraints=None, learning_rate=0.08564167838681164,\n",
        "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
        "             max_delta_step=None, max_depth=5, max_leaves=None,\n",
        "             min_child_weight=None, monotone_constraints=None,\n",
        "             multi_strategy=None, n_estimators=724, n_jobs=None)\n",
        "\n",
        "xgb_model1.fit(x_total, y_total1)\n",
        "y_pred_total_xg1 = xgb_model1.predict(x_total)\n",
        "evaluate(y_total1, y_pred_total_xg1, \"Total\")\n",
        "\n",
        "# Plot Predicted vs Actual\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total1, y_pred_total_xg1, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(y_total1), np.min(y_pred_total_xg1))\n",
        "mx = max(np.max(y_total1), np.max(y_pred_total_xg1))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual Recovery%\")\n",
        "plt.ylabel(\"Predicted Recovery%\")\n",
        "plt.title(\"PSO-XGBoost: Predicted vs Actual (Total Set)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T0CYW3A0H9sX",
      "metadata": {
        "id": "T0CYW3A0H9sX"
      },
      "outputs": [],
      "source": [
        "# SEC\n",
        "from xgboost import XGBRegressor\n",
        "xgb_model2 = XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
        "             colsample_bylevel=None, colsample_bynode=None,\n",
        "             colsample_bytree=0.9207502204636566, device=None,\n",
        "             early_stopping_rounds=None, enable_categorical=False,\n",
        "             eval_metric=None, feature_types=None, feature_weights=None,\n",
        "             gamma=4.562640895437547, grow_policy=None, importance_type=None,\n",
        "             interaction_constraints=None, learning_rate=0.033250273864687305,\n",
        "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
        "             max_delta_step=None, max_depth=5, max_leaves=None,\n",
        "             min_child_weight=None, monotone_constraints=None,\n",
        "             multi_strategy=None, n_estimators=672, n_jobs=None)\n",
        "\n",
        "xgb_model2.fit(x_total, y_total2)\n",
        "y_pred_total_xg2 = xgb_model2.predict(x_total)\n",
        "evaluate(y_total2, y_pred_total_xg2, \"Total\")\n",
        "\n",
        "# Plot Predicted vs Actual\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total2, y_pred_total_xg2, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(y_total2), np.min(y_pred_total_xg2))\n",
        "mx = max(np.max(y_total2), np.max(y_pred_total_xg2))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual SEC\")\n",
        "plt.ylabel(\"Predicted SEC\")\n",
        "plt.title(\"PSO-XGBoost: Predicted vs Actual (Total Set)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OAki7nerH91e",
      "metadata": {
        "id": "OAki7nerH91e"
      },
      "outputs": [],
      "source": [
        "# Purity\n",
        "from xgboost import XGBRegressor\n",
        "xgb_model3 = XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
        "             colsample_bylevel=None, colsample_bynode=None,\n",
        "             colsample_bytree=0.9727111775192585, device=None,\n",
        "             early_stopping_rounds=None, enable_categorical=False,\n",
        "             eval_metric=None, feature_types=None, feature_weights=None,\n",
        "             gamma=0.5320395779360891, grow_policy=None, importance_type=None,\n",
        "             interaction_constraints=None, learning_rate=0.041150584937100235,\n",
        "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
        "             max_delta_step=None, max_depth=6, max_leaves=None,\n",
        "             min_child_weight=None, monotone_constraints=None,\n",
        "             multi_strategy=None, n_estimators=938, n_jobs=None)\n",
        "\n",
        "xgb_model3.fit(x_total, y_total3)\n",
        "y_pred_total_xg3 = xgb_model3.predict(x_total)\n",
        "evaluate(y_total3, y_pred_total_xg3, \"Total\")\n",
        "\n",
        "# Plot Predicted vs Actual\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total3, y_pred_total_xg3, alpha=0.7, edgecolor=\"k\")\n",
        "mn = min(np.min(y_total3), np.min(y_pred_total_xg3))\n",
        "mx = max(np.max(y_total3), np.max(y_pred_total_xg3))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"Actual Purity\")\n",
        "plt.ylabel(\"Predicted Purity\")\n",
        "plt.title(\"PSO-XGBoost: Predicted vs Actual (Total Set)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sv3SlwpzxlUH",
      "metadata": {
        "id": "Sv3SlwpzxlUH"
      },
      "outputs": [],
      "source": [
        "total_vals1 = y_pred_total_xg1.flatten()\n",
        "total_vals2 = y_pred_total_xg2.flatten()\n",
        "total_vals3 = y_pred_total_xg3.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = len(total_vals1)\n",
        "total_vals1 = np.pad(total_vals1, (0, max_len - len(total_vals1)), constant_values=np.nan)\n",
        "total_vals2 = np.pad(total_vals2, (0, max_len - len(total_vals2)), constant_values=np.nan)\n",
        "total_vals3 = np.pad(total_vals3, (0, max_len - len(total_vals3)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_total_xg1\": total_vals1,\n",
        "    \"y_pred_total_xg2\": total_vals2,\n",
        "    \"y_pred_total_xg3\": total_vals3\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"xg_total_predictions.xlsx\", sheet_name=\"xg\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'xg_total_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j_QQ5b-GRvOG",
      "metadata": {
        "id": "j_QQ5b-GRvOG",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-GBR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uScapEhpGpzQ",
      "metadata": {
        "id": "uScapEhpGpzQ"
      },
      "outputs": [],
      "source": [
        "# Recovery\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbr_model1 = GradientBoostingRegressor(learning_rate=0.07780212667426754, max_depth=4,\n",
        "                          max_features=0.7150294538727542, min_samples_leaf=4,\n",
        "                          min_samples_split=14, n_estimators=633,\n",
        "                          random_state=42, subsample=0.7757643293874783)\n",
        "\n",
        "gbr_model1.fit(x_total, y_total1)\n",
        "y_pred_total_gbr1 = gbr_model1.predict(x_total)\n",
        "evaluate(y_total1, y_pred_total_gbr1, \"Total\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total1, y_pred_total_gbr1, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_total1.min(), y_total1.max()], [y_total1.min(), y_total1.max()], 'r--', lw=2)\n",
        "plt.title(\"GBR: Predicted vs Actual (Total Set)\")\n",
        "plt.xlabel(\"Actual Recovery%\")\n",
        "plt.ylabel(\"Predicted Recovery%\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9m_CLFoM3aL",
      "metadata": {
        "id": "f9m_CLFoM3aL"
      },
      "outputs": [],
      "source": [
        "# SEC\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbr_model2 = GradientBoostingRegressor(learning_rate=0.14250485305726465, max_depth=4,\n",
        "                          max_features=0.9075248053662535, min_samples_leaf=2,\n",
        "                          min_samples_split=18, n_estimators=607,\n",
        "                          random_state=42, subsample=0.9951297938254593)\n",
        "\n",
        "gbr_model2.fit(x_total, y_total2)\n",
        "y_pred_total_gbr2 = gbr_model2.predict(x_total)\n",
        "evaluate(y_total2, y_pred_total_gbr2, \"Total\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total2, y_pred_total_gbr2, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_total2.min(), y_total2.max()], [y_total2.min(), y_total2.max()], 'r--', lw=2)\n",
        "plt.title(\"GBR: Predicted vs Actual (Total Set)\")\n",
        "plt.xlabel(\"Actual SEC\")\n",
        "plt.ylabel(\"Predicted SEC\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kwUFkmSGM3fh",
      "metadata": {
        "id": "kwUFkmSGM3fh"
      },
      "outputs": [],
      "source": [
        "# Purity\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbr_model3 = GradientBoostingRegressor(learning_rate=0.028832399541631026, max_depth=8,\n",
        "                          max_features=0.9306761987596761, min_samples_leaf=7,\n",
        "                          min_samples_split=11, n_estimators=919,\n",
        "                          random_state=42, subsample=0.5396359220219887)\n",
        "\n",
        "gbr_model3.fit(x_total, y_total3)\n",
        "y_pred_total_gbr3 = gbr_model3.predict(x_total)\n",
        "evaluate(y_total3, y_pred_total_gbr3, \"Total\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total3, y_pred_total_gbr3, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_total3.min(), y_total3.max()], [y_total3.min(), y_total3.max()], 'r--', lw=2)\n",
        "plt.title(\"GBR: Predicted vs Actual (Total Set)\")\n",
        "plt.xlabel(\"Actual Purity\")\n",
        "plt.ylabel(\"Predicted Purity\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R9MOTdelyMo4",
      "metadata": {
        "id": "R9MOTdelyMo4"
      },
      "outputs": [],
      "source": [
        "total_vals1 = y_pred_total_gbr1.flatten()\n",
        "total_vals2 = y_pred_total_gbr2.flatten()\n",
        "total_vals3 = y_pred_total_gbr3.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = len(total_vals1)\n",
        "total_vals1 = np.pad(total_vals1, (0, max_len - len(total_vals1)), constant_values=np.nan)\n",
        "total_vals2 = np.pad(total_vals2, (0, max_len - len(total_vals2)), constant_values=np.nan)\n",
        "total_vals3 = np.pad(total_vals3, (0, max_len - len(total_vals3)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_total_gbr1\": total_vals1,\n",
        "    \"y_pred_total_gbr2\": total_vals2,\n",
        "    \"y_pred_total_gbr3\": total_vals3\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"gbr_total_predictions.xlsx\", sheet_name=\"gbr\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'gbr_total_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c895d5-a04d-4754-8686-f6066a8528da",
      "metadata": {
        "id": "13c895d5-a04d-4754-8686-f6066a8528da",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f44bf9-4712-4530-b1c3-6b697dd4b2e8",
      "metadata": {
        "id": "60f44bf9-4712-4530-b1c3-6b697dd4b2e8"
      },
      "outputs": [],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20685275-8d44-44fb-af34-b4103ff2addb",
      "metadata": {
        "id": "20685275-8d44-44fb-af34-b4103ff2addb"
      },
      "outputs": [],
      "source": [
        "# Recovery\n",
        "from catboost import CatBoostRegressor\n",
        "cb_model1 = CatBoostRegressor(iterations=987, learning_rate=0.06921084015993914,\n",
        "                              depth=7, l2_leaf_reg=1.6926258102111902, rsm=0.8583651828877268,\n",
        "                              loss_function='RMSE', random_seed=42, verbose=False,\n",
        "                              allow_writing_files=False, task_type='CPU',\n",
        "                              bootstrap_type='Bernoulli', subsample=0.8125328711078112)\n",
        "\n",
        "cb_model1.fit(x_total, y_total1)\n",
        "y_pred_total_cb1 = cb_model1.predict(x_total)\n",
        "evaluate(y_total1, y_pred_total_cb1, \"Total\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total1, y_pred_total_cb1, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_total1.min(), y_total1.max()], [y_total1.min(), y_total1.max()], 'r--', lw=2)\n",
        "plt.title(\"CatBoost: Predicted vs Actual (Total Set)\")\n",
        "plt.xlabel(\"Actual Recovery%\")\n",
        "plt.ylabel(\"Predicted Recovery%\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003acb21-cd47-4828-a791-2e15e0d42144",
      "metadata": {
        "id": "003acb21-cd47-4828-a791-2e15e0d42144"
      },
      "outputs": [],
      "source": [
        "# SEC\n",
        "from catboost import CatBoostRegressor\n",
        "cb_model2 = CatBoostRegressor(iterations=484, learning_rate=0.04463768687603424, depth=5,\n",
        "                              l2_leaf_reg=0.5987692889341945, rsm=0.6484702678951451,\n",
        "                              loss_function='RMSE', random_seed=42, verbose=False,\n",
        "                              allow_writing_files=False, task_type='CPU',\n",
        "                              bootstrap_type='Bernoulli', subsample=0.8601134735851019)\n",
        "\n",
        "cb_model2.fit(x_total, y_total2)\n",
        "y_pred_total_cb2 = cb_model2.predict(x_total)\n",
        "evaluate(y_total2, y_pred_total_cb2, \"Total\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total2, y_pred_total_cb2, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_total2.min(), y_total2.max()], [y_total2.min(), y_total2.max()], 'r--', lw=2)\n",
        "plt.title(\"CatBoost: Predicted vs Actual (Total Set)\")\n",
        "plt.xlabel(\"Actual SEC\")\n",
        "plt.ylabel(\"Predicted SEC\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91046da0-b603-4350-b20f-ea5e425fa0e7",
      "metadata": {
        "id": "91046da0-b603-4350-b20f-ea5e425fa0e7"
      },
      "outputs": [],
      "source": [
        "# Purity\n",
        "from catboost import CatBoostRegressor\n",
        "cb_model3 = CatBoostRegressor(iterations=54, learning_rate=0.13410536212915647, depth=8,\n",
        "                              l2_leaf_reg=0.376584630037079, rsm=0.9710562268314031,\n",
        "                              loss_function='RMSE', random_seed=42, verbose=False,\n",
        "                              allow_writing_files=False, task_type='CPU',\n",
        "                              bootstrap_type='Bernoulli', subsample=0.5895788236923274)\n",
        "\n",
        "cb_model3.fit(x_total, y_total3)\n",
        "y_pred_total_cb3 = cb_model3.predict(x_total)\n",
        "evaluate(y_total3, y_pred_total_cb3, \"Total\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total3, y_pred_total_cb3, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_total3.min(), y_total3.max()], [y_total3.min(), y_total3.max()], 'r--', lw=2)\n",
        "plt.title(\"CatBoost: Predicted vs Actual (Total Set)\")\n",
        "plt.xlabel(\"Actual Purity\")\n",
        "plt.ylabel(\"Predicted Purity\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "quhtfolmyqpP",
      "metadata": {
        "id": "quhtfolmyqpP"
      },
      "outputs": [],
      "source": [
        "total_vals1 = y_pred_total_cb1.flatten()\n",
        "total_vals2 = y_pred_total_cb2.flatten()\n",
        "total_vals3 = y_pred_total_cb3.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = len(total_vals1)\n",
        "total_vals1 = np.pad(total_vals1, (0, max_len - len(total_vals1)), constant_values=np.nan)\n",
        "total_vals2 = np.pad(total_vals2, (0, max_len - len(total_vals2)), constant_values=np.nan)\n",
        "total_vals3 = np.pad(total_vals3, (0, max_len - len(total_vals3)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_total_cb1\": total_vals1,\n",
        "    \"y_pred_total_cb2\": total_vals2,\n",
        "    \"y_pred_total_cb3\": total_vals3\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"cb_total_predictions.xlsx\", sheet_name=\"cb\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'cb_total_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e3824b7-6af4-48b8-bf70-c772e124db13",
      "metadata": {
        "id": "2e3824b7-6af4-48b8-bf70-c772e124db13",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## PSO-AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91034631-71dc-4b18-96f3-a45acbd6d88d",
      "metadata": {
        "id": "91034631-71dc-4b18-96f3-a45acbd6d88d"
      },
      "outputs": [],
      "source": [
        "# Recovery\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define base estimator\n",
        "base_tree = DecisionTreeRegressor(\n",
        "    criterion='squared_error',\n",
        "    max_depth=10,\n",
        "    min_samples_leaf=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define AdaBoost model\n",
        "adb_model1 = AdaBoostRegressor(\n",
        "    estimator=base_tree,\n",
        "    learning_rate=0.9007685647106364,\n",
        "    loss='square',\n",
        "    n_estimators=471,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "adb_model1.fit(x_total, y_total1)\n",
        "\n",
        "# Predict\n",
        "y_pred_total_adb1 = adb_model1.predict(x_total)\n",
        "evaluate(y_total1, y_pred_total_adb1, \"Total\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total1, y_pred_total_adb1, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_total1.min(), y_total1.max()], [y_total1.min(), y_total1.max()], 'r--', lw=2)\n",
        "plt.title(\"AdaBoost: Predicted vs Actual (Total Set)\")\n",
        "plt.xlabel(\"Actual Recovery%\")\n",
        "plt.ylabel(\"Predicted Recovery%\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd679cb-8409-4f61-8ab9-cd468271b800",
      "metadata": {
        "id": "bcd679cb-8409-4f61-8ab9-cd468271b800"
      },
      "outputs": [],
      "source": [
        "# SEC\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define base estimator\n",
        "base_tree = DecisionTreeRegressor(\n",
        "    criterion='squared_error',\n",
        "    max_depth=10,\n",
        "    min_samples_leaf=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define AdaBoost model\n",
        "adb_model2 = AdaBoostRegressor(\n",
        "    estimator=base_tree,\n",
        "    learning_rate=0.479481012015941,\n",
        "    loss='square',\n",
        "    n_estimators=593,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "adb_model2.fit(x_total, y_total2)\n",
        "\n",
        "# Predict\n",
        "y_pred_total_adb2 = adb_model2.predict(x_total)\n",
        "evaluate(y_total2, y_pred_total_adb2, \"Total\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total2, y_pred_total_adb2, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_total2.min(), y_total2.max()], [y_total2.min(), y_total2.max()], 'r--', lw=2)\n",
        "plt.title(\"AdaBoost: Predicted vs Actual (Total Set)\")\n",
        "plt.xlabel(\"Actual SEC\")\n",
        "plt.ylabel(\"Predicted SEC\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407e2e3a-bd87-494e-8503-70fac6cd1dbc",
      "metadata": {
        "id": "407e2e3a-bd87-494e-8503-70fac6cd1dbc"
      },
      "outputs": [],
      "source": [
        "# Purity\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define base estimator\n",
        "base_tree = DecisionTreeRegressor(\n",
        "    criterion='squared_error',\n",
        "    max_depth=10,\n",
        "    min_samples_leaf=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define AdaBoost model\n",
        "adb_model3 = AdaBoostRegressor(\n",
        "    estimator=base_tree,\n",
        "    learning_rate=0.03597743718960905,\n",
        "    loss='square',\n",
        "    n_estimators=874,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "adb_model3.fit(x_total, y_total3)\n",
        "\n",
        "# Predict\n",
        "y_pred_total_adb3 = adb_model3.predict(x_total)\n",
        "evaluate(y_total3, y_pred_total_adb3, \"Total\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_total3, y_pred_total_adb3, color='dodgerblue', alpha=0.7, edgecolor='k')\n",
        "plt.plot([y_total3.min(), y_total3.max()], [y_total3.min(), y_total3.max()], 'r--', lw=2)\n",
        "plt.title(\"AdaBoost: Predicted vs Actual (Total Set)\")\n",
        "plt.xlabel(\"Actual Purity\")\n",
        "plt.ylabel(\"Predicted Purity\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "py0Jg2NNzCz-",
      "metadata": {
        "id": "py0Jg2NNzCz-"
      },
      "outputs": [],
      "source": [
        "total_vals1 = y_pred_total_adb1.flatten()\n",
        "total_vals2 = y_pred_total_adb2.flatten()\n",
        "total_vals3 = y_pred_total_adb3.flatten()\n",
        "\n",
        "# Pad shorter array with NaN so they align in Excel\n",
        "max_len = len(total_vals1)\n",
        "total_vals1 = np.pad(total_vals1, (0, max_len - len(total_vals1)), constant_values=np.nan)\n",
        "total_vals2 = np.pad(total_vals2, (0, max_len - len(total_vals2)), constant_values=np.nan)\n",
        "total_vals3 = np.pad(total_vals3, (0, max_len - len(total_vals3)), constant_values=np.nan)\n",
        "\n",
        "# Create the DataFrame with exactly two columns\n",
        "df_pred = pd.DataFrame({\n",
        "    \"y_pred_total_adb1\": total_vals1,\n",
        "    \"y_pred_total_adb2\": total_vals2,\n",
        "    \"y_pred_total_adb3\": total_vals3\n",
        "})\n",
        "\n",
        "# Save to Excel (single sheet)\n",
        "df_pred.to_excel(\"adb_total_predictions.xlsx\", sheet_name=\"adb\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'adb_total_predictions.xlsx'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OOL_jm9PvNFP",
      "metadata": {
        "id": "OOL_jm9PvNFP"
      },
      "source": [
        "# **Results and Comparison**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C9ZLP1E2JXee",
      "metadata": {
        "id": "C9ZLP1E2JXee",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Plots and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EZbjD_PQX04z",
      "metadata": {
        "id": "EZbjD_PQX04z"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "file_name2 = list(uploaded.keys())[0]\n",
        "df_pred = pd.read_excel(file_name2)\n",
        "all_sheets = pd.read_excel(\"predictions.xlsx\", sheet_name=None)\n",
        "\n",
        "## --------------------------------------------\n",
        "\n",
        "# df_pred = pd.read_excel(\"C:\\\\Users\\\\Pgshco\\\\Desktop\\\\Python ML\\\\Carbon Capture\\\\ML codes\\\\Final Code\\\\predictions.xlsx\")\n",
        "# all_sheets = pd.read_excel(\"C:\\\\Users\\\\Pgshco\\\\Desktop\\\\Python ML\\\\Carbon Capture\\\\ML codes\\\\Final Code\\\\predictions.xlsx\", sheet_name=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HJLsBM-6UgC-",
      "metadata": {
        "id": "HJLsBM-6UgC-",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Loop through each sheet and create variables dynamically\n",
        "for sheet_name, df_pred in all_sheets.items():\n",
        "    model_name = ''.join([c for c in sheet_name if not c.isdigit()])  # e.g., 'gbr'\n",
        "    target_num = ''.join([c for c in sheet_name if c.isdigit()])      # e.g., '3'\n",
        "\n",
        "    # Extract columns\n",
        "    train_col = df_pred.iloc[:, 0].values\n",
        "    test_col  = df_pred.iloc[:, 1].values\n",
        "\n",
        "    # Create variable names like y_pred_train_gbr3, y_pred_test_gbr3\n",
        "    globals()[f\"y_pred_train_{model_name}{target_num}\"] = train_col\n",
        "    globals()[f\"y_pred_test_{model_name}{target_num}\"]  = test_col\n",
        "\n",
        "    print(f\"Loaded: y_pred_train_{model_name}{target_num}, y_pred_test_{model_name}{target_num}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B3k1HsUpec5A",
      "metadata": {
        "id": "B3k1HsUpec5A",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "### Train\n",
        "# Iterate over a snapshot of current globals to avoid runtime error\n",
        "for name in list(globals().keys()):\n",
        "    value = globals()[name]\n",
        "    if name.startswith(\"y_pred_train_\") and isinstance(value, np.ndarray):\n",
        "        cleaned = value[~np.isnan(value)]   # remove NaN values\n",
        "        globals()[name] = cleaned\n",
        "        print(f\"{name}: cleaned -> {len(cleaned)} values\")\n",
        "\n",
        "\n",
        "\n",
        "### Test\n",
        "# Iterate over a snapshot of current globals to avoid runtime error\n",
        "for name in list(globals().keys()):\n",
        "    value = globals()[name]\n",
        "    if name.startswith(\"y_pred_test_\") and isinstance(value, np.ndarray):\n",
        "        cleaned = value[~np.isnan(value)]   # remove NaN values\n",
        "        globals()[name] = cleaned\n",
        "        print(f\"{name}: cleaned -> {len(cleaned)} values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "297d5111-2c4a-4e51-ac75-7067412fb16c",
      "metadata": {
        "id": "297d5111-2c4a-4e51-ac75-7067412fb16c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ===== Prepare true and predicted values =====\n",
        "y_true_train_rec = np.asarray(df_label1).ravel()\n",
        "y_true_train_sec = np.asarray(df_label2).ravel()\n",
        "y_true_train_pur = np.asarray(df_label3).ravel()\n",
        "\n",
        "y_true_test_rec = np.asarray(ts_label1).ravel()\n",
        "y_true_test_sec = np.asarray(ts_label2).ravel()\n",
        "y_true_test_pur = np.asarray(ts_label3).ravel()\n",
        "\n",
        "y_pred_train_rec = y_pred_train_ann1\n",
        "y_pred_train_sec = y_pred_train_ann2\n",
        "y_pred_train_pur = y_pred_train_ann3\n",
        "\n",
        "y_pred_test_rec = y_pred_test_ann1\n",
        "y_pred_test_sec = y_pred_test_ann2\n",
        "y_pred_test_pur = y_pred_test_ann3\n",
        "\n",
        "eps = 1e-8\n",
        "\n",
        "\n",
        "####### Recovery ##########\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Train\n",
        "rel_train = (y_pred_train_rec - y_true_train_rec) / (y_true_train_rec + eps) * 100\n",
        "plt.scatter(y_true_train_rec, rel_train,\n",
        "            s=45, alpha=0.75, color='red',\n",
        "            edgecolor='black', linewidth=0.4, label='Train')\n",
        "\n",
        "# Test\n",
        "rel_test = (y_pred_test_rec - y_true_test_rec) / (y_true_test_rec + eps) * 100\n",
        "plt.scatter(y_true_test_rec, rel_test,\n",
        "            s=45, alpha=0.75, color='blue',\n",
        "            edgecolor='black', linewidth=0.4, label='Test')\n",
        "\n",
        "plt.axhline(0, color='black', linestyle='--')\n",
        "plt.xlabel(\"Actual Recovery (%)\", fontsize=14)\n",
        "plt.ylabel(\"Relative deviation (%)\", fontsize=14)\n",
        "plt.title(\"ANN – Relative deviation for Recovery (%)\", fontsize=16)\n",
        "plt.legend(fontsize=14)\n",
        "plt.grid(True, linestyle='--', alpha=0.45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Figures/rel_dev_recovery.png\", dpi=600, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "####### SEC ##########\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "rel_train = (y_pred_train_sec - y_true_train_sec) / (y_true_train_sec + eps) * 100\n",
        "plt.scatter(y_true_train_sec, rel_train,\n",
        "            s=45, alpha=0.75, color='red',\n",
        "            edgecolor='black', linewidth=0.4, label='Train')\n",
        "\n",
        "rel_test = (y_pred_test_sec - y_true_test_sec) / (y_true_test_sec + eps) * 100\n",
        "plt.scatter(y_true_test_sec, rel_test,\n",
        "            s=45, alpha=0.75, color='blue',\n",
        "            edgecolor='black', linewidth=0.4, label='Test')\n",
        "\n",
        "plt.axhline(0, color='black', linestyle='--')\n",
        "plt.xlabel(\"Actual SEC (kJ/kg)\", fontsize=14)\n",
        "plt.ylabel(\"Relative deviation (%)\", fontsize=14)\n",
        "plt.title(\"ANN – Relative deviation for SEC (kJ/kg)\", fontsize=16)\n",
        "plt.legend(fontsize=14)\n",
        "plt.grid(True, linestyle='--', alpha=0.45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Figures/rel_dev_sec.png\", dpi=600, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "####### Purity ###########\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "rel_train = (y_pred_train_pur - y_true_train_pur) / (y_true_train_pur + eps) * 100\n",
        "plt.scatter(y_true_train_pur, rel_train,\n",
        "            s=45, alpha=0.75, color='red',\n",
        "            edgecolor='black', linewidth=0.4, label='Train')\n",
        "\n",
        "rel_test = (y_pred_test_pur - y_true_test_pur) / (y_true_test_pur + eps) * 100\n",
        "plt.scatter(y_true_test_pur, rel_test,\n",
        "            s=45, alpha=0.75, color='blue',\n",
        "            edgecolor='black', linewidth=0.4, label='Test')\n",
        "\n",
        "plt.axhline(0, color='black', linestyle='--')\n",
        "plt.xlabel(\"Actual Purity (mol%)\", fontsize=14)\n",
        "plt.ylabel(\"Relative deviation (%)\", fontsize=14)\n",
        "plt.title(\"ANN – Relative deviation for Purity (mol%)\", fontsize=16)\n",
        "plt.legend(fontsize=14)\n",
        "plt.grid(True, linestyle='--', alpha=0.45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Figures/rel_dev_purity.png\", dpi=600, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AkuusaK_i9kD",
      "metadata": {
        "id": "AkuusaK_i9kD",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- True values mapping ---\n",
        "true_values = {\n",
        "    1: (df_label1, ts_label1),\n",
        "    2: (df_label2, ts_label2),\n",
        "    3: (df_label3, ts_label3)\n",
        "}\n",
        "\n",
        "models = [\"ann\", \"tab\", \"xg\", \"gbr\", \"cb\", \"adb\"]\n",
        "metric_names = [\"R²\", \"Adj-R²\", \"MSE\", \"RMSE\", \"NRMSE\", \"std_er\",\"MAE\", \"MAPE\", \"MedAE\", \"EVS\"]\n",
        "\n",
        "results = []\n",
        "\n",
        "# --- Loop through each model and target ---\n",
        "for model in models:\n",
        "    for target in [1, 2, 3]:\n",
        "        y_train_true, y_test_true = true_values[target]\n",
        "        y_pred_train_var = f\"y_pred_train_{model}{target}\"\n",
        "        y_pred_test_var  = f\"y_pred_test_{model}{target}\"\n",
        "\n",
        "        # Skip missing variables\n",
        "        if y_pred_test_var not in globals():\n",
        "            print(f\"⚠️ Skipping {y_pred_test_var} (not found)\")\n",
        "            continue\n",
        "\n",
        "        y_pred_train = globals()[y_pred_train_var]\n",
        "        y_pred_test  = globals()[y_pred_test_var]\n",
        "\n",
        "        # Evaluate\n",
        "        train_metrics = evaluate(y_train_true, y_pred_train, label=\"Train\")\n",
        "        test_metrics  = evaluate(y_test_true,  y_pred_test,  label=\"Test\")\n",
        "\n",
        "        # Combine metrics side-by-side (Train_Test pairs)\n",
        "        combined_metrics = {\"Model\": model.upper(), \"Target\": {1:\"Recovery\",2:\"SEC\",3:\"Purity\"}[target]}\n",
        "        for i, metric in enumerate(metric_names):\n",
        "            combined_metrics[f\"{metric}_Train\"] = train_metrics[i]\n",
        "            combined_metrics[f\"{metric}_Test\"]  = test_metrics[i]\n",
        "\n",
        "        results.append(combined_metrics)\n",
        "\n",
        "# --- Create final DataFrame ---\n",
        "metrics_table = pd.DataFrame(results)\n",
        "\n",
        "# --- Display neatly ---\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", 200)\n",
        "display(metrics_table)\n",
        "\n",
        "# --- Save to Excel ---\n",
        "metrics_table.to_excel(\"All_Model_Metrics.xlsx\", index=False)\n",
        "print(\"✅ Saved metrics table with Train/Test pairs to 'All_Model_Metrics.xlsx'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yvRM1XFVqvOV",
      "metadata": {
        "id": "yvRM1XFVqvOV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "# Explicit mapping for true values (no globals lookups)\n",
        "true_values = {\n",
        "    1: np.asarray(ts_label1, dtype=float),\n",
        "    2: np.asarray(ts_label2, dtype=float),\n",
        "    3: np.asarray(ts_label3, dtype=float),\n",
        "}\n",
        "\n",
        "models = [\n",
        "    (\"ann\", \"ANN\"),\n",
        "    (\"tab\", \"TabPFN\"),\n",
        "    (\"xg\",  \"PSO-XGBoost\"),\n",
        "    (\"gbr\", \"PSO-GBR\"),\n",
        "    (\"cb\", \"PSO-CatBoost\"),\n",
        "    (\"adb\", \"PSO-AdaBoost\")\n",
        "]\n",
        "target_names = {1: \"Recovery\", 2: \"SEC\", 3: \"Purity\"}\n",
        "\n",
        "fig, axes = plt.subplots(nrows=6, ncols=3, figsize=(14, 20))\n",
        "for r, (mkey, mname) in enumerate(models):\n",
        "    for c, t in enumerate([1, 2, 3]):\n",
        "        ax = axes[r, c]\n",
        "        y_true = true_values[t]\n",
        "        # predictions still fetched by name:\n",
        "        y_pred = np.asarray(globals()[f\"y_pred_test_{mkey}{t}\"], dtype=float)\n",
        "\n",
        "        mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
        "        yt, yp = y_true[mask], y_pred[mask]\n",
        "\n",
        "        ax.scatter(yt, yp, alpha=0.7, edgecolor=\"k\")\n",
        "        mn, mx = float(min(yt.min(), yp.min())), float(max(yt.max(), yp.max()))\n",
        "        ax.plot([mn, mx], [mn, mx], \"r--\", linewidth=2)\n",
        "\n",
        "        tname = target_names[t]\n",
        "        ax.set_xlabel(f\"Actual {tname}\")\n",
        "        ax.set_ylabel(f\"Predicted {tname}\")\n",
        "        ax.set_title(f\"{mname}: Predicted vs Actual (Test)\")\n",
        "        ax.grid(True)\n",
        "\n",
        "        # Format SEC axes as ×10^5\n",
        "        if t == 2:\n",
        "            ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x/1e5:.0f}\"))\n",
        "            ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: f\"{y/1e5:.0f}\"))\n",
        "            ax.text(0.98, -0.14, r\"$\\times10^{5}$\",\n",
        "                    transform=ax.transAxes, ha=\"right\", va=\"center\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"All_Model_Scatter.svg\", format=\"svg\", dpi=300)\n",
        "plt.show()\n",
        "print(\"✅ Saved as All_Model_Scatter.svg\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kU3UfBvO94Uc",
      "metadata": {
        "id": "kU3UfBvO94Uc"
      },
      "source": [
        "## 5-fold cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z_Agu000-HyT",
      "metadata": {
        "id": "z_Agu000-HyT"
      },
      "source": [
        "### Recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VYNWKkkB-N-q",
      "metadata": {
        "id": "VYNWKkkB-N-q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Assuming your DataFrame is df\n",
        "# X = features, y = target\n",
        "\n",
        "X = x_total.values  # all columns except last\n",
        "y = y_total1.values # Recovery\n",
        "\n",
        "# Define a function to create the ANN model\n",
        "def create_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dense(72, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))  # regression output\n",
        "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
        "    return model\n",
        "\n",
        "# K-Fold Cross Validation\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "r2_scores = []\n",
        "rmse_scores = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X), 1):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    model = create_model(input_dim=X.shape[1])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "\n",
        "    # Predict on validation set\n",
        "    y_pred = model.predict(X_val).flatten()\n",
        "\n",
        "    # Evaluate metrics\n",
        "    r2 = r2_score(y_val, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "\n",
        "    r2_scores.append(r2)\n",
        "    rmse_scores.append(rmse)\n",
        "\n",
        "    print(f\"Fold {fold}: R2 = {r2:.4f}, RMSE = {rmse:.4f}\")\n",
        "\n",
        "# Final average metrics\n",
        "print(f\"\\nAverage R2: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1GOrCEab-Lrr",
      "metadata": {
        "id": "1GOrCEab-Lrr",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### SEC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p-vMLnkD940d",
      "metadata": {
        "id": "p-vMLnkD940d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Assuming your DataFrame is df\n",
        "# X = features, y = target\n",
        "\n",
        "X = x_total.values  # all columns except last\n",
        "y = y_total2.values # Recovery\n",
        "\n",
        "# Define a function to create the ANN model\n",
        "def create_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(120, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))  # regression output\n",
        "    model.compile(optimizer=Adam(learning_rate=0.1), loss='mse')\n",
        "    return model\n",
        "\n",
        "# K-Fold Cross Validation\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "r2_scores = []\n",
        "rmse_scores = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X), 1):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    model = create_model(input_dim=X.shape[1])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "\n",
        "    # Predict on validation set\n",
        "    y_pred = model.predict(X_val).flatten()\n",
        "\n",
        "    # Evaluate metrics\n",
        "    r2 = r2_score(y_val, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "\n",
        "    r2_scores.append(r2)\n",
        "    rmse_scores.append(rmse)\n",
        "\n",
        "    print(f\"Fold {fold}: R2 = {r2:.4f}, RMSE = {rmse:.4f}\")\n",
        "\n",
        "# Final average metrics\n",
        "print(f\"\\nAverage R2: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEC with the tunned hyper parameter of Rec and Pu"
      ],
      "metadata": {
        "id": "Rt296JdBgnTI"
      },
      "id": "Rt296JdBgnTI"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Assuming your DataFrame is df\n",
        "# X = features, y = target\n",
        "\n",
        "X = x_total.values  # all columns except last\n",
        "y = y_total2.values # Recovery\n",
        "\n",
        "# Define a function to create the ANN model\n",
        "def create_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dense(72, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))  # regression output\n",
        "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
        "    return model\n",
        "\n",
        "# K-Fold Cross Validation\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "r2_scores = []\n",
        "rmse_scores = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X), 1):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    model = create_model(input_dim=X.shape[1])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "\n",
        "    # Predict on validation set\n",
        "    y_pred = model.predict(X_val).flatten()\n",
        "\n",
        "    # Evaluate metrics\n",
        "    r2 = r2_score(y_val, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "\n",
        "    r2_scores.append(r2)\n",
        "    rmse_scores.append(rmse)\n",
        "\n",
        "    print(f\"Fold {fold}: R2 = {r2:.4f}, RMSE = {rmse:.4f}\")\n",
        "\n",
        "# Final average metrics\n",
        "print(f\"\\nAverage R2: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "ySGKEhQgfLr5"
      },
      "id": "ySGKEhQgfLr5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "FzqOuDpr-Olr",
      "metadata": {
        "id": "FzqOuDpr-Olr",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Purity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ge2tBMf7-Q9G",
      "metadata": {
        "id": "ge2tBMf7-Q9G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Assuming your DataFrame is df\n",
        "# X = features, y = target\n",
        "\n",
        "X = x_total.values  # all columns except last\n",
        "y = y_total3.values # Recovery\n",
        "\n",
        "# Define a function to create the ANN model\n",
        "def create_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dense(72, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))  # regression output\n",
        "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
        "    return model\n",
        "\n",
        "# K-Fold Cross Validation\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "r2_scores = []\n",
        "rmse_scores = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X), 1):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    model = create_model(input_dim=X.shape[1])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "\n",
        "    # Predict on validation set\n",
        "    y_pred = model.predict(X_val).flatten()\n",
        "\n",
        "    # Evaluate metrics\n",
        "    r2 = r2_score(y_val, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "\n",
        "    r2_scores.append(r2)\n",
        "    rmse_scores.append(rmse)\n",
        "\n",
        "    print(f\"Fold {fold}: R2 = {r2:.4f}, RMSE = {rmse:.4f}\")\n",
        "\n",
        "# Final average metrics\n",
        "print(f\"\\nAverage R2: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-4EeQQObeYmW",
      "metadata": {
        "id": "-4EeQQObeYmW"
      },
      "source": [
        "## Shap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DGHPVYQgtzOY",
      "metadata": {
        "id": "DGHPVYQgtzOY",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8VsGa8eMgUFK",
      "metadata": {
        "id": "8VsGa8eMgUFK"
      },
      "outputs": [],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "342f3b8b-76a3-4572-a756-1e05514cc82e",
      "metadata": {
        "id": "342f3b8b-76a3-4572-a756-1e05514cc82e"
      },
      "outputs": [],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BqionG8PgSRC",
      "metadata": {
        "collapsed": true,
        "id": "BqionG8PgSRC",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "from shap import summary_plot\n",
        "\n",
        "# Model\n",
        "# Recovery\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "X_train = df\n",
        "y_train1 = df_label1\n",
        "X_test = ts\n",
        "n_features = X_train.shape[1]\n",
        "# Best model\n",
        "ann_model1 = keras.Sequential([\n",
        "    layers.Input(shape=(n_features,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(72, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)                       # single output\n",
        "])\n",
        "\n",
        "# Compile\n",
        "ann_model1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "                   loss='mean_absolute_error')\n",
        "# fit/predict/evaluate\n",
        "ann_model1.fit(X_train, y_train1, epochs=100, batch_size=32, verbose=0)\n",
        "\n",
        "\n",
        "feature_names = list(df.columns[:-1])\n",
        "explainer = shap.Explainer(ann_model1.predict, X_train, feature_names = feature_names)\n",
        "shap_values = explainer(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RQpeH6rkkCjM",
      "metadata": {
        "id": "RQpeH6rkkCjM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Column index for \"Unnamed: 0\"\n",
        "col_index = X_test.columns.get_loc(\"Unnamed: 0\")\n",
        "\n",
        "# 1. Drop column from the feature dataframe\n",
        "X_test_clean = X_test.drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "# 2. Remove the same column from SHAP values\n",
        "values_clean = np.delete(shap_values.values, col_index, axis=1)\n",
        "\n",
        "# Optional: SHAP stores the original features in .data\n",
        "if shap_values.data is not None:\n",
        "    data_clean = np.delete(shap_values.data, col_index, axis=1)\n",
        "else:\n",
        "    data_clean = None\n",
        "\n",
        "# Rebuild the Explanation object\n",
        "shap_values_clean = shap.Explanation(\n",
        "    values=values_clean,\n",
        "    base_values=shap_values.base_values,\n",
        "    data=data_clean,\n",
        "    feature_names=[name for i, name in enumerate(shap_values.feature_names) if i != col_index]\n",
        ")\n",
        "# --- BAR PLOT ---\n",
        "plt.figure(figsize=(20, 16))   # <<< CRITICAL FIX\n",
        "shap.summary_plot(\n",
        "    shap_values_clean,\n",
        "    X_test_clean,\n",
        "    plot_type=\"bar\",\n",
        "    show=False                # prevent SHAP from clearing the figure\n",
        ")\n",
        "plt.gca().set_xlabel(\"mean(|SHAP value|)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- DOT PLOT ---\n",
        "shap.summary_plot(shap_values_clean, X_test_clean)\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aB2EUf71vjK3",
      "metadata": {
        "id": "aB2EUf71vjK3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "\n",
        "# --- BAR PLOT ---\n",
        "shap.summary_plot(shap_values_clean, X_test_clean, plot_type=\"bar\", show=False)\n",
        "plt.gca().set_xlabel(\"mean(|SHAP value|)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"shap_rec_bar.png\", dpi=600, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# --- DOT PLOT ---\n",
        "shap.summary_plot(shap_values_clean, X_test_clean, show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"shap_rec_dot.png\", dpi=600, bbox_inches='tight')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rWskTFlJt4qQ",
      "metadata": {
        "id": "rWskTFlJt4qQ",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### SEC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2339a0b2-6fc9-4962-959c-4ad55dd11f5e",
      "metadata": {
        "collapsed": true,
        "id": "2339a0b2-6fc9-4962-959c-4ad55dd11f5e",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install shap\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Urn-_F5ouDA_",
      "metadata": {
        "collapsed": true,
        "id": "Urn-_F5ouDA_",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "from shap import summary_plot\n",
        "\n",
        "# Model\n",
        "# Recovery\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "X_train = df.drop(columns=['Unnamed: 0'])\n",
        "y_train2 = df_label2\n",
        "X_test = ts.drop(columns=['Unnamed: 0'])\n",
        "n_features = X_train.shape[1]\n",
        "# Best model\n",
        "ann_model1 = keras.Sequential([\n",
        "    layers.Input(shape=(n_features,)),\n",
        "    layers.Dense(120, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1)                       # single output\n",
        "])\n",
        "\n",
        "# Compile\n",
        "ann_model1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "                   loss='mean_absolute_error')\n",
        "# fit/predict/evaluate\n",
        "ann_model1.fit(X_train, y_train2, epochs=101, batch_size=32, verbose=0)\n",
        "\n",
        "\n",
        "feature_names = list(df.columns[:-1])\n",
        "explainer = shap.Explainer(ann_model1.predict, X_train, feature_names = feature_names)\n",
        "shap_values = explainer(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IQYkHh6q1p66",
      "metadata": {
        "id": "IQYkHh6q1p66"
      },
      "outputs": [],
      "source": [
        "# --- BAR PLOT ---\n",
        "shap.summary_plot(\n",
        "    shap_values,\n",
        "    X_test,\n",
        "    plot_type=\"bar\",\n",
        "    show=False                # prevent SHAP from clearing the figure\n",
        ")\n",
        "plt.gca().set_xlabel(\"mean(|SHAP value|)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- DOT PLOT ---\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V_W37bXcuDEy",
      "metadata": {
        "id": "V_W37bXcuDEy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "\n",
        "# --- BAR PLOT ---\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
        "plt.gca().set_xlabel(\"mean(|SHAP value|)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"shap_sec_bar.png\", dpi=600, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# --- DOT PLOT ---\n",
        "shap.summary_plot(shap_values, X_test, show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"shap_sec_dot.png\", dpi=600, bbox_inches='tight')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uUIaiK7r1VBC",
      "metadata": {
        "id": "uUIaiK7r1VBC",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Purity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aJixt-4H1YIB",
      "metadata": {
        "collapsed": true,
        "id": "aJixt-4H1YIB",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "from shap import summary_plot\n",
        "\n",
        "# Model\n",
        "# Recovery\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "X_train = df.drop(columns=['Unnamed: 0'])\n",
        "y_train3 = df_label3\n",
        "X_test = ts.drop(columns=['Unnamed: 0'])\n",
        "n_features = X_train.shape[1]\n",
        "# Best model\n",
        "ann_model1 = keras.Sequential([\n",
        "    layers.Input(shape=(n_features,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(72, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)                       # single output\n",
        "])\n",
        "\n",
        "# Compile\n",
        "ann_model1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "                   loss='mean_absolute_error')\n",
        "# fit/predict/evaluate\n",
        "ann_model1.fit(X_train, y_train3, epochs=100, batch_size=32, verbose=0)\n",
        "\n",
        "\n",
        "feature_names = list(df.columns[:-1])\n",
        "explainer = shap.Explainer(ann_model1.predict, X_train, feature_names = feature_names)\n",
        "shap_values = explainer(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2y5psKim69tH",
      "metadata": {
        "id": "2y5psKim69tH"
      },
      "outputs": [],
      "source": [
        "# --- BAR PLOT ---\n",
        "shap.summary_plot(\n",
        "    shap_values,\n",
        "    X_test,\n",
        "    plot_type=\"bar\",\n",
        "    show=False                # prevent SHAP from clearing the figure\n",
        ")\n",
        "plt.gca().set_xlabel(\"mean(|SHAP value|)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- DOT PLOT ---\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZeF9ICPk7A40",
      "metadata": {
        "id": "ZeF9ICPk7A40"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "\n",
        "# --- BAR PLOT ---\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
        "plt.gca().set_xlabel(\"mean(|SHAP value|)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"shap_pur_bar.png\", dpi=600, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# --- DOT PLOT ---\n",
        "shap.summary_plot(shap_values, X_test, show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"shap_pur_dot.png\", dpi=600, bbox_inches='tight')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9G17y8majDLw",
      "metadata": {
        "id": "9G17y8majDLw"
      },
      "source": [
        "## William's Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recovery**"
      ],
      "metadata": {
        "id": "BkDpPaw11G9o"
      },
      "id": "BkDpPaw11G9o"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "# محاسبه استاندارد شده‌ی باقیمانده‌ها و leverage\n",
        "y_true = df_label1\n",
        "X_train = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "\n",
        "# model\n",
        "n_features = X_train.shape[1]\n",
        "# Best model\n",
        "ann_model1 = keras.Sequential([\n",
        "    layers.Input(shape=(n_features,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(72, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)                       # single output\n",
        "])\n",
        "\n",
        "# Compile\n",
        "ann_model1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "                   loss='mean_absolute_error')\n",
        "# fit/predict/evaluate\n",
        "ann_model1.fit(X_train, y_true, epochs=101, batch_size=32, verbose=0)\n",
        "\n",
        "\n",
        "y_pred = ann_model1.predict(X_train)\n",
        "y_pred = np.ravel(y_pred)          # (3540,)\n",
        "y_true = np.ravel(y_true)          # (3540,)\n",
        "residuals = y_true - y_pred\n",
        "std_residuals = residuals / np.std(residuals)\n",
        "\n",
        "# طراحی X با ستون ثابت\n",
        "X_design = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
        "XT_X_inv = np.linalg.inv(X_design.T.dot(X_design))\n",
        "H = X_design @ XT_X_inv @ X_design.T\n",
        "leverage = np.diag(H)\n",
        "\n",
        "# آستانه‌ها\n",
        "n, p = X_train.shape\n",
        "h_star = 3*(p+1)/n\n",
        "resid_thresh = 3\n",
        "\n",
        "# دسته‌بندی نقاط\n",
        "valid_idx     = (np.abs(std_residuals) <= resid_thresh) & (leverage <= h_star)\n",
        "suspect_idx   = (np.abs(std_residuals) >  resid_thresh) & (leverage <= h_star)\n",
        "leverage_idx  = (leverage > h_star) & (np.abs(std_residuals) <= resid_thresh)\n",
        "both_idx      = (leverage > h_star) & (np.abs(std_residuals) > resid_thresh)\n",
        "\n",
        "# رسم نمودار\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.axhline(resid_thresh,  color='darkorange', linestyle='--', label='Upper suspected limit')\n",
        "plt.axhline(-resid_thresh, color='darkorange', linestyle='--', label='Lower suspected limit')\n",
        "plt.axvline(h_star,        color='purple', linestyle='--', label='Out of Leverage')\n",
        "\n",
        "# نقاط\n",
        "plt.scatter(leverage[valid_idx], std_residuals[valid_idx],   c='darkblue', label='Valid data', s=50)\n",
        "plt.scatter(leverage[suspect_idx], std_residuals[suspect_idx], c='red', label='Suspected data', edgecolor='black', s=80)\n",
        "plt.scatter(leverage[leverage_idx], std_residuals[leverage_idx], c='lime', label='Out of Leverage', edgecolor='black', s=80)\n",
        "plt.scatter(leverage[both_idx], std_residuals[both_idx], c='maroon', label='Suspected & Out of Leverage', s=90, edgecolor='black')\n",
        "\n",
        "plt.xlabel('Hat')\n",
        "plt.ylabel('Standardized residuals')\n",
        "plt.title('Leverage vs Standardized Residuals (ANN-Recovery)')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"leverage_residuals_rec.png\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the number of points in each category\n",
        "num_valid = np.sum(valid_idx)\n",
        "num_suspect = np.sum(suspect_idx)\n",
        "num_out_of_leverage = np.sum(leverage_idx)\n",
        "num_both = np.sum(both_idx)\n",
        "\n",
        "# Total points\n",
        "total_points = len(X_train)\n",
        "\n",
        "# Calculate the percentages\n",
        "valid_percentage = (num_valid / total_points) * 100\n",
        "suspect_percentage = (num_suspect / total_points) * 100\n",
        "out_of_leverage_percentage = (num_out_of_leverage / total_points) * 100\n",
        "both_percentage = (num_both / total_points) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Number of valid data points: {num_valid} ({valid_percentage:.2f}%)\")\n",
        "print(f\"Number of suspected data points: {num_suspect} ({suspect_percentage:.2f}%)\")\n",
        "print(f\"Number of out of leverage data points: {num_out_of_leverage} ({out_of_leverage_percentage:.2f}%)\")\n",
        "print(f\"Number of suspected & out of leverage data points: {num_both} ({both_percentage:.2f}%)\")"
      ],
      "metadata": {
        "id": "wRMrZtFA1Kf-"
      },
      "id": "wRMrZtFA1Kf-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SEC**"
      ],
      "metadata": {
        "id": "o16XoSnr1Kz-"
      },
      "id": "o16XoSnr1Kz-"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "# محاسبه استاندارد شده‌ی باقیمانده‌ها و leverage\n",
        "y_true = df_label2\n",
        "X_train = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# model\n",
        "n_features = X_train.shape[1]\n",
        "# Best model\n",
        "ann_model1 = keras.Sequential([\n",
        "    layers.Input(shape=(n_features,)),\n",
        "    layers.Dense(120, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1)                       # single output\n",
        "])\n",
        "\n",
        "# Compile\n",
        "ann_model1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.1),\n",
        "                   loss='mean_absolute_error')\n",
        "# fit/predict/evaluate\n",
        "ann_model1.fit(X_train, y_true, epochs=101, batch_size=32, verbose=0)\n",
        "\n",
        "y_pred = ann_model1.predict(X_train)\n",
        "y_pred = np.ravel(y_pred)          # (3540,)\n",
        "y_true = np.ravel(y_true)          # (3540,)\n",
        "residuals = y_true - y_pred\n",
        "std_residuals = residuals / np.std(residuals)\n",
        "\n",
        "# طراحی X با ستون ثابت\n",
        "X_design = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
        "XT_X_inv = np.linalg.inv(X_design.T.dot(X_design))\n",
        "H = X_design @ XT_X_inv @ X_design.T\n",
        "leverage = np.diag(H)\n",
        "\n",
        "# آستانه‌ها\n",
        "n, p = X_train.shape\n",
        "h_star = 3*(p+1)/n\n",
        "resid_thresh = 3\n",
        "\n",
        "# دسته‌بندی نقاط\n",
        "valid_idx     = (np.abs(std_residuals) <= resid_thresh) & (leverage <= h_star)\n",
        "suspect_idx   = (np.abs(std_residuals) >  resid_thresh) & (leverage <= h_star)\n",
        "leverage_idx  = (leverage > h_star) & (np.abs(std_residuals) <= resid_thresh)\n",
        "both_idx      = (leverage > h_star) & (np.abs(std_residuals) > resid_thresh)\n",
        "\n",
        "# رسم نمودار\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.axhline(resid_thresh,  color='darkorange', linestyle='--', label='Upper suspected limit')\n",
        "plt.axhline(-resid_thresh, color='darkorange', linestyle='--', label='Lower suspected limit')\n",
        "plt.axvline(h_star,        color='purple', linestyle='--', label='Out of Leverage')\n",
        "\n",
        "# نقاط\n",
        "plt.scatter(leverage[valid_idx], std_residuals[valid_idx],   c='darkblue', label='Valid data', s=50)\n",
        "plt.scatter(leverage[suspect_idx], std_residuals[suspect_idx], c='red', label='Suspected data', edgecolor='black', s=80)\n",
        "plt.scatter(leverage[leverage_idx], std_residuals[leverage_idx], c='lime', label='Out of Leverage', edgecolor='black', s=80)\n",
        "plt.scatter(leverage[both_idx], std_residuals[both_idx], c='maroon', label='Suspected & Out of Leverage', s=90, edgecolor='black')\n",
        "\n",
        "plt.xlabel('Hat')\n",
        "plt.ylabel('Standardized residuals')\n",
        "plt.title('Leverage vs Standardized Residuals (ANN-SEC)')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"leverage_residuals_sec.png\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the number of points in each category\n",
        "num_valid = np.sum(valid_idx)\n",
        "num_suspect = np.sum(suspect_idx)\n",
        "num_out_of_leverage = np.sum(leverage_idx)\n",
        "num_both = np.sum(both_idx)\n",
        "\n",
        "# Total points\n",
        "total_points = len(X_train)\n",
        "\n",
        "# Calculate the percentages\n",
        "valid_percentage = (num_valid / total_points) * 100\n",
        "suspect_percentage = (num_suspect / total_points) * 100\n",
        "out_of_leverage_percentage = (num_out_of_leverage / total_points) * 100\n",
        "both_percentage = (num_both / total_points) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Number of valid data points: {num_valid} ({valid_percentage:.2f}%)\")\n",
        "print(f\"Number of suspected data points: {num_suspect} ({suspect_percentage:.2f}%)\")\n",
        "print(f\"Number of out of leverage data points: {num_out_of_leverage} ({out_of_leverage_percentage:.2f}%)\")\n",
        "print(f\"Number of suspected & out of leverage data points: {num_both} ({both_percentage:.2f}%)\")"
      ],
      "metadata": {
        "id": "3u_hNe7Q3hKE"
      },
      "id": "3u_hNe7Q3hKE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purity**"
      ],
      "metadata": {
        "id": "kvC0xsMI1Njo"
      },
      "id": "kvC0xsMI1Njo"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "# محاسبه استاندارد شده‌ی باقیمانده‌ها و leverage\n",
        "y_true = df_label3\n",
        "X_train = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "\n",
        "# model\n",
        "n_features = X_train.shape[1]\n",
        "# Best model\n",
        "ann_model1 = keras.Sequential([\n",
        "    layers.Input(shape=(n_features,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(72, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)                       # single output\n",
        "])\n",
        "\n",
        "# Compile\n",
        "ann_model1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "                   loss='mean_absolute_error')\n",
        "# fit/predict/evaluate\n",
        "ann_model1.fit(X_train, y_true, epochs=101, batch_size=32, verbose=0)\n",
        "\n",
        "\n",
        "y_pred = ann_model1.predict(X_train)\n",
        "y_pred = np.ravel(y_pred)          # (3540,)\n",
        "y_true = np.ravel(y_true)          # (3540,)\n",
        "residuals = y_true - y_pred\n",
        "std_residuals = residuals / np.std(residuals)\n",
        "\n",
        "# طراحی X با ستون ثابت\n",
        "X_design = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
        "XT_X_inv = np.linalg.inv(X_design.T.dot(X_design))\n",
        "H = X_design @ XT_X_inv @ X_design.T\n",
        "leverage = np.diag(H)\n",
        "\n",
        "# آستانه‌ها\n",
        "n, p = X_train.shape\n",
        "h_star = 3*(p+1)/n\n",
        "resid_thresh = 3\n",
        "\n",
        "# دسته‌بندی نقاط\n",
        "valid_idx     = (np.abs(std_residuals) <= resid_thresh) & (leverage <= h_star)\n",
        "suspect_idx   = (np.abs(std_residuals) >  resid_thresh) & (leverage <= h_star)\n",
        "leverage_idx  = (leverage > h_star) & (np.abs(std_residuals) <= resid_thresh)\n",
        "both_idx      = (leverage > h_star) & (np.abs(std_residuals) > resid_thresh)\n",
        "\n",
        "# رسم نمودار\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.axhline(resid_thresh,  color='darkorange', linestyle='--', label='Upper suspected limit')\n",
        "plt.axhline(-resid_thresh, color='darkorange', linestyle='--', label='Lower suspected limit')\n",
        "plt.axvline(h_star,        color='purple', linestyle='--', label='Out of Leverage')\n",
        "\n",
        "# نقاط\n",
        "plt.scatter(leverage[valid_idx], std_residuals[valid_idx],   c='darkblue', label='Valid data', s=50)\n",
        "plt.scatter(leverage[suspect_idx], std_residuals[suspect_idx], c='red', label='Suspected data', edgecolor='black', s=80)\n",
        "plt.scatter(leverage[leverage_idx], std_residuals[leverage_idx], c='lime', label='Out of Leverage', edgecolor='black', s=80)\n",
        "plt.scatter(leverage[both_idx], std_residuals[both_idx], c='maroon', label='Suspected & Out of Leverage', s=90, edgecolor='black')\n",
        "\n",
        "plt.xlabel('Hat')\n",
        "plt.ylabel('Standardized residuals')\n",
        "plt.title('Leverage vs Standardized Residuals (ANN-Purity)')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"leverage_residuals_pur.png\", dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the number of points in each category\n",
        "num_valid = np.sum(valid_idx)\n",
        "num_suspect = np.sum(suspect_idx)\n",
        "num_out_of_leverage = np.sum(leverage_idx)\n",
        "num_both = np.sum(both_idx)\n",
        "\n",
        "# Total points\n",
        "total_points = len(X_train)\n",
        "\n",
        "# Calculate the percentages\n",
        "valid_percentage = (num_valid / total_points) * 100\n",
        "suspect_percentage = (num_suspect / total_points) * 100\n",
        "out_of_leverage_percentage = (num_out_of_leverage / total_points) * 100\n",
        "both_percentage = (num_both / total_points) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Number of valid data points: {num_valid} ({valid_percentage:.2f}%)\")\n",
        "print(f\"Number of suspected data points: {num_suspect} ({suspect_percentage:.2f}%)\")\n",
        "print(f\"Number of out of leverage data points: {num_out_of_leverage} ({out_of_leverage_percentage:.2f}%)\")\n",
        "print(f\"Number of suspected & out of leverage data points: {num_both} ({both_percentage:.2f}%)\")"
      ],
      "metadata": {
        "id": "od26tiE61StV"
      },
      "id": "od26tiE61StV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Surface plot"
      ],
      "metadata": {
        "id": "Yw63KmZmZDrH"
      },
      "id": "Yw63KmZmZDrH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recovery**"
      ],
      "metadata": {
        "id": "fiN3meAzH3Yb"
      },
      "id": "fiN3meAzH3Yb"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from mpl_toolkits.mplot3d import Axes3D  # This is needed for 3D plots\n",
        "\n",
        "def surface_plot_2d(model, X_ref_df, feat_x, feat_y, ax, title=\"Surface plot\", n_grid=60, fixed_strategy=\"median\", cmap=\"viridis\"):\n",
        "    \"\"\"\n",
        "    model: trained model with predict(X)\n",
        "    X_ref_df: pandas DataFrame of features (use train or total)\n",
        "    feat_x, feat_y: two feature names to vary\n",
        "    ax: Matplotlib axis to plot on (for subplot layout)\n",
        "    fixed_strategy: \"median\" or \"mean\"\n",
        "    cmap: Colormap for the surface plot\n",
        "    \"\"\"\n",
        "    X_ref = X_ref_df.copy()\n",
        "\n",
        "    # fixed point for other features\n",
        "    if fixed_strategy == \"median\":\n",
        "        base = X_ref.median(numeric_only=True)\n",
        "    else:\n",
        "        base = X_ref.mean(numeric_only=True)\n",
        "\n",
        "    # grid ranges based on data\n",
        "    x_vals = np.linspace(X_ref[feat_x].min(), X_ref[feat_x].max(), n_grid)\n",
        "    y_vals = np.linspace(X_ref[feat_y].min(), X_ref[feat_y].max(), n_grid)\n",
        "    XX, YY = np.meshgrid(x_vals, y_vals)\n",
        "\n",
        "    # build grid dataframe for prediction\n",
        "    grid = pd.DataFrame(np.tile(base.values, (n_grid*n_grid, 1)), columns=base.index)\n",
        "    grid[feat_x] = XX.ravel()\n",
        "    grid[feat_y] = YY.ravel()\n",
        "\n",
        "    # ensure column order matches training\n",
        "    grid = grid[X_ref_df.columns]\n",
        "\n",
        "    ZZ = model.predict(grid).reshape(n_grid, n_grid)\n",
        "\n",
        "    # --- Plot on provided 3D axis ---\n",
        "    surface = ax.plot_surface(XX, YY, ZZ, linewidth=0, antialiased=True, alpha=0.9, cmap=cmap)\n",
        "    ax.set_xlabel(feat_x)\n",
        "    ax.set_ylabel(feat_y)\n",
        "    ax.set_zlabel(\"Predicted\")\n",
        "    ax.set_title(title)\n",
        "\n",
        "    # Add color bar for each surface plot\n",
        "    fig.colorbar(surface, ax=ax, shrink=0.5, aspect=10)\n",
        "\n",
        "# Prepare plot grid with 3 rows and 2 columns\n",
        "fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
        "\n",
        "# Make sure to create 3D axes for each subplot\n",
        "for row in range(3):\n",
        "    for col in range(2):\n",
        "        axes[row, col] = fig.add_subplot(3, 2, 2 * row + col + 1, projection=\"3d\")  # Convert to 3D axis\n",
        "\n",
        "# List of features pairs and titles for surface plots and different colormaps\n",
        "plot_configs = [\n",
        "    (\"Fluegas Flow (kg/h)\", \"Amine type\", \"Recovery surface: flue gas flow vs amine type\", \"coolwarm\"),\n",
        "    (\"Fluegas Flow (kg/h)\", \"Fluegas CO2 mol%\", \"Recovery surface: flue gas flow vs CO2 mol%\", \"coolwarm\"),\n",
        "    (\"Fluegas Flow (kg/h)\", \"Lean Solvent Loading\", \"Recovery surface: flue gas flow vs loading\", \"coolwarm\"),\n",
        "    (\"Fluegas CO2 mol%\", \"Amine type\", \"Recovery surface: CO2 mol% vs amine type\", \"coolwarm\"),\n",
        "    (\"Fluegas CO2 mol%\", \"Lean Solvent Loading\", \"Recovery surface: CO2 mol% vs loading\", \"coolwarm\"),\n",
        "    (\"Amine type\", \"Lean Solvent Loading\", \"Recovery surface: amine type vs loading\", \"coolwarm\")\n",
        "]\n",
        "\n",
        "# Ensure the correct feature names and target values are loaded\n",
        "X_train = df  # X_train should be your feature DataFrame\n",
        "y_true = df_label1  # Ensure you have the correct target for the ANN\n",
        "\n",
        "# Model setup (ensure it's already trained)\n",
        "n_features = X_train.shape[1]  # Your features are in X_train\n",
        "ann_model1 = keras.Sequential([\n",
        "    layers.Input(shape=(n_features,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(72, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Single output\n",
        "])\n",
        "\n",
        "ann_model1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mean_absolute_error')\n",
        "ann_model1.fit(X_train, y_true, epochs=101, batch_size=32, verbose=0)\n",
        "\n",
        "# Generate surface plots for each pair of features in the configuration list\n",
        "for i, (feat_x, feat_y, title, cmap) in enumerate(plot_configs):\n",
        "    row, col = divmod(i, 2)\n",
        "    surface_plot_2d(ann_model1, X_train, feat_x, feat_y, axes[row, col], title=title, n_grid=100, cmap=cmap)\n",
        "\n",
        "# Adjust layout and save as SVG\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"surface_plots_rec.svg\", format=\"svg\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w-AJH9rZXVQA"
      },
      "id": "w-AJH9rZXVQA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SEC**"
      ],
      "metadata": {
        "id": "QuaS1fh5a5a-"
      },
      "id": "QuaS1fh5a5a-"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from mpl_toolkits.mplot3d import Axes3D  # This is needed for 3D plots\n",
        "\n",
        "def surface_plot_2d(model, X_ref_df, feat_x, feat_y, ax, title=\"Surface plot\", n_grid=60, fixed_strategy=\"median\", cmap=\"magma\"):\n",
        "    \"\"\"\n",
        "    model: trained model with predict(X)\n",
        "    X_ref_df: pandas DataFrame of features (use train or total)\n",
        "    feat_x, feat_y: two feature names to vary\n",
        "    ax: Matplotlib axis to plot on (for subplot layout)\n",
        "    fixed_strategy: \"median\" or \"mean\"\n",
        "    cmap: Colormap for the surface plot\n",
        "    \"\"\"\n",
        "    X_ref = X_ref_df.copy()\n",
        "\n",
        "    # fixed point for other features\n",
        "    if fixed_strategy == \"median\":\n",
        "        base = X_ref.median(numeric_only=True)\n",
        "    else:\n",
        "        base = X_ref.mean(numeric_only=True)\n",
        "\n",
        "    # grid ranges based on data\n",
        "    x_vals = np.linspace(X_ref[feat_x].min(), X_ref[feat_x].max(), n_grid)\n",
        "    y_vals = np.linspace(X_ref[feat_y].min(), X_ref[feat_y].max(), n_grid)\n",
        "    XX, YY = np.meshgrid(x_vals, y_vals)\n",
        "\n",
        "    # build grid dataframe for prediction\n",
        "    grid = pd.DataFrame(np.tile(base.values, (n_grid*n_grid, 1)), columns=base.index)\n",
        "    grid[feat_x] = XX.ravel()\n",
        "    grid[feat_y] = YY.ravel()\n",
        "\n",
        "    # ensure column order matches training\n",
        "    grid = grid[X_ref.columns]\n",
        "\n",
        "    ZZ = model.predict(grid).reshape(n_grid, n_grid)\n",
        "\n",
        "    # --- Plot on provided 3D axis ---\n",
        "    surface = ax.plot_surface(XX, YY, ZZ, linewidth=0, antialiased=True, alpha=0.9, cmap=cmap)\n",
        "    ax.set_xlabel(feat_x)\n",
        "    ax.set_ylabel(feat_y)\n",
        "    ax.set_zlabel(\"Predicted\", labelpad=10)  # Adjust label padding to avoid overlap\n",
        "    ax.set_title(title)\n",
        "\n",
        "    # Hide the SEC axis values (i.e., don't display any ticks on the z-axis)\n",
        "    ax.zaxis.set_ticks([])  # Hides the ticks on the z-axis\n",
        "\n",
        "    # Add color bar for each surface plot\n",
        "    fig.colorbar(surface, ax=ax, shrink=0.5, aspect=10)\n",
        "\n",
        "# Prepare plot grid with 5 rows and 2 columns\n",
        "fig, axes = plt.subplots(5, 2, figsize=(15, 25))  # Increase figure size to fit 10 subplots\n",
        "\n",
        "# Make sure to create 3D axes for each subplot\n",
        "for row in range(5):\n",
        "    for col in range(2):\n",
        "        axes[row, col] = fig.add_subplot(5, 2, 2 * row + col + 1, projection=\"3d\")  # Convert to 3D axis\n",
        "\n",
        "# List of features pairs and titles for surface plots and different colormaps\n",
        "plot_configs = [\n",
        "    (\"Fluegas Flow (kg/h)\", \"Amine type\", \"SEC surface: flue gas flow vs amine type\", \"magma\"),\n",
        "    (\"Fluegas Flow (kg/h)\", \"Amine Conc (wt%)\", \"SEC surface: flue gas flow vs amine concentration\", \"magma\"),\n",
        "    (\"Fluegas Flow (kg/h)\", \"Fluegas CO2 mol%\", \"SEC surface: flue gas flow vs CO2 mol%\", \"magma\"),\n",
        "    (\"Fluegas Flow (kg/h)\", \"Reboiler Duty (MW)\", \"SEC surface: flue gas flow vs reboiler duty\", \"magma\"),\n",
        "    (\"Fluegas CO2 mol%\", \"Amine type\", \"SEC surface: CO2 mol% vs amine type\", \"magma\"),\n",
        "    (\"Fluegas CO2 mol%\", \"Reboiler Duty (MW)\", \"SEC surface: CO2 mol% vs reboiler duty\", \"magma\"),\n",
        "    (\"Fluegas CO2 mol%\", \"Amine Conc (wt%)\", \"SEC surface: CO2 mol% vs amine concentration\", \"magma\"),\n",
        "    (\"Amine type\", \"Reboiler Duty (MW)\", \"SEC surface: amine type vs reboiler duty\", \"magma\"),\n",
        "    (\"Amine type\", \"Amine Conc (wt%)\", \"SEC surface: amine type vs amine concentration\", \"magma\"),\n",
        "    (\"Reboiler Duty (MW)\", \"Amine Conc (wt%)\", \"SEC surface: reboiler duty vs amine concentration\", \"magma\")\n",
        "]\n",
        "\n",
        "# Ensure the correct feature names and target values are loaded\n",
        "X_train = df  # X_train should be your feature DataFrame\n",
        "y_true = df_label2  # Ensure you have the correct target for the ANN\n",
        "\n",
        "# Model setup (ensure it's already trained)\n",
        "n_features = X_train.shape[1]  # Your features are in X_train\n",
        "ann_model1 = keras.Sequential([\n",
        "    layers.Input(shape=(n_features,)),\n",
        "    layers.Dense(120, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1)  # Single output\n",
        "])\n",
        "\n",
        "ann_model1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error')\n",
        "ann_model1.fit(X_train, y_true, epochs=101, batch_size=32, verbose=0)\n",
        "\n",
        "# Generate surface plots for each pair of features in the configuration list\n",
        "for i, (feat_x, feat_y, title, cmap) in enumerate(plot_configs):\n",
        "    row, col = divmod(i, 2)\n",
        "    surface_plot_2d(ann_model1, X_train, feat_x, feat_y, axes[row, col], title=title, n_grid=100, cmap=cmap)\n",
        "\n",
        "# Adjust layout and save as SVG\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"surface_plots_sec_without_ticks.svg\", format=\"svg\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4N8PyXIDj8hA"
      },
      "id": "4N8PyXIDj8hA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purity**"
      ],
      "metadata": {
        "id": "Fb6KqP3sgkbC"
      },
      "id": "Fb6KqP3sgkbC"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from mpl_toolkits.mplot3d import Axes3D  # This is needed for 3D plots\n",
        "\n",
        "def surface_plot_2d(model, X_ref_df, feat_x, feat_y, ax, title=\"Surface plot\", n_grid=60, fixed_strategy=\"median\", cmap=\"viridis\"):\n",
        "    \"\"\"\n",
        "    model: trained model with predict(X)\n",
        "    X_ref_df: pandas DataFrame of features (use train or total)\n",
        "    feat_x, feat_y: two feature names to vary\n",
        "    ax: Matplotlib axis to plot on (for subplot layout)\n",
        "    fixed_strategy: \"median\" or \"mean\"\n",
        "    cmap: Colormap for the surface plot\n",
        "    \"\"\"\n",
        "    X_ref = X_ref_df.copy()\n",
        "\n",
        "    # fixed point for other features\n",
        "    if fixed_strategy == \"median\":\n",
        "        base = X_ref.median(numeric_only=True)\n",
        "    else:\n",
        "        base = X_ref.mean(numeric_only=True)\n",
        "\n",
        "    # grid ranges based on data\n",
        "    x_vals = np.linspace(X_ref[feat_x].min(), X_ref[feat_x].max(), n_grid)\n",
        "    y_vals = np.linspace(X_ref[feat_y].min(), X_ref[feat_y].max(), n_grid)\n",
        "    XX, YY = np.meshgrid(x_vals, y_vals)\n",
        "\n",
        "    # build grid dataframe for prediction\n",
        "    grid = pd.DataFrame(np.tile(base.values, (n_grid*n_grid, 1)), columns=base.index)\n",
        "    grid[feat_x] = XX.ravel()\n",
        "    grid[feat_y] = YY.ravel()\n",
        "\n",
        "    # ensure column order matches training\n",
        "    grid = grid[X_ref_df.columns]\n",
        "\n",
        "    ZZ = model.predict(grid).reshape(n_grid, n_grid)\n",
        "\n",
        "    # --- Plot on provided 3D axis ---\n",
        "    surface = ax.plot_surface(XX, YY, ZZ, linewidth=0, antialiased=True, alpha=0.9, cmap=cmap)\n",
        "    ax.set_xlabel(feat_x)\n",
        "    ax.set_ylabel(feat_y)\n",
        "    ax.set_zlabel(\"Predicted\")\n",
        "    ax.set_title(title)\n",
        "\n",
        "    # Add color bar for each surface plot\n",
        "    fig.colorbar(surface, ax=ax, shrink=0.5, aspect=10)\n",
        "\n",
        "# Prepare plot grid with 3 rows and 2 columns\n",
        "fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
        "\n",
        "# Make sure to create 3D axes for each subplot\n",
        "for row in range(3):\n",
        "    for col in range(2):\n",
        "        axes[row, col] = fig.add_subplot(3, 2, 2 * row + col + 1, projection=\"3d\")  # Convert to 3D axis\n",
        "\n",
        "# List of features pairs and titles for surface plots and different colormaps\n",
        "plot_configs = [\n",
        "    (\"Fluegas Flow (kg/h)\", \"Amine type\", \"Purity surface: flue gas flow vs amine type\", \"cividis\"),\n",
        "    (\"Fluegas Flow (kg/h)\", \"Fluegas CO2 mol%\", \"Purity surface: flue gas flow vs CO2 mol%\", \"cividis\"),\n",
        "    (\"Fluegas Flow (kg/h)\", \"Lean Solvent Loading\", \"Purity surface: flue gas flow vs loading\", \"cividis\"),\n",
        "    (\"Fluegas CO2 mol%\", \"Amine type\", \"Purity surface: CO2 mol% vs amine type\", \"cividis\"),\n",
        "    (\"Fluegas CO2 mol%\", \"Lean Solvent Loading\", \"Purity surface: CO2 mol% vs loading\", \"cividis\"),\n",
        "    (\"Amine type\", \"Lean Solvent Loading\", \"Purity surface: amine type vs loading\", \"cividis\")\n",
        "]\n",
        "\n",
        "# Ensure the correct feature names and target values are loaded\n",
        "X_train = df  # X_train should be your feature DataFrame\n",
        "y_true = df_label3  # Ensure you have the correct target for the ANN\n",
        "\n",
        "# Model setup (ensure it's already trained)\n",
        "n_features = X_train.shape[1]  # Your features are in X_train\n",
        "ann_model1 = keras.Sequential([\n",
        "    layers.Input(shape=(n_features,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(72, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Single output\n",
        "])\n",
        "\n",
        "ann_model1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mean_absolute_error')\n",
        "ann_model1.fit(X_train, y_true, epochs=101, batch_size=32, verbose=0)\n",
        "\n",
        "# Generate surface plots for each pair of features in the configuration list\n",
        "for i, (feat_x, feat_y, title, cmap) in enumerate(plot_configs):\n",
        "    row, col = divmod(i, 2)\n",
        "    surface_plot_2d(ann_model1, X_train, feat_x, feat_y, axes[row, col], title=title, n_grid=100, cmap=cmap)\n",
        "\n",
        "# Adjust layout and save as SVG\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"surface_plots_pur.svg\", format=\"svg\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OtUy5IPQfuTy"
      },
      "id": "OtUy5IPQfuTy",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "B0T62JfjEbLL",
        "9e16e1c3-d3e8-4f73-a13f-e9c8e6f53d3f",
        "5bc304ec-d4a8-490a-8c33-8ee4f10a2ba4",
        "ab34b875-4a79-4b04-b9b3-8c6ae028376b",
        "f64e8ed4-09d1-4b3f-8b7e-0f60af5b090e",
        "eNRcfMSxDcwh",
        "6ef84f28-51b4-49de-b89e-52d9ee3dba9d",
        "9660c3c2-b5e1-48bb-83ae-00dc23aa4b6e",
        "saJUd25k_M9y",
        "Wv8Ur-_jsDi8",
        "KdrziX7-yxO5",
        "iI5J66EidAFF",
        "KEKTIloTqOGL",
        "yf7M3eot5Xja",
        "2xVSXfujlcOT",
        "64b3f088-9f4e-4187-8cf4-e88b6c9a1b41",
        "sKl_o8eWQlcX",
        "ho-atkgGV1mP",
        "521mM8KzUEk7",
        "FJ8zmb4nQ_wP",
        "wi18J1eoAyBn",
        "aed70d62-bdfb-4a38-921c-11b86bdea90a",
        "55432305-c9dc-4239-ac15-0e7b8772df80",
        "DH-S2nopUypw",
        "XrzOvxdUdEW8",
        "SbAViSE3U5-w",
        "fLelr0jwWHrB",
        "KluITpGbA5hy",
        "d65d9314-fde4-439d-9e62-43c0b69e14ad",
        "22bb8c6b-c6cb-40ee-bd3d-b435573cbac5",
        "iPS6_oDiY0Xp",
        "Z_5JZDFoSGxf",
        "N6yQdfoLSCLO",
        "s4QoGGS0Gg6_",
        "j_QQ5b-GRvOG",
        "13c895d5-a04d-4754-8686-f6066a8528da",
        "2e3824b7-6af4-48b8-bf70-c772e124db13",
        "C9ZLP1E2JXee",
        "kU3UfBvO94Uc",
        "z_Agu000-HyT",
        "1GOrCEab-Lrr",
        "FzqOuDpr-Olr",
        "DGHPVYQgtzOY",
        "rWskTFlJt4qQ",
        "uUIaiK7r1VBC",
        "9G17y8majDLw",
        "Yw63KmZmZDrH",
        "fiN3meAzH3Yb",
        "QuaS1fh5a5a-",
        "Fb6KqP3sgkbC"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}